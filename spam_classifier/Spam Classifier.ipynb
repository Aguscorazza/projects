{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46e2bf95",
   "metadata": {},
   "source": [
    "# Spam Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c74b779",
   "metadata": {},
   "source": [
    "The objective of this project is to build a spam classifier using Machine Learning Techniques. The data used in this notebook was provided by \"Apache SpamAssasin\" and it consists of around 6000 emails. I've implemented BagOfWords and Term Frequency-Inverse Document Frequency vectors to represent the data and feed the ML models. As a model I've only implemented a Logistic Regression model, which trains fast enough and achieves high performance (around 97%) on the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be1fa402",
   "metadata": {},
   "source": [
    "## Get the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96efcfd6",
   "metadata": {},
   "source": [
    "First, let's fetch the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "73154929",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tarfile\n",
    "import urllib.request\n",
    "import re\n",
    "\n",
    "DOWNLOAD_ROOT = \"http://spamassassin.apache.org/old/publiccorpus/\"\n",
    "\n",
    "HAM_URL = DOWNLOAD_ROOT + \"20030228_easy_ham.tar.bz2\"\n",
    "SPAM_URL = DOWNLOAD_ROOT + \"20030228_spam.tar.bz2\"\n",
    "HAM2_URL = DOWNLOAD_ROOT + \"20030228_easy_ham_2.tar.bz2\"\n",
    "HAM3_URL = DOWNLOAD_ROOT + \"20030228_hard_ham.tar.bz2\"\n",
    "SPAM2_URL = DOWNLOAD_ROOT + \"20050311_spam_2.tar.bz2\"\n",
    "\n",
    "DATA_PATH = os.path.join(os.curdir, \"data\")\n",
    "\n",
    "def fetch_spam_data(urls, data_path=DATA_PATH):\n",
    "    if not os.path.isdir(data_path):\n",
    "        os.makedirs(data_path)\n",
    "    \n",
    "    for url in urls:\n",
    "        filename = re.search(r\"_(.*)\", url).group(1)\n",
    "        print(f\"Current filename: {filename}\")\n",
    "        \n",
    "        path = os.path.join(data_path,filename)\n",
    "        \n",
    "        urllib.request.urlretrieve(url, path)\n",
    "        tar_bz2_file = tarfile.open(path)\n",
    "        tar_bz2_file.extractall(path=data_path)\n",
    "        tar_bz2_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e03a862e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current filename: easy_ham.tar.bz2\n",
      "Current filename: easy_ham_2.tar.bz2\n",
      "Current filename: hard_ham.tar.bz2\n",
      "Current filename: spam.tar.bz2\n",
      "Current filename: spam_2.tar.bz2\n"
     ]
    }
   ],
   "source": [
    "urls = [HAM_URL, HAM2_URL, HAM3_URL, SPAM_URL, SPAM2_URL]\n",
    "fetch_spam_data(urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a9e3e7b",
   "metadata": {},
   "source": [
    "Now let's load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac3381e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "HAM_DIRS = [os.path.join(DATA_PATH, \"easy_ham\"),\n",
    "           os.path.join(DATA_PATH, \"easy_ham_2\"),\n",
    "           os.path.join(DATA_PATH, \"hard_ham\")]\n",
    "SPAM_DIRS = [os.path.join(DATA_PATH, \"spam\"),\n",
    "            os.path.join(DATA_PATH, \"spam_2\")]\n",
    "\n",
    "ham_filenames=list()\n",
    "spam_filenames=list()\n",
    "for ham_dir in HAM_DIRS:\n",
    "    for filename in sorted(os.listdir(ham_dir)):\n",
    "        if len(filename) > 20:\n",
    "            ham_filenames.append(os.path.join(ham_dir,filename))\n",
    "\n",
    "for spam_dir in SPAM_DIRS:\n",
    "    for filename in sorted(os.listdir(spam_dir)):\n",
    "        if len(filename) > 20:\n",
    "            spam_filenames.append(os.path.join(spam_dir,filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de4c61ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of ham emails: 4150\n",
      "Number of spam emails: 1896\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of ham emails: {len(ham_filenames)}\\nNumber of spam emails: {len(spam_filenames)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f0064052",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'.\\\\data\\\\easy_ham\\\\00001.7c53336b37003a9286aba55d2945844c'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ham_filenames[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e09f23c",
   "metadata": {},
   "source": [
    "We can use Python's **email** module to parse these emails (this handles headers, encoding, and so on):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "364e1d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import email\n",
    "import email.policy\n",
    "\n",
    "def load_emails(filenames, data_path=DATA_PATH):\n",
    "    emails = []\n",
    "    for filename in filenames:\n",
    "        with open(filename, \"rb\") as f:\n",
    "            emails.append(email.parser.BytesParser(policy=email.policy.default).parse(f))\n",
    "    return emails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "89af66f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ham_emails = load_emails(ham_filenames)\n",
    "spam_emails = load_emails(spam_filenames)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "964c686c",
   "metadata": {},
   "source": [
    "Let's look at some examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bc636fb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help wanted.  We are a 14 year old fortune 500 company, that is\n",
      "growing at a tremendous rate.  We are looking for individuals who\n",
      "want to work from home.\n",
      "\n",
      "This is an opportunity to make an excellent income.  No experience\n",
      "is required.  We will train you.\n",
      "\n",
      "So if you are looking to be employed from home with a career that has\n",
      "vast opportunities, then go:\n",
      "\n",
      "http://www.basetel.com/wealthnow\n",
      "\n",
      "We are looking for energetic and self motivated people.  If that is you\n",
      "than click on the link and fill out the form, and one of our\n",
      "employement specialist will contact you.\n",
      "\n",
      "To be removed from our link simple go to:\n",
      "\n",
      "http://www.basetel.com/remove.html\n",
      "\n",
      "\n",
      "4139vOLW7-758DoDY1425FRhM1-764SMFc8513fCsLl40\n"
     ]
    }
   ],
   "source": [
    "print(spam_emails[6].get_content().strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a7c8ff00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Martin A posted:\n",
      "Tassos Papadopoulos, the Greek sculptor behind the plan, judged that the\n",
      " limestone of Mount Kerdylio, 70 miles east of Salonika and not far from the\n",
      " Mount Athos monastic community, was ideal for the patriotic sculpture. \n",
      " \n",
      " As well as Alexander's granite features, 240 ft high and 170 ft wide, a\n",
      " museum, a restored amphitheatre and car park for admiring crowds are\n",
      "planned\n",
      "---------------------\n",
      "So is this mountain limestone or granite?\n",
      "If it's limestone, it'll weather pretty fast.\n",
      "\n",
      "------------------------ Yahoo! Groups Sponsor ---------------------~-->\n",
      "4 DVDs Free +s&p Join Now\n",
      "http://us.click.yahoo.com/pt6YBB/NXiEAA/mG3HAA/7gSolB/TM\n",
      "---------------------------------------------------------------------~->\n",
      "\n",
      "To unsubscribe from this group, send an email to:\n",
      "forteana-unsubscribe@egroups.com\n",
      "\n",
      " \n",
      "\n",
      "Your use of Yahoo! Groups is subject to http://docs.yahoo.com/info/terms/\n"
     ]
    }
   ],
   "source": [
    "print(ham_emails[1].get_content().strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2030bbd",
   "metadata": {},
   "source": [
    "Some emails are actually multipart, with images and attachments (which can have their own attachments). Let's look at the various types of structures we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e1a320ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_email_structure(email):\n",
    "    if isinstance(email, str):\n",
    "        return email\n",
    "    payload = email.get_payload()\n",
    "    if isinstance(payload, list):\n",
    "        return \"multipart({})\".format(\", \".join([\n",
    "            get_email_structure(sub_email)\n",
    "            for sub_email in payload\n",
    "        ]))\n",
    "    else:\n",
    "        return email.get_content_type()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0d80348a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def structures_counter(emails):\n",
    "    structures = Counter()\n",
    "    for email in emails:\n",
    "        structure = get_email_structure(email)\n",
    "        structures[structure] += 1\n",
    "    return structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0d40dc8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('text/plain', 3832),\n",
       " ('text/html', 120),\n",
       " ('multipart(text/plain, application/pgp-signature)', 101),\n",
       " ('multipart(text/plain, text/html)', 63),\n",
       " ('multipart(text/plain, text/plain)', 5)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "structures_counter(ham_emails).most_common()[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7093d385",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('text/plain', 815),\n",
       " ('text/html', 772),\n",
       " ('multipart(text/plain, text/html)', 159),\n",
       " ('multipart(text/html)', 49),\n",
       " ('multipart(text/plain)', 44)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "structures_counter(spam_emails).most_common()[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "657fe38d",
   "metadata": {},
   "source": [
    "It seems that the ham emails are more often plain text, while spam has quite a lot of HTML. Moreover, quite a few ham emails are signed using PGP, while no spam is. In short, it seems that the email structure is useful information to have."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f09ec24c",
   "metadata": {},
   "source": [
    "Let's split the data into a training set and a test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1d628156",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = np.array(ham_emails + spam_emails, dtype=object)\n",
    "y = np.array([0] * len(ham_emails) + [1] * len(spam_emails))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1404f8a",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c43060",
   "metadata": {},
   "source": [
    "### HTML to plain text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d43abcd2",
   "metadata": {},
   "source": [
    "Okay, let's start writing the preprocessing functions. First, we will need a function to convert HTML to plain text using BeautifulSoup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "12a23fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from html import unescape\n",
    "import re\n",
    "\n",
    "def html_to_plain_text(html):\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    \n",
    "    # Remove script and style tags\n",
    "    for script in soup([\"script\", \"style\"]):\n",
    "        script.extract()\n",
    "    \n",
    "    # Replace line breaks with space\n",
    "    text = soup.get_text(separator=' ')\n",
    "    text = text.replace('\\n', ' ')\n",
    "    \n",
    "    # Unescape HTML entities\n",
    "    text = unescape(text)\n",
    "    \n",
    "    return re.sub('\\s+', ' ', text.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d758b13f",
   "metadata": {},
   "source": [
    "Let's see if this works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3197d550",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE HTML PUBLIC \"-//W3C//DTD HTML 4.0 Transitional//EN\">\n",
      "<html>\n",
      "\n",
      "<head>\n",
      "<meta http-equiv=\"Content-Language\" content=\"en-ie\">\n",
      "<meta name=\"Microsoft Theme 2.00\" content=\"9to6 1011 011\">\n",
      "<meta http ....\n"
     ]
    }
   ],
   "source": [
    "html_spam_emails = [email for email in X_train[y_train==1]\n",
    "                    if get_email_structure(email) == \"text/html\"]\n",
    "sample_html_spam = html_spam_emails[7]\n",
    "print(sample_html_spam.get_content().strip()[:200], \"....\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5461a805",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ink_refill_toner PRINTER INK CARTRIDGES & REFILL KITS from  4.85... BULK ORDERS or TRADE welcome... please contact us at info@9to6.ie for discounted prices guaranteed to give you huge savings of betw ....\n"
     ]
    }
   ],
   "source": [
    "print(html_to_plain_text(sample_html_spam.get_content())[:200], \"....\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3582ac08",
   "metadata": {},
   "source": [
    "Great! Now let's write a function that takes an email as input and returns its content as plain text, whatever its format is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2ab545c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def email_to_text(email):\n",
    "    html = None\n",
    "    for part in email.walk():\n",
    "        ctype = part.get_content_type()\n",
    "        if not ctype in (\"text/plain\", \"text/html\"):\n",
    "            continue\n",
    "        try:\n",
    "            content = part.get_content()\n",
    "        except: # in case of encoding issues\n",
    "            content = str(part.get_payload())\n",
    "        if ctype == \"text/plain\":\n",
    "            return content\n",
    "        else:\n",
    "            html = content\n",
    "    if html:\n",
    "        return html_to_plain_text(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0160b408",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ink_refill_toner PRINTER INK CARTRIDGES & REFILL KITS from  4.85... BULK ORDERS or TRADE welcome... ...\n"
     ]
    }
   ],
   "source": [
    "print(email_to_text(sample_html_spam)[:100], \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf0475a3",
   "metadata": {},
   "source": [
    "Now let's build this as a Transformer, so then we can use it in a Data Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5e709cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import urlextract # may require an Internet connection to download root domain names\n",
    "    \n",
    "\n",
    "class EmailToTextTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, replace_numbers=True, remove_punctuation=True, replace_urls=True, to_lower=True):\n",
    "        self.replace_numbers = replace_numbers\n",
    "        self.remove_punctuation = remove_punctuation\n",
    "        self.replace_urls = replace_urls\n",
    "        self.to_lower = to_lower\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        X_transformed = []\n",
    "        for email in X:\n",
    "            text = email_to_text(email) or \"\"\n",
    "            text = re.sub(r'_', '', text)\n",
    "            if self.to_lower:\n",
    "                text = text.lower()\n",
    "            if self.replace_urls:\n",
    "                url_extractor = urlextract.URLExtract()\n",
    "                urls = list(set(url_extractor.find_urls(text)))\n",
    "                urls.sort(key=lambda url: len(url), reverse=True)\n",
    "                for url in urls:\n",
    "                    text = text.replace(url, \" URL \")\n",
    "            if self.replace_numbers:\n",
    "                text = re.sub(r'\\d+(?:\\.\\d*)?(?:[eE][+-]?\\d+)?', 'NUMBER', text)\n",
    "            if self.remove_punctuation:\n",
    "                text = re.sub(r'\\W+', ' ', text, flags=re.M)\n",
    "            X_transformed.append(text)\n",
    "        return X_transformed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbba19e9",
   "metadata": {},
   "source": [
    "### Word Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "32ce4517",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "class WordTokenizer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        X_transformed = []\n",
    "        for text_email in X:\n",
    "            # Tokenize the text into words\n",
    "            words = word_tokenize(text_email)\n",
    "            X_transformed.append(words)\n",
    "            \n",
    "        return X_transformed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade7d4e7",
   "metadata": {},
   "source": [
    "### Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "58005059",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Agustin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Agustin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Agustin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Agustin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Download the stopwords corpus if not already present\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Download the open Multilingual WordNet. It is a lexical database of English words and their semantic meanings.\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "class DataCleaner(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, remove_stopwords=True, stemming=True, lemmatization=True):\n",
    "        self.remove_stopwords = remove_stopwords\n",
    "        self.stemming = stemming\n",
    "        self.lemmatization = lemmatization\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        X_transformed = []\n",
    "        for word_list in X:\n",
    "            word_list_transformed = []\n",
    "            if self.remove_stopwords:\n",
    "                stop_words = set(stopwords.words('english'))\n",
    "                for word in word_list:\n",
    "                    if word.lower() not in stop_words:\n",
    "                        word_list_transformed.append(word)\n",
    "                word_list = word_list_transformed\n",
    "                word_list_transformed = []\n",
    "                \n",
    "            if self.lemmatization:\n",
    "                lemmatizer = WordNetLemmatizer()\n",
    "                for word in word_list:\n",
    "                    lemma = lemmatizer.lemmatize(word)\n",
    "                    word_list_transformed.append(lemma)\n",
    "                word_list = word_list_transformed\n",
    "                word_list_transformed = []\n",
    "            \n",
    "            if self.stemming:\n",
    "                stemmer = nltk.PorterStemmer()\n",
    "                for word in word_list:\n",
    "                    stemmed_word = stemmer.stem(word)\n",
    "                    word_list_transformed.append(stemmed_word)\n",
    "                word_list = word_list_transformed\n",
    "        \n",
    "            word_list_transformed = word_list\n",
    "            X_transformed.append(word_list_transformed)\n",
    "        return X_transformed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df143e6c",
   "metadata": {},
   "source": [
    "Let's try these transformers in a Pipeline with a few emails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "da731163",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['digital',\n",
       " 'dispatch',\n",
       " 'weekly',\n",
       " 'newsletter',\n",
       " 'cnet',\n",
       " 'web',\n",
       " 'apple',\n",
       " 'expand',\n",
       " 'imac',\n",
       " 'lcd']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "preprocessing_pipeline = Pipeline([\n",
    "    ('email_to_text', EmailToTextTransformer()),\n",
    "    ('word_tokenizer', WordTokenizer()),\n",
    "    ('data_cleaner', DataCleaner(stemming=False)),\n",
    "])\n",
    "\"\"\"\n",
    "    \"\"\"\n",
    "X_few = X_train[:3]\n",
    "X_few_wordlist = preprocessing_pipeline.fit_transform(X_few)\n",
    "X_few_wordlist[0][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a5e512",
   "metadata": {},
   "source": [
    "## Feature Representation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6379964",
   "metadata": {},
   "source": [
    "### Word Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1e46d504",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class WordCounter(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        X_transformed = []\n",
    "        for word_list in X:\n",
    "            word_counts = Counter(word_list)\n",
    "            X_transformed.append(word_counts)\n",
    "        return np.array(X_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "33fa3eef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('NUMBER', 32), ('free', 11), ('cnet', 10), ('player', 10), ('mpNUMBER', 8)]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_few_wordcounts = WordCounter().fit_transform(X_few_wordlist)\n",
    "X_few_wordcounts[0].most_common(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "849ac1df",
   "metadata": {},
   "source": [
    "### Bags of Words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fae401c",
   "metadata": {},
   "source": [
    "Now we have the word counts transformer, and we need to convert them to vectors. Firstly, we will use a **Bag of Words (BoW) vectorizer**.  It involves converting a piece of text, such as an email or message, into a numerical feature vector based on the frequency of occurrence of words in that text. The idea behind BoW is that the presence and frequency of specific words can help determine whether a text is spam or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "27b5d15a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "\n",
    "class BagsOfWordsVectorizer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, vocabulary_size=1000):\n",
    "        self.vocabulary_size = vocabulary_size\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        total_count = Counter()\n",
    "        for word_count in X:\n",
    "            for word, count in word_count.items():\n",
    "                total_count[word] += min(count, 10)\n",
    "        most_common = total_count.most_common()[:self.vocabulary_size]\n",
    "        self.vocabulary_ = {word: index + 1 for index, (word, count) in enumerate(most_common)}\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        rows = []\n",
    "        cols = []\n",
    "        data = []\n",
    "        for row, word_count in enumerate(X):\n",
    "            for word, count in word_count.items():\n",
    "                rows.append(row)\n",
    "                cols.append(self.vocabulary_.get(word, 0))\n",
    "                data.append(count)\n",
    "        return csr_matrix((data, (rows, cols)), shape=(len(X), self.vocabulary_size + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f93bfd23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<3x11 sparse matrix of type '<class 'numpy.intc'>'\n",
       "\twith 19 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_transformer = BagsOfWordsVectorizer(vocabulary_size=10)\n",
    "X_few_vectors = vocab_transformer.fit_transform(X_few_wordcounts)\n",
    "X_few_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "59fbbebc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[597,  32,  10,  10,  11,   0,   7,   3,   8,   7,   6],\n",
       "       [147,  37,   0,   0,   0,  10,   1,   4,   0,   0,   1],\n",
       "       [153,   2,   0,   0,   0,   0,   0,   1,   0,   0,   0]],\n",
       "      dtype=int32)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_few_vectors.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b6be3e6",
   "metadata": {},
   "source": [
    "What does this matrix mean? Well, the 147 in the second row, first column, means that the second email contains 147 words that are not part of the vocabulary. The 37 next to it means that the first word in the vocabulary is present 37 times in this email. The 0 next to it means that the second word is present 0 times, and so on. You can look at the vocabulary to know which words we are talking about. The first word are numbers that were replaced by the word \"NUMBER\", the second word is \"cnet\", etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "530e2929",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'NUMBER': 1,\n",
       " 'cnet': 2,\n",
       " 'player': 3,\n",
       " 'free': 4,\n",
       " 'openssl': 5,\n",
       " 'e': 6,\n",
       " 'version': 7,\n",
       " 'mpNUMBER': 8,\n",
       " 'web': 9,\n",
       " 'time': 10}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_transformer.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "166734e5",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c4adeb",
   "metadata": {},
   "source": [
    "**TF-IDF (Term Frequency-Inverse Document Frequency)** vectors are another commonly used technique for text representation, particularly in information retrieval and text mining tasks, including spam classification. TF-IDF takes into account not only the frequency of occurrence of words but also their importance in the context of the entire corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "58ad8c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import math\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "class TFIDFVectorizer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.vocabulary_ = None\n",
    "        self.idf_scores = None\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        # Create vocabulary\n",
    "        self.vocabulary_ = self._create_vocabulary(X)\n",
    "\n",
    "        # Compute IDF scores\n",
    "        self.idf_scores = self._compute_idf(X)\n",
    "        \n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        # Compute TF scores\n",
    "        tf_scores = self._compute_tf(X)\n",
    "    \n",
    "        tfidf_scores = []\n",
    "        \n",
    "        for tf in tf_scores:\n",
    "            tfidf_scores.append({word: tf[word] * self.idf_scores.get(word, 0.0) for word in tf})\n",
    "\n",
    "        # Convert the tf-idf scores to a numpy array\n",
    "        tfidf_matrix = np.zeros((len(X), len(self.vocabulary_)), dtype=np.float32)\n",
    "\n",
    "        for i, tfidf_dict in enumerate(tfidf_scores):\n",
    "            for j, word in enumerate(self.vocabulary_):\n",
    "                tfidf_matrix[i, j] = tfidf_dict.get(word, 0.0)\n",
    "\n",
    "        return tfidf_matrix\n",
    "        \n",
    "    def _create_vocabulary(self, X):\n",
    "        vocab = set()\n",
    "        for word_list in X:\n",
    "            vocab.update(word_list)\n",
    "        return sorted(list(vocab))\n",
    "    \n",
    "    def _compute_tf(self, X):\n",
    "        tf_scores = []\n",
    "        for word_list in X:\n",
    "            word_counts = Counter(word_list)\n",
    "            total_words = len(word_list)\n",
    "            tf_scores.append({word: word_counts[word] / total_words for word in word_list})\n",
    "        return tf_scores\n",
    "    \n",
    "    def _compute_idf(self, X):\n",
    "        idf_scores = {}\n",
    "        num_documents = len(X)\n",
    "        for word_list in X:\n",
    "            unique_words = set(word_list)\n",
    "            for word in unique_words:\n",
    "                if word in idf_scores:\n",
    "                    idf_scores[word] += 1\n",
    "                else:\n",
    "                    idf_scores[word] = 1\n",
    "        idf_scores = {word: math.log(num_documents / count) for word, count in idf_scores.items()}\n",
    "        return idf_scores\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0c4b24bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 623)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_few_tfidf_vector = TFIDFVectorizer().fit_transform(X_few_wordlist)\n",
    "X_few_tfidf_vector.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "285fde94",
   "metadata": {},
   "source": [
    "## Full Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f0415b",
   "metadata": {},
   "source": [
    "Once the data vectors are ready we can write pipelines to train different models. In this case I've only tried Logistic Regression models due to their fast convergence and high performance in this dataset.\n",
    "\n",
    "Here I've implemented several preprocessing options, which consist of including or not **stop words** in the vectorized dataset and whether or not use **stemming** and **lemmatization**. I've explored the performance of these options using GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4f0ff5e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "preprocessing_param_grid = {\n",
    "    \"preprocessing__data_cleaner__remove_stopwords\": [True, False],\n",
    "    \"preprocessing__data_cleaner__stemming\": [True, False],\n",
    "    \"preprocessing__data_cleaner__lemmatization\": [True, False],\n",
    "    \"model__solver\": ['lbfgs', 'liblinear']\n",
    "}\n",
    "\n",
    "bow_full_pipeline = Pipeline([\n",
    "    (\"preprocessing\", preprocessing_pipeline),\n",
    "    (\"word_counter\", WordCounter()),\n",
    "    (\"bags_of_words\", BagsOfWordsVectorizer()),\n",
    "    (\"model\", LogisticRegression(max_iter=5000)),\n",
    "])\n",
    "\n",
    "\n",
    "tfidf_full_pipeline = Pipeline([\n",
    "    (\"preprocessing\", preprocessing_pipeline),\n",
    "    (\"tfidf_vectorizer\", TFIDFVectorizer()),\n",
    "    (\"model\", LogisticRegression(max_iter=5000)),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "706130d3",
   "metadata": {},
   "source": [
    "We are now ready to train our first spam classifier! Let's transform the whole dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bd806a7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 16 candidates, totalling 48 fits\n",
      "[CV 1/3] END model__solver=lbfgs, preprocessing__data_cleaner__lemmatization=True, preprocessing__data_cleaner__remove_stopwords=True, preprocessing__data_cleaner__stemming=True;, score=0.973 total time= 4.5min\n",
      "[CV 2/3] END model__solver=lbfgs, preprocessing__data_cleaner__lemmatization=True, preprocessing__data_cleaner__remove_stopwords=True, preprocessing__data_cleaner__stemming=True;, score=0.972 total time= 4.1min\n",
      "[CV 3/3] END model__solver=lbfgs, preprocessing__data_cleaner__lemmatization=True, preprocessing__data_cleaner__remove_stopwords=True, preprocessing__data_cleaner__stemming=True;, score=0.976 total time= 3.9min\n",
      "[CV 1/3] END model__solver=lbfgs, preprocessing__data_cleaner__lemmatization=True, preprocessing__data_cleaner__remove_stopwords=True, preprocessing__data_cleaner__stemming=False;, score=0.971 total time= 4.3min\n",
      "[CV 2/3] END model__solver=lbfgs, preprocessing__data_cleaner__lemmatization=True, preprocessing__data_cleaner__remove_stopwords=True, preprocessing__data_cleaner__stemming=False;, score=0.970 total time= 4.1min\n",
      "[CV 3/3] END model__solver=lbfgs, preprocessing__data_cleaner__lemmatization=True, preprocessing__data_cleaner__remove_stopwords=True, preprocessing__data_cleaner__stemming=False;, score=0.973 total time= 4.2min\n",
      "[CV 1/3] END model__solver=lbfgs, preprocessing__data_cleaner__lemmatization=True, preprocessing__data_cleaner__remove_stopwords=False, preprocessing__data_cleaner__stemming=True;, score=0.977 total time= 5.0min\n",
      "[CV 2/3] END model__solver=lbfgs, preprocessing__data_cleaner__lemmatization=True, preprocessing__data_cleaner__remove_stopwords=False, preprocessing__data_cleaner__stemming=True;, score=0.971 total time= 5.1min\n",
      "[CV 3/3] END model__solver=lbfgs, preprocessing__data_cleaner__lemmatization=True, preprocessing__data_cleaner__remove_stopwords=False, preprocessing__data_cleaner__stemming=True;, score=0.977 total time= 5.1min\n",
      "[CV 1/3] END model__solver=lbfgs, preprocessing__data_cleaner__lemmatization=True, preprocessing__data_cleaner__remove_stopwords=False, preprocessing__data_cleaner__stemming=False;, score=0.975 total time= 4.4min\n",
      "[CV 2/3] END model__solver=lbfgs, preprocessing__data_cleaner__lemmatization=True, preprocessing__data_cleaner__remove_stopwords=False, preprocessing__data_cleaner__stemming=False;, score=0.973 total time= 4.3min\n",
      "[CV 3/3] END model__solver=lbfgs, preprocessing__data_cleaner__lemmatization=True, preprocessing__data_cleaner__remove_stopwords=False, preprocessing__data_cleaner__stemming=False;, score=0.973 total time= 4.4min\n",
      "[CV 1/3] END model__solver=lbfgs, preprocessing__data_cleaner__lemmatization=False, preprocessing__data_cleaner__remove_stopwords=True, preprocessing__data_cleaner__stemming=True;, score=0.971 total time= 4.7min\n",
      "[CV 2/3] END model__solver=lbfgs, preprocessing__data_cleaner__lemmatization=False, preprocessing__data_cleaner__remove_stopwords=True, preprocessing__data_cleaner__stemming=True;, score=0.973 total time= 4.9min\n",
      "[CV 3/3] END model__solver=lbfgs, preprocessing__data_cleaner__lemmatization=False, preprocessing__data_cleaner__remove_stopwords=True, preprocessing__data_cleaner__stemming=True;, score=0.976 total time= 4.8min\n",
      "[CV 1/3] END model__solver=lbfgs, preprocessing__data_cleaner__lemmatization=False, preprocessing__data_cleaner__remove_stopwords=True, preprocessing__data_cleaner__stemming=False;, score=0.969 total time= 4.2min\n",
      "[CV 2/3] END model__solver=lbfgs, preprocessing__data_cleaner__lemmatization=False, preprocessing__data_cleaner__remove_stopwords=True, preprocessing__data_cleaner__stemming=False;, score=0.973 total time= 4.4min\n",
      "[CV 3/3] END model__solver=lbfgs, preprocessing__data_cleaner__lemmatization=False, preprocessing__data_cleaner__remove_stopwords=True, preprocessing__data_cleaner__stemming=False;, score=0.973 total time= 4.6min\n",
      "[CV 1/3] END model__solver=lbfgs, preprocessing__data_cleaner__lemmatization=False, preprocessing__data_cleaner__remove_stopwords=False, preprocessing__data_cleaner__stemming=True;, score=0.976 total time= 4.6min\n",
      "[CV 2/3] END model__solver=lbfgs, preprocessing__data_cleaner__lemmatization=False, preprocessing__data_cleaner__remove_stopwords=False, preprocessing__data_cleaner__stemming=True;, score=0.972 total time= 4.7min\n",
      "[CV 3/3] END model__solver=lbfgs, preprocessing__data_cleaner__lemmatization=False, preprocessing__data_cleaner__remove_stopwords=False, preprocessing__data_cleaner__stemming=True;, score=0.977 total time= 4.8min\n",
      "[CV 1/3] END model__solver=lbfgs, preprocessing__data_cleaner__lemmatization=False, preprocessing__data_cleaner__remove_stopwords=False, preprocessing__data_cleaner__stemming=False;, score=0.974 total time= 4.3min\n",
      "[CV 2/3] END model__solver=lbfgs, preprocessing__data_cleaner__lemmatization=False, preprocessing__data_cleaner__remove_stopwords=False, preprocessing__data_cleaner__stemming=False;, score=0.972 total time= 4.4min\n",
      "[CV 3/3] END model__solver=lbfgs, preprocessing__data_cleaner__lemmatization=False, preprocessing__data_cleaner__remove_stopwords=False, preprocessing__data_cleaner__stemming=False;, score=0.976 total time= 4.3min\n",
      "[CV 1/3] END model__solver=liblinear, preprocessing__data_cleaner__lemmatization=True, preprocessing__data_cleaner__remove_stopwords=True, preprocessing__data_cleaner__stemming=True;, score=0.973 total time= 4.6min\n",
      "[CV 2/3] END model__solver=liblinear, preprocessing__data_cleaner__lemmatization=True, preprocessing__data_cleaner__remove_stopwords=True, preprocessing__data_cleaner__stemming=True;, score=0.972 total time= 5.2min\n",
      "[CV 3/3] END model__solver=liblinear, preprocessing__data_cleaner__lemmatization=True, preprocessing__data_cleaner__remove_stopwords=True, preprocessing__data_cleaner__stemming=True;, score=0.976 total time= 5.4min\n",
      "[CV 1/3] END model__solver=liblinear, preprocessing__data_cleaner__lemmatization=True, preprocessing__data_cleaner__remove_stopwords=True, preprocessing__data_cleaner__stemming=False;, score=0.973 total time= 4.3min\n",
      "[CV 2/3] END model__solver=liblinear, preprocessing__data_cleaner__lemmatization=True, preprocessing__data_cleaner__remove_stopwords=True, preprocessing__data_cleaner__stemming=False;, score=0.970 total time= 3.9min\n",
      "[CV 3/3] END model__solver=liblinear, preprocessing__data_cleaner__lemmatization=True, preprocessing__data_cleaner__remove_stopwords=True, preprocessing__data_cleaner__stemming=False;, score=0.973 total time= 4.3min\n",
      "[CV 1/3] END model__solver=liblinear, preprocessing__data_cleaner__lemmatization=True, preprocessing__data_cleaner__remove_stopwords=False, preprocessing__data_cleaner__stemming=True;, score=0.977 total time= 5.6min\n",
      "[CV 2/3] END model__solver=liblinear, preprocessing__data_cleaner__lemmatization=True, preprocessing__data_cleaner__remove_stopwords=False, preprocessing__data_cleaner__stemming=True;, score=0.972 total time= 5.4min\n",
      "[CV 3/3] END model__solver=liblinear, preprocessing__data_cleaner__lemmatization=True, preprocessing__data_cleaner__remove_stopwords=False, preprocessing__data_cleaner__stemming=True;, score=0.977 total time= 5.3min\n",
      "[CV 1/3] END model__solver=liblinear, preprocessing__data_cleaner__lemmatization=True, preprocessing__data_cleaner__remove_stopwords=False, preprocessing__data_cleaner__stemming=False;, score=0.975 total time= 4.9min\n",
      "[CV 2/3] END model__solver=liblinear, preprocessing__data_cleaner__lemmatization=True, preprocessing__data_cleaner__remove_stopwords=False, preprocessing__data_cleaner__stemming=False;, score=0.973 total time= 4.2min\n",
      "[CV 3/3] END model__solver=liblinear, preprocessing__data_cleaner__lemmatization=True, preprocessing__data_cleaner__remove_stopwords=False, preprocessing__data_cleaner__stemming=False;, score=0.974 total time= 4.6min\n",
      "[CV 1/3] END model__solver=liblinear, preprocessing__data_cleaner__lemmatization=False, preprocessing__data_cleaner__remove_stopwords=True, preprocessing__data_cleaner__stemming=True;, score=0.971 total time= 4.8min\n",
      "[CV 2/3] END model__solver=liblinear, preprocessing__data_cleaner__lemmatization=False, preprocessing__data_cleaner__remove_stopwords=True, preprocessing__data_cleaner__stemming=True;, score=0.973 total time= 5.2min\n",
      "[CV 3/3] END model__solver=liblinear, preprocessing__data_cleaner__lemmatization=False, preprocessing__data_cleaner__remove_stopwords=True, preprocessing__data_cleaner__stemming=True;, score=0.976 total time= 5.9min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3] END model__solver=liblinear, preprocessing__data_cleaner__lemmatization=False, preprocessing__data_cleaner__remove_stopwords=True, preprocessing__data_cleaner__stemming=False;, score=0.969 total time=10.9min\n",
      "[CV 2/3] END model__solver=liblinear, preprocessing__data_cleaner__lemmatization=False, preprocessing__data_cleaner__remove_stopwords=True, preprocessing__data_cleaner__stemming=False;, score=0.972 total time=13.6min\n",
      "[CV 3/3] END model__solver=liblinear, preprocessing__data_cleaner__lemmatization=False, preprocessing__data_cleaner__remove_stopwords=True, preprocessing__data_cleaner__stemming=False;, score=0.974 total time=11.6min\n",
      "[CV 1/3] END model__solver=liblinear, preprocessing__data_cleaner__lemmatization=False, preprocessing__data_cleaner__remove_stopwords=False, preprocessing__data_cleaner__stemming=True;, score=0.976 total time=12.4min\n",
      "[CV 2/3] END model__solver=liblinear, preprocessing__data_cleaner__lemmatization=False, preprocessing__data_cleaner__remove_stopwords=False, preprocessing__data_cleaner__stemming=True;, score=0.972 total time=12.3min\n",
      "[CV 3/3] END model__solver=liblinear, preprocessing__data_cleaner__lemmatization=False, preprocessing__data_cleaner__remove_stopwords=False, preprocessing__data_cleaner__stemming=True;, score=0.978 total time=16.0min\n",
      "[CV 1/3] END model__solver=liblinear, preprocessing__data_cleaner__lemmatization=False, preprocessing__data_cleaner__remove_stopwords=False, preprocessing__data_cleaner__stemming=False;, score=0.974 total time=15.2min\n",
      "[CV 2/3] END model__solver=liblinear, preprocessing__data_cleaner__lemmatization=False, preprocessing__data_cleaner__remove_stopwords=False, preprocessing__data_cleaner__stemming=False;, score=0.972 total time=14.4min\n",
      "[CV 3/3] END model__solver=liblinear, preprocessing__data_cleaner__lemmatization=False, preprocessing__data_cleaner__remove_stopwords=False, preprocessing__data_cleaner__stemming=False;, score=0.978 total time=16.8min\n",
      "Fitting 3 folds for each of 16 candidates, totalling 48 fits\n",
      "[CV 1/3] END model__solver=lbfgs, preprocessing__data_cleaner__lemmatization=True, preprocessing__data_cleaner__remove_stopwords=True, preprocessing__data_cleaner__stemming=True;, score=0.892 total time=29.0min\n",
      "[CV 2/3] END model__solver=lbfgs, preprocessing__data_cleaner__lemmatization=True, preprocessing__data_cleaner__remove_stopwords=True, preprocessing__data_cleaner__stemming=True;, score=0.885 total time=14.8min\n",
      "[CV 3/3] END model__solver=lbfgs, preprocessing__data_cleaner__lemmatization=True, preprocessing__data_cleaner__remove_stopwords=True, preprocessing__data_cleaner__stemming=True;, score=0.913 total time= 5.0min\n",
      "[CV 1/3] END model__solver=lbfgs, preprocessing__data_cleaner__lemmatization=True, preprocessing__data_cleaner__remove_stopwords=True, preprocessing__data_cleaner__stemming=False;, score=0.890 total time= 4.6min\n",
      "[CV 2/3] END model__solver=lbfgs, preprocessing__data_cleaner__lemmatization=True, preprocessing__data_cleaner__remove_stopwords=True, preprocessing__data_cleaner__stemming=False;, score=0.891 total time= 4.6min\n",
      "[CV 3/3] END model__solver=lbfgs, preprocessing__data_cleaner__lemmatization=True, preprocessing__data_cleaner__remove_stopwords=True, preprocessing__data_cleaner__stemming=False;, score=0.914 total time= 4.8min\n",
      "[CV 1/3] END model__solver=lbfgs, preprocessing__data_cleaner__lemmatization=True, preprocessing__data_cleaner__remove_stopwords=False, preprocessing__data_cleaner__stemming=True;, score=0.815 total time= 4.8min\n",
      "[CV 2/3] END model__solver=lbfgs, preprocessing__data_cleaner__lemmatization=True, preprocessing__data_cleaner__remove_stopwords=False, preprocessing__data_cleaner__stemming=True;, score=0.812 total time= 4.8min\n",
      "[CV 3/3] END model__solver=lbfgs, preprocessing__data_cleaner__lemmatization=True, preprocessing__data_cleaner__remove_stopwords=False, preprocessing__data_cleaner__stemming=True;, score=0.824 total time= 5.0min\n",
      "[CV 1/3] END model__solver=lbfgs, preprocessing__data_cleaner__lemmatization=True, preprocessing__data_cleaner__remove_stopwords=False, preprocessing__data_cleaner__stemming=False;, score=0.818 total time= 4.6min\n",
      "[CV 2/3] END model__solver=lbfgs, preprocessing__data_cleaner__lemmatization=True, preprocessing__data_cleaner__remove_stopwords=False, preprocessing__data_cleaner__stemming=False;, score=0.813 total time= 4.6min\n",
      "[CV 3/3] END model__solver=lbfgs, preprocessing__data_cleaner__lemmatization=True, preprocessing__data_cleaner__remove_stopwords=False, preprocessing__data_cleaner__stemming=False;, score=0.826 total time= 4.8min\n",
      "[CV 1/3] END model__solver=lbfgs, preprocessing__data_cleaner__lemmatization=False, preprocessing__data_cleaner__remove_stopwords=True, preprocessing__data_cleaner__stemming=True;, score=0.892 total time= 4.7min\n",
      "[CV 2/3] END model__solver=lbfgs, preprocessing__data_cleaner__lemmatization=False, preprocessing__data_cleaner__remove_stopwords=True, preprocessing__data_cleaner__stemming=True;, score=0.884 total time= 4.6min\n",
      "[CV 3/3] END model__solver=lbfgs, preprocessing__data_cleaner__lemmatization=False, preprocessing__data_cleaner__remove_stopwords=True, preprocessing__data_cleaner__stemming=True;, score=0.913 total time= 4.9min\n",
      "[CV 1/3] END model__solver=lbfgs, preprocessing__data_cleaner__lemmatization=False, preprocessing__data_cleaner__remove_stopwords=True, preprocessing__data_cleaner__stemming=False;, score=0.890 total time= 4.6min\n",
      "[CV 2/3] END model__solver=lbfgs, preprocessing__data_cleaner__lemmatization=False, preprocessing__data_cleaner__remove_stopwords=True, preprocessing__data_cleaner__stemming=False;, score=0.891 total time= 4.6min\n",
      "[CV 3/3] END model__solver=lbfgs, preprocessing__data_cleaner__lemmatization=False, preprocessing__data_cleaner__remove_stopwords=True, preprocessing__data_cleaner__stemming=False;, score=0.914 total time= 4.8min\n",
      "[CV 1/3] END model__solver=lbfgs, preprocessing__data_cleaner__lemmatization=False, preprocessing__data_cleaner__remove_stopwords=False, preprocessing__data_cleaner__stemming=True;, score=0.816 total time= 4.8min\n",
      "[CV 2/3] END model__solver=lbfgs, preprocessing__data_cleaner__lemmatization=False, preprocessing__data_cleaner__remove_stopwords=False, preprocessing__data_cleaner__stemming=True;, score=0.812 total time= 4.8min\n",
      "[CV 3/3] END model__solver=lbfgs, preprocessing__data_cleaner__lemmatization=False, preprocessing__data_cleaner__remove_stopwords=False, preprocessing__data_cleaner__stemming=True;, score=0.825 total time= 4.9min\n",
      "[CV 1/3] END model__solver=lbfgs, preprocessing__data_cleaner__lemmatization=False, preprocessing__data_cleaner__remove_stopwords=False, preprocessing__data_cleaner__stemming=False;, score=0.812 total time= 4.6min\n",
      "[CV 2/3] END model__solver=lbfgs, preprocessing__data_cleaner__lemmatization=False, preprocessing__data_cleaner__remove_stopwords=False, preprocessing__data_cleaner__stemming=False;, score=0.808 total time= 4.6min\n",
      "[CV 3/3] END model__solver=lbfgs, preprocessing__data_cleaner__lemmatization=False, preprocessing__data_cleaner__remove_stopwords=False, preprocessing__data_cleaner__stemming=False;, score=0.821 total time= 4.7min\n",
      "[CV 1/3] END model__solver=liblinear, preprocessing__data_cleaner__lemmatization=True, preprocessing__data_cleaner__remove_stopwords=True, preprocessing__data_cleaner__stemming=True;, score=0.893 total time= 4.6min\n",
      "[CV 2/3] END model__solver=liblinear, preprocessing__data_cleaner__lemmatization=True, preprocessing__data_cleaner__remove_stopwords=True, preprocessing__data_cleaner__stemming=True;, score=0.885 total time= 4.7min\n",
      "[CV 3/3] END model__solver=liblinear, preprocessing__data_cleaner__lemmatization=True, preprocessing__data_cleaner__remove_stopwords=True, preprocessing__data_cleaner__stemming=True;, score=0.913 total time= 4.9min\n",
      "[CV 1/3] END model__solver=liblinear, preprocessing__data_cleaner__lemmatization=True, preprocessing__data_cleaner__remove_stopwords=True, preprocessing__data_cleaner__stemming=False;, score=0.890 total time= 4.6min\n",
      "[CV 2/3] END model__solver=liblinear, preprocessing__data_cleaner__lemmatization=True, preprocessing__data_cleaner__remove_stopwords=True, preprocessing__data_cleaner__stemming=False;, score=0.892 total time= 4.6min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3] END model__solver=liblinear, preprocessing__data_cleaner__lemmatization=True, preprocessing__data_cleaner__remove_stopwords=True, preprocessing__data_cleaner__stemming=False;, score=0.914 total time= 4.8min\n",
      "[CV 1/3] END model__solver=liblinear, preprocessing__data_cleaner__lemmatization=True, preprocessing__data_cleaner__remove_stopwords=False, preprocessing__data_cleaner__stemming=True;, score=0.815 total time= 4.9min\n",
      "[CV 2/3] END model__solver=liblinear, preprocessing__data_cleaner__lemmatization=True, preprocessing__data_cleaner__remove_stopwords=False, preprocessing__data_cleaner__stemming=True;, score=0.813 total time= 4.9min\n",
      "[CV 3/3] END model__solver=liblinear, preprocessing__data_cleaner__lemmatization=True, preprocessing__data_cleaner__remove_stopwords=False, preprocessing__data_cleaner__stemming=True;, score=0.826 total time= 5.1min\n",
      "[CV 1/3] END model__solver=liblinear, preprocessing__data_cleaner__lemmatization=True, preprocessing__data_cleaner__remove_stopwords=False, preprocessing__data_cleaner__stemming=False;, score=0.818 total time= 4.6min\n",
      "[CV 2/3] END model__solver=liblinear, preprocessing__data_cleaner__lemmatization=True, preprocessing__data_cleaner__remove_stopwords=False, preprocessing__data_cleaner__stemming=False;, score=0.813 total time= 4.6min\n",
      "[CV 3/3] END model__solver=liblinear, preprocessing__data_cleaner__lemmatization=True, preprocessing__data_cleaner__remove_stopwords=False, preprocessing__data_cleaner__stemming=False;, score=0.827 total time= 4.8min\n",
      "[CV 1/3] END model__solver=liblinear, preprocessing__data_cleaner__lemmatization=False, preprocessing__data_cleaner__remove_stopwords=True, preprocessing__data_cleaner__stemming=True;, score=0.893 total time= 4.6min\n",
      "[CV 2/3] END model__solver=liblinear, preprocessing__data_cleaner__lemmatization=False, preprocessing__data_cleaner__remove_stopwords=True, preprocessing__data_cleaner__stemming=True;, score=0.884 total time= 4.7min\n",
      "[CV 3/3] END model__solver=liblinear, preprocessing__data_cleaner__lemmatization=False, preprocessing__data_cleaner__remove_stopwords=True, preprocessing__data_cleaner__stemming=True;, score=0.913 total time= 5.3min\n",
      "[CV 1/3] END model__solver=liblinear, preprocessing__data_cleaner__lemmatization=False, preprocessing__data_cleaner__remove_stopwords=True, preprocessing__data_cleaner__stemming=False;, score=0.890 total time= 4.6min\n",
      "[CV 2/3] END model__solver=liblinear, preprocessing__data_cleaner__lemmatization=False, preprocessing__data_cleaner__remove_stopwords=True, preprocessing__data_cleaner__stemming=False;, score=0.891 total time= 4.6min\n",
      "[CV 3/3] END model__solver=liblinear, preprocessing__data_cleaner__lemmatization=False, preprocessing__data_cleaner__remove_stopwords=True, preprocessing__data_cleaner__stemming=False;, score=0.914 total time= 4.8min\n",
      "[CV 1/3] END model__solver=liblinear, preprocessing__data_cleaner__lemmatization=False, preprocessing__data_cleaner__remove_stopwords=False, preprocessing__data_cleaner__stemming=True;, score=0.817 total time= 4.8min\n",
      "[CV 2/3] END model__solver=liblinear, preprocessing__data_cleaner__lemmatization=False, preprocessing__data_cleaner__remove_stopwords=False, preprocessing__data_cleaner__stemming=True;, score=0.814 total time= 4.8min\n",
      "[CV 3/3] END model__solver=liblinear, preprocessing__data_cleaner__lemmatization=False, preprocessing__data_cleaner__remove_stopwords=False, preprocessing__data_cleaner__stemming=True;, score=0.825 total time= 4.9min\n",
      "[CV 1/3] END model__solver=liblinear, preprocessing__data_cleaner__lemmatization=False, preprocessing__data_cleaner__remove_stopwords=False, preprocessing__data_cleaner__stemming=False;, score=0.813 total time= 4.5min\n",
      "[CV 2/3] END model__solver=liblinear, preprocessing__data_cleaner__lemmatization=False, preprocessing__data_cleaner__remove_stopwords=False, preprocessing__data_cleaner__stemming=False;, score=0.808 total time= 4.6min\n",
      "[CV 3/3] END model__solver=liblinear, preprocessing__data_cleaner__lemmatization=False, preprocessing__data_cleaner__remove_stopwords=False, preprocessing__data_cleaner__stemming=False;, score=0.821 total time= 4.8min\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=3,\n",
       "             estimator=Pipeline(steps=[(&#x27;preprocessing&#x27;,\n",
       "                                        Pipeline(steps=[(&#x27;email_to_text&#x27;,\n",
       "                                                         EmailToTextTransformer()),\n",
       "                                                        (&#x27;word_tokenizer&#x27;,\n",
       "                                                         WordTokenizer()),\n",
       "                                                        (&#x27;data_cleaner&#x27;,\n",
       "                                                         DataCleaner(stemming=False))])),\n",
       "                                       (&#x27;tfidf_vectorizer&#x27;, TFIDFVectorizer()),\n",
       "                                       (&#x27;model&#x27;,\n",
       "                                        LogisticRegression(max_iter=5000))]),\n",
       "             param_grid={&#x27;model__solver&#x27;: [&#x27;lbfgs&#x27;, &#x27;liblinear&#x27;],\n",
       "                         &#x27;preprocessing__data_cleaner__lemmatization&#x27;: [True,\n",
       "                                                                        False],\n",
       "                         &#x27;preprocessing__data_cleaner__remove_stopwords&#x27;: [True,\n",
       "                                                                           False],\n",
       "                         &#x27;preprocessing__data_cleaner__stemming&#x27;: [True,\n",
       "                                                                   False]},\n",
       "             verbose=5)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=3,\n",
       "             estimator=Pipeline(steps=[(&#x27;preprocessing&#x27;,\n",
       "                                        Pipeline(steps=[(&#x27;email_to_text&#x27;,\n",
       "                                                         EmailToTextTransformer()),\n",
       "                                                        (&#x27;word_tokenizer&#x27;,\n",
       "                                                         WordTokenizer()),\n",
       "                                                        (&#x27;data_cleaner&#x27;,\n",
       "                                                         DataCleaner(stemming=False))])),\n",
       "                                       (&#x27;tfidf_vectorizer&#x27;, TFIDFVectorizer()),\n",
       "                                       (&#x27;model&#x27;,\n",
       "                                        LogisticRegression(max_iter=5000))]),\n",
       "             param_grid={&#x27;model__solver&#x27;: [&#x27;lbfgs&#x27;, &#x27;liblinear&#x27;],\n",
       "                         &#x27;preprocessing__data_cleaner__lemmatization&#x27;: [True,\n",
       "                                                                        False],\n",
       "                         &#x27;preprocessing__data_cleaner__remove_stopwords&#x27;: [True,\n",
       "                                                                           False],\n",
       "                         &#x27;preprocessing__data_cleaner__stemming&#x27;: [True,\n",
       "                                                                   False]},\n",
       "             verbose=5)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;preprocessing&#x27;,\n",
       "                 Pipeline(steps=[(&#x27;email_to_text&#x27;, EmailToTextTransformer()),\n",
       "                                 (&#x27;word_tokenizer&#x27;, WordTokenizer()),\n",
       "                                 (&#x27;data_cleaner&#x27;,\n",
       "                                  DataCleaner(stemming=False))])),\n",
       "                (&#x27;tfidf_vectorizer&#x27;, TFIDFVectorizer()),\n",
       "                (&#x27;model&#x27;, LogisticRegression(max_iter=5000))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">preprocessing: Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;email_to_text&#x27;, EmailToTextTransformer()),\n",
       "                (&#x27;word_tokenizer&#x27;, WordTokenizer()),\n",
       "                (&#x27;data_cleaner&#x27;, DataCleaner(stemming=False))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">EmailToTextTransformer</label><div class=\"sk-toggleable__content\"><pre>EmailToTextTransformer()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">WordTokenizer</label><div class=\"sk-toggleable__content\"><pre>WordTokenizer()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" ><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DataCleaner</label><div class=\"sk-toggleable__content\"><pre>DataCleaner(stemming=False)</pre></div></div></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" ><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TFIDFVectorizer</label><div class=\"sk-toggleable__content\"><pre>TFIDFVectorizer()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-8\" type=\"checkbox\" ><label for=\"sk-estimator-id-8\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(max_iter=5000)</pre></div></div></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(cv=3,\n",
       "             estimator=Pipeline(steps=[('preprocessing',\n",
       "                                        Pipeline(steps=[('email_to_text',\n",
       "                                                         EmailToTextTransformer()),\n",
       "                                                        ('word_tokenizer',\n",
       "                                                         WordTokenizer()),\n",
       "                                                        ('data_cleaner',\n",
       "                                                         DataCleaner(stemming=False))])),\n",
       "                                       ('tfidf_vectorizer', TFIDFVectorizer()),\n",
       "                                       ('model',\n",
       "                                        LogisticRegression(max_iter=5000))]),\n",
       "             param_grid={'model__solver': ['lbfgs', 'liblinear'],\n",
       "                         'preprocessing__data_cleaner__lemmatization': [True,\n",
       "                                                                        False],\n",
       "                         'preprocessing__data_cleaner__remove_stopwords': [True,\n",
       "                                                                           False],\n",
       "                         'preprocessing__data_cleaner__stemming': [True,\n",
       "                                                                   False]},\n",
       "             verbose=5)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "bow_grid_srch = GridSearchCV(bow_full_pipeline, param_grid=preprocessing_param_grid, cv=3, verbose=5)\n",
    "bow_grid_srch.fit(X_train, y_train)\n",
    "\n",
    "tfidf_grid_srch = GridSearchCV(tfidf_full_pipeline, param_grid=preprocessing_param_grid, cv=3, verbose=5)\n",
    "tfidf_grid_srch.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c72c2e",
   "metadata": {},
   "source": [
    "Save and load the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a72b66d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tfidf_grid_search_model.joblib']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# Save the models to a file\n",
    "joblib.dump(bow_grid_srch, 'bow_grid_search_model.joblib')\n",
    "joblib.dump(tfidf_grid_srch, 'tfidf_grid_search_model.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4f7fc428",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_bow_grid_search = joblib.load('bow_grid_search_model.joblib')\n",
    "loaded_tfidf_grid_search = joblib.load('tfidf_grid_search_model.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4484638",
   "metadata": {},
   "source": [
    "## Predictions & Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2083919a",
   "metadata": {},
   "source": [
    "Let's transform the test set and use the best model make predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3a8da427",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best models for BoW and TF-IDF vectorization techniques:\n",
    "bow_best_model = loaded_bow_grid_search.best_estimator_\n",
    "tfidf_best_model = loaded_tfidf_grid_search.best_estimator_\n",
    "\n",
    "bow_y_pred = bow_best_model.predict(X_test)\n",
    "tfidf_y_pred = tfidf_best_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa7f8a01",
   "metadata": {},
   "source": [
    "In the following cell we can see that the Bag of Words Vectorization technique achieves a high performance in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "748a6b1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag of Words Vectorization:\n",
      "Precision: 96.73%\n",
      "Recall: 98.34%\n",
      "F1 score: 97.53%\n",
      "----------\n",
      "TF-IDF Vectorization:\n",
      "Precision: 100.00%\n",
      "Recall: 80.61%\n",
      "F1 score: 89.26%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "print(\"Bag of Words Vectorization:\")\n",
    "print(\"Precision: {:.2f}%\".format(100 * precision_score(y_test, bow_y_pred)))\n",
    "print(\"Recall: {:.2f}%\".format(100 * recall_score(y_test, bow_y_pred)))\n",
    "print(\"F1 score: {:.2f}%\".format(100 * f1_score(y_test, bow_y_pred)))\n",
    "print(\"-\"*10)\n",
    "print(\"TF-IDF Vectorization:\")\n",
    "print(\"Precision: {:.2f}%\".format(100 * precision_score(y_test, tfidf_y_pred)))\n",
    "print(\"Recall: {:.2f}%\".format(100 * recall_score(y_test, tfidf_y_pred)))\n",
    "print(\"F1 score: {:.2f}%\".format(100 * f1_score(y_test, tfidf_y_pred)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
