{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46e2bf95",
   "metadata": {},
   "source": [
    "# Spam Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c74b779",
   "metadata": {},
   "source": [
    "The objective of this project is to build a simple email spam classifier using Machine Learning Techniques. The data used in this notebook was provided by [Euron-spam corpus](https://www2.aueb.gr/users/ion/data/enron-spam/) and it consists of around 33716 preprocessed emails. I've implemented BagOfWords and Term Frequency-Inverse Document Frequency vectors to represent the data and feed the ML models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be1fa402",
   "metadata": {},
   "source": [
    "## Get the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96efcfd6",
   "metadata": {},
   "source": [
    "First, let's fetch the data from the dataset's website. Let's create a simple function that allows us to download the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "73154929",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tarfile\n",
    "import urllib.request\n",
    "import re\n",
    "\n",
    "DOWNLOAD_ROOT = \"http://nlp.cs.aueb.gr/software_and_datasets/Enron-Spam/preprocessed/\"\n",
    "DATA_PATH = os.path.join(os.curdir, \"data\")\n",
    "\n",
    "def fetch_spam_data(data_path=DATA_PATH):\n",
    "    if not os.path.isdir(data_path):\n",
    "        os.makedirs(data_path)\n",
    "    \n",
    "    for i in range(1,7):\n",
    "        filename = f\"enron{i}.tar.gz\"\n",
    "        print(f\"Current filename: {filename}\")\n",
    "        \n",
    "        url = DOWNLOAD_ROOT + filename\n",
    "        path = os.path.join(data_path,filename)\n",
    "        \n",
    "        urllib.request.urlretrieve(url, path)\n",
    "        tar_bz2_file = tarfile.open(path)\n",
    "        tar_bz2_file.extractall(path=data_path)\n",
    "        tar_bz2_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e03a862e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current filename: enron1.tar.gz\n",
      "Current filename: enron2.tar.gz\n",
      "Current filename: enron3.tar.gz\n",
      "Current filename: enron4.tar.gz\n",
      "Current filename: enron5.tar.gz\n",
      "Current filename: enron6.tar.gz\n"
     ]
    }
   ],
   "source": [
    "fetch_spam_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a9e3e7b",
   "metadata": {},
   "source": [
    "Now let's load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac3381e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "HAM_DIRS = [os.path.join(DATA_PATH, f\"enron{i}\", \"ham\") for i in range(1,7)]\n",
    "SPAM_DIRS = [os.path.join(DATA_PATH, f\"enron{i}\", \"spam\") for i in range(1,7)]\n",
    "\n",
    "ham_filenames=list()\n",
    "spam_filenames=list()\n",
    "for ham_dir in HAM_DIRS:\n",
    "    for filename in sorted(os.listdir(ham_dir)):\n",
    "            ham_filenames.append(os.path.join(ham_dir,filename))\n",
    "\n",
    "for spam_dir in SPAM_DIRS:\n",
    "    for filename in sorted(os.listdir(spam_dir)):\n",
    "            spam_filenames.append(os.path.join(spam_dir,filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de4c61ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of ham emails: 16545\n",
      "Number of spam emails: 17171\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of ham emails: {len(ham_filenames)}\\nNumber of spam emails: {len(spam_filenames)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f0064052",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_ham_file = ham_filenames[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1927d455",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject: calpine daily gas nomination\n",
      "- calpine daily gas nomination 1 . doc\n"
     ]
    }
   ],
   "source": [
    "with open(sample_ham_file,'r') as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1d628156",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "ham_emails=list()\n",
    "spam_emails=list()\n",
    "for file in ham_filenames + spam_filenames:\n",
    "    try:\n",
    "        with open(file, 'r') as f:\n",
    "            if file in ham_filenames:\n",
    "                ham_emails.append(f.read())\n",
    "            else:\n",
    "                spam_emails.append(f.read())\n",
    "    except UnicodeDecodeError:\n",
    "        pass\n",
    "\n",
    "X = np.array(ham_emails + spam_emails, dtype=object)\n",
    "y = np.array([0] * len(ham_emails) + [1] * len(spam_emails))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aa9153cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26960"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d9985594",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Subject: localized software , all languages available .\\nhello , we would like to offer localized software versions ( german , french , spanish , uk , and many others ) .\\naii iisted software is availabie for immediate downioad !\\nno need to wait 2 - 3 week for cd deiivery !\\njust few exampies :\\n- norton lnternet security pro 2005 - $ 29 . 95\\n- windows xp professionai with sp 2 fuil version - $ 59 . 95\\n- corei draw graphics suite 12 - $ 49 . 95\\n- dreamweaver mx 2004 ( homesite 5 . 5 inciudinq ) - $ 39 . 95\\n- macromedia studio mx 2004 - $ 119 . 95\\njust browse our site and find any software you need in your native lanquaqe !\\nbest regards ,\\nmae\\n',\n",
       "       'Subject: industry forum # 136\\nthe industry forum\\nminute man ii\\n160 lbs . light - requires no electricity - under $ 6000 complete ! now everybody can be a foamer !\\nsmall , one time project ? froth - pak is the answer ! smallest self - contained out - of - box foam application for repairs and small jobs ! also available : insta - stick , tilebond , roofpak and more !\\nget your copy of the industry catalog ! most complete reference for our industry !\\nclick on the picture above to learn more about the equipment !\\nthe industry forum issue # 136\\n10341 forum members\\n# 136 - 1 : dik m , pa\\nit would be much more useful to the industry , if spfa would help find a \" code conforming \" fire barrier for attics , crawl spaces etc . , than lecture on fast and loose applications of code . something about 30 cents ( a bd . ft . ) installed , meeting all codes and spray applied . many companies we talk to have a product that appears to be perfect but they haven \\' t spent the money to test over foam . maybe a small committee could do a evaluation of various products , report results to spfa , who in turn will \" acquire funding \" from us .\\n# 136 - 2 william b , australia\\nwe are in australia but we are finding it hard to locate polyurea sprayers here so that we can start a chapter of the pda do u have any list of australian contractors / suppliers ? ? ? ? or maybe they can put their hand up and lets us know at enviroline @ powerup . com . au , it isfor the betterof the game that an association is need to inform clients and specifiers of the uses and properties of polyureas and train its own members , also to keep cowboys out that give the industry a bad name\\n# 136 - 3 ed m .\\ni am a writer for aviation magazines and one that i write for is a test international , a new publication that just launched this month . i need any new processes you might have that would be used on aircraft , particularly commercial aviation .\\n# 136 - 4 john c , louisiana\\ni have a foam cat 2000 graco machine and a probler gun with a 01 tip . i use this setup to spray truck bed liners . my question is can i and how do i spray foam insulation using my equipment ? and what are the different types of insulation ?\\n# 136 - 5 murph mahaffey , glas - craft\\ni like the new look of the industry forum !\\n# 136 - 6 mark w , south carolina\\ni \\' m sick of all this garbage about covering foam with a \" thermal barrier \" . what the hell do you think foam is ? if you are saying that anything that can be forced to burn should be covered with a fireproof barrier , then you \\' d better get busy covering all those trusses and joists and plywood up there that are definitely not carrying a class one fire rating . this double standard for foam is coming directly from those who stand to lose business from it . icynene is different . it doesn \\' t burn alone . it doesn \\' t melt . it doesn \\' t emit phosgene . does it smoke in a fire situation ? sure , so what ?\\nblaming foam in the attic for a house fire because it forces the foam to eventually burn is about as stupid as blaming foam seat cushions for burning when a gas tank ruptures and burns up a car . get real , inspectors . you want us to cover up the only thing in the attic assembly that doesn \\' t burn !\\nas for the vaunted r rating system , it is totally bogus . the test is designed to make porous insulations appear to perform better than they actually do . measuring only conduction and ignoring convection and radiant to convective transfer , the test should have been thrown out for the industry lackey it is 20 years ago , along with that obsolete fiberglassgarbage they \\' re still selling everybody . there \\' s a reason they don \\' t make refrigerators , freezers coolers coffee cups out of fiberglass or cellulose . think about it .\\n# 136 - 7 dirk benthien , forum moderator\\nthank you , mark and dik , for your statements above . i also feel that far too often individuals and companies are too complacent and quietly live with rules and regulations without trying to change them - even if everybody knows they do not make sense . we are all experts and representatives of this industry and should speak up and promote change !\\n# 136 - 8 martin s , canada\\ndoes anyone make a dispensing machine for crumb rubber / urethane blends ?\\n# 136 - 9 carole l , california re # 135 - 8 otto v , germany\\nwhat are the answers ? ( phase - out of 141 b )\\n# 136 - 10 brian d , canada\\nthere has been a lot of talk about ceramic coatings . we are a distributor of such a coating . we have never professed an r - 20 . reason = is there isn \\' t an astm test available to measure coatings for r - value . we compare standardized insulation to ceramic coatings via btu loss calculations . depending on the criteria we can equal 2 to 3 inches of standard insulation with an aluminum jacket . we have the data and the projects to prove it .\\n# 136 - 11 cpi\\nwe have inquiries from a number of people for used equipment - especially gusmer h 20 / 35 , h 2000 , h 2 , gx - 7 and glas - craft probler . please contact 805 - 552 - 9128 . + + + + + + + + + + + + + + + + + + + + + + + + + + + +\\nend of messages .\\nthis forum welcomes anyone interested in the processing of single - or plural - component materials such as polyurethane , polyurea , coatings , epoxies , and other spray - applied materials .\\nthe industry forum . a free eservice from\\nto ask or answer a question , or to contribute anything , simply send an e - mail to forum @ cpillc . com\\nused gusmer h 20 / 35 and probler gun for sale ! 805 - 552 - 9128\\nyour privacy is protected ! please read the policies and rules at cpillc . com .\\nshow your name here ! become a sponsor ! call 805 - 552 - 9128 or send an email to cpi .\\nread this and previous forum issues all on one page . it will only work if you are connected to the internet ! click here to go to cpillc . com / forumdiscussion . htm .\\nvisit cpi , llc on the web . click http : / / www . cpillc . com /\\ncpi is authorized distributor for all leading manufacturers in this industry including gusmer , glas - craft , graco , resin technology , dynasolve .\\ncpi \\' s customers enjoy impartial advice , full service , life - long free phone support , training and set - up with all new system at a very fair price !\\nshop our online warehouse 24 / 7 most efficient procurement anywhere ! www . cpillc . com / warehouse . htm\\njob marketspray jobs\\ndid we miss someone ? feel free to submit any number of e - mail addresses of coworkers and friends to be included here . again , this service is free for all ! help grow our forum !\\nwe \\' re cpi . we make it work !\\ncall us toll - free 877 - cpi - 2100805 - 552 - 9128\\ncopyright ( c ) 2000 , 2001 , 2002 cpi , llc . all rights reserved . disclaimers and limitations of liabilities posted at cpillc . comthis free eservice is made possible by cpi . please visit their web site at www . cpillc . com or call toll - free 877 - 274 - 2600 or 805 - 552 - 9128 . if you wish to unsubscribe , please hit the reply button - subject = remove . please allow 3 days to take effect .\\n'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1404f8a",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf0475a3",
   "metadata": {},
   "source": [
    "As you might have seen, we've split the dataset into a training set and a test set containing 26960 and 6756 emails respectively.\n",
    "\n",
    "In any text mining problem, text cleaning is the first step where we remove those words from the document which may not contribute to the information we want to extract. Emails may contain a lot of undesirable characters like punctuation marks, stop words, digits, etc which may not be helpful in detecting the spam email. \n",
    "\n",
    "That's why we will implement the following preprocessing steps:\n",
    "- **Remove Stopwords**: Stop words like “and”, “the”, “of”, etc are very common in all English sentences and are not very meaningful in deciding spam or legitimate status, so these words have been removed from the emails.\n",
    "- **Lemmatization**: It is the process of grouping together the different inflected forms of a word so they can be analysed as a single item. For example, “include”, “includes,” and “included” would all be represented as “include”. The context of the sentence is also preserved in lemmatization as opposed to stemming (another buzz word in text mining which does not consider meaning of the sentence).\n",
    "- **Replace Numbers and URLs**.\n",
    "- **Transform all the text to lowercase**.\n",
    "- **Remove non-non-word characters**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5e709cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import urlextract # may require an Internet connection to download root domain names\n",
    "    \n",
    "\n",
    "class EmailToTextTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, replace_numbers=True, remove_punctuation=True, replace_urls=True, to_lower=True):\n",
    "        self.replace_numbers = replace_numbers\n",
    "        self.remove_punctuation = remove_punctuation\n",
    "        self.replace_urls = replace_urls\n",
    "        self.to_lower = to_lower\n",
    "        \n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        X_transformed = []\n",
    "        for email in X:\n",
    "            email = re.sub(r'[_-]+', ' ', email)\n",
    "            email = re.sub(r'(?i)subject:', '', email)\n",
    "            if self.to_lower:\n",
    "                email = email.lower()\n",
    "            if self.replace_urls:\n",
    "                url_extractor = urlextract.URLExtract()\n",
    "                urls = list(set(url_extractor.find_urls(email)))\n",
    "                urls.sort(key=lambda url: len(url), reverse=True)\n",
    "                for url in urls:\n",
    "                    email = email.replace(url, \" URL \")\n",
    "            if self.replace_numbers:\n",
    "                email = re.sub(r'\\d+(?:\\.\\d*)?(?:[eE][+-]?\\d+)?', 'NUMBER', email)\n",
    "            if self.remove_punctuation:\n",
    "                email = re.sub(r'\\W+', ' ', email, flags=re.M)\n",
    "            X_transformed.append(email)\n",
    "        return X_transformed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbba19e9",
   "metadata": {},
   "source": [
    "### Word Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "32ce4517",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "class WordTokenizer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        X_transformed = []\n",
    "        for text_email in X:\n",
    "            # Tokenize the text into words\n",
    "            words = word_tokenize(text_email)\n",
    "            X_transformed.append(words)\n",
    "            \n",
    "        return X_transformed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade7d4e7",
   "metadata": {},
   "source": [
    "### Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "58005059",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Agustin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Agustin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Agustin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Agustin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Download the stopwords corpus if not already present\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Download the open Multilingual WordNet. It is a lexical database of English words and their semantic meanings.\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "class DataCleaner(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, remove_stopwords=True, lemmatization=True):\n",
    "        self.remove_stopwords = remove_stopwords\n",
    "        self.lemmatization = lemmatization\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        X_transformed = []\n",
    "        for word_list in X:\n",
    "            word_list_transformed = []\n",
    "            if self.remove_stopwords:\n",
    "                stop_words = set(stopwords.words('english'))\n",
    "                for word in word_list:\n",
    "                    if word.lower() not in stop_words:\n",
    "                        word_list_transformed.append(word)\n",
    "                word_list = word_list_transformed\n",
    "                word_list_transformed = []\n",
    "                \n",
    "            if self.lemmatization:\n",
    "                lemmatizer = WordNetLemmatizer()\n",
    "                for word in word_list:\n",
    "                    lemma = lemmatizer.lemmatize(word)\n",
    "                    word_list_transformed.append(lemma)\n",
    "                word_list = word_list_transformed\n",
    "        \n",
    "            word_list_transformed = word_list\n",
    "            X_transformed.append(word_list_transformed)\n",
    "        return X_transformed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df143e6c",
   "metadata": {},
   "source": [
    "Let's try these transformers in a Pipeline with a few emails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "da731163",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['localized',\n",
       " 'software',\n",
       " 'language',\n",
       " 'available',\n",
       " 'hello',\n",
       " 'would',\n",
       " 'like',\n",
       " 'offer',\n",
       " 'localized',\n",
       " 'software']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "preprocessing_pipeline = Pipeline([\n",
    "    ('email_to_text', EmailToTextTransformer()),\n",
    "    ('word_tokenizer', WordTokenizer()),\n",
    "    ('data_cleaner', DataCleaner()),\n",
    "])\n",
    "\"\"\"\n",
    "    \"\"\"\n",
    "X_few = X_train[:3]\n",
    "X_few_wordlist = preprocessing_pipeline.fit_transform(X_few)\n",
    "X_few_wordlist[0][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a5e512",
   "metadata": {},
   "source": [
    "## Feature Representation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6379964",
   "metadata": {},
   "source": [
    "### Word Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1e46d504",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class WordCounter(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        X_transformed = []\n",
    "        for word_list in X:\n",
    "            word_counts = Counter(word_list)\n",
    "            X_transformed.append(word_counts)\n",
    "        return np.array(X_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "33fa3eef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('NUMBER', 19),\n",
       " ('software', 4),\n",
       " ('localized', 2),\n",
       " ('version', 2),\n",
       " ('need', 2)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_few_wordcounts = WordCounter().fit_transform(X_few_wordlist)\n",
    "X_few_wordcounts[0].most_common(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "849ac1df",
   "metadata": {},
   "source": [
    "### Bags of Words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fae401c",
   "metadata": {},
   "source": [
    "Now we have the word counts transformer, and we need to convert them to vectors. Firstly, we will use a **Bag of Words (BoW) vectorizer**.  It involves converting a piece of text, such as an email or message, into a numerical feature vector based on the frequency of occurrence of words in that text. The idea behind BoW is that the presence and frequency of specific words can help determine whether a text is spam or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "27b5d15a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "\n",
    "class BagsOfWordsVectorizer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, vocabulary_size=1000):\n",
    "        self.vocabulary_size = vocabulary_size\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        total_count = Counter()\n",
    "        for word_count in X:\n",
    "            for word, count in word_count.items():\n",
    "                total_count[word] += min(count, 10)\n",
    "        most_common = total_count.most_common()[:self.vocabulary_size]\n",
    "        self.vocabulary_ = {word: index + 1 for index, (word, count) in enumerate(most_common)}\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        rows = []\n",
    "        cols = []\n",
    "        data = []\n",
    "        for row, word_count in enumerate(X):\n",
    "            for word, count in word_count.items():\n",
    "                rows.append(row)\n",
    "                cols.append(self.vocabulary_.get(word, 0))\n",
    "                data.append(count)\n",
    "        return csr_matrix((data, (rows, cols)), shape=(len(X), self.vocabulary_size + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f93bfd23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<3x11 sparse matrix of type '<class 'numpy.intc'>'\n",
       "\twith 15 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_transformer = BagsOfWordsVectorizer(vocabulary_size=10)\n",
    "X_few_vectors = vocab_transformer.fit_transform(X_few_wordcounts)\n",
    "X_few_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "59fbbebc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 60,  19,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "       [563,  69,  12,  11,  10,   9,   7,   7,   7,   5,   5],\n",
       "       [ 30,   4,   0,   0,   0,   0,   0,   0,   0,   0,   0]],\n",
       "      dtype=int32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_few_vectors.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b6be3e6",
   "metadata": {},
   "source": [
    "What does this matrix mean? Well, the 147 in the second row, first column, means that the second email contains 147 words that are not part of the vocabulary. The 37 next to it means that the first word in the vocabulary is present 37 times in this email. The 0 next to it means that the second word is present 0 times, and so on. You can look at the vocabulary to know which words we are talking about. The first word are numbers that were replaced by the word \"NUMBER\", the second word is \"cnet\", etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "530e2929",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'NUMBER': 1,\n",
       " 'industry': 2,\n",
       " 'forum': 3,\n",
       " 'foam': 4,\n",
       " 'cpi': 5,\n",
       " 'com': 6,\n",
       " 'free': 7,\n",
       " 'cpillc': 8,\n",
       " 'test': 9,\n",
       " 'u': 10}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_transformer.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "166734e5",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c4adeb",
   "metadata": {},
   "source": [
    "**TF-IDF (Term Frequency-Inverse Document Frequency)** vectors are another commonly used technique for text representation, particularly in information retrieval and text mining tasks, including spam classification. TF-IDF takes into account not only the frequency of occurrence of words but also their importance in the context of the entire corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "285fde94",
   "metadata": {},
   "source": [
    "## Full Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f0415b",
   "metadata": {},
   "source": [
    "Once the transformers are ready we can apply these transformations to our dataset with pipelines and then train different models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4f0ff5e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "preprocessing_param_grid = {\n",
    "    \"preprocessing__data_cleaner__remove_stopwords\": [True, False],\n",
    "    \"preprocessing__data_cleaner__lemmatization\": [True, False],\n",
    "}\n",
    "\n",
    "bow_full_vectorizer = Pipeline([\n",
    "    (\"preprocessing\", preprocessing_pipeline),\n",
    "    (\"word_counter\", WordCounter()),\n",
    "    (\"bags_of_words\", BagsOfWordsVectorizer()),\n",
    "])\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "X_train_transformed_bow = bow_full_vectorizer.fit_transform(X_train)\n",
    "X_train_transformed_tfidf = tfidf_vectorizer.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "706130d3",
   "metadata": {},
   "source": [
    "We are now ready to train our first spam classifier! Let's transform the whole dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "853f00da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
      "Best bow score: 0.9810459940652819\n",
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
      "Best bow score: 0.9897255192878338\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import joblib\n",
    "\n",
    "param_grid = {'C': [0.1, 0.5, 1, 10], 'solver': ['lbfgs', 'liblinear']}\n",
    "\n",
    "lr_grid_search_bow = GridSearchCV(LogisticRegression(max_iter=5000), param_grid=param_grid, cv=5, verbose=5, n_jobs=-1)\n",
    "lr_grid_search_bow.fit(X_train_transformed_bow, y_train)\n",
    "\n",
    "lr_bow_best_score = lr_grid_search_bow.best_score_\n",
    "print(f\"Best bow score: {lr_bow_best_score}\")\n",
    "\n",
    "lr_grid_search_tfidf = GridSearchCV(LogisticRegression(max_iter=5000), param_grid=param_grid, cv=5, verbose=5, n_jobs=-1)\n",
    "lr_grid_search_tfidf.fit(X_train_transformed_tfidf, y_train)\n",
    "\n",
    "lr_tfidf_best_score = lr_grid_search_tfidf.best_score_\n",
    "print(f\"Best tfidf score: {lr_tfidf_best_score}\")\n",
    "\n",
    "if lr_bow_best_score > lr_tfidf_best_score:\n",
    "    lr_best_model = lr_grid_search_bow.best_estimator_\n",
    "    # Save the models to a file\n",
    "    joblib.dump(lr_best_model, 'bow_grid_search_model.joblib')\n",
    "else:\n",
    "    lr_best_model = lr_grid_search_tfidf.best_estimator_\n",
    "    # Save the models to a file\n",
    "    joblib.dump(lr_best_model, 'tfidf_grid_search_model.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4484638",
   "metadata": {},
   "source": [
    "## Predictions & Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2083919a",
   "metadata": {},
   "source": [
    "Let's transform the test set and use the best model make predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3a8da427",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_transformed = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "y_pred = lr_best_model.predict(X_test_transformed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c360915",
   "metadata": {},
   "source": [
    "Let's plot the confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a72f83c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3260,   60],\n",
       "       [   9, 3412]], dtype=int64)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa7f8a01",
   "metadata": {},
   "source": [
    "In the following cell we can see that the Bag of Words Vectorization technique achieves a high performance in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "748a6b1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 98.27%\n",
      "Recall: 99.74%\n",
      "F1 score: 99.00%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "print(\"Precision: {:.2f}%\".format(100 * precision_score(y_test, y_pred)))\n",
    "print(\"Recall: {:.2f}%\".format(100 * recall_score(y_test, y_pred)))\n",
    "print(\"F1 score: {:.2f}%\".format(100 * f1_score(y_test, y_pred)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
