{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5cab1260",
   "metadata": {},
   "source": [
    "# Word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f94e1c22",
   "metadata": {},
   "source": [
    "So, **bag-of-words** models may be surprisingly successful, but they are limited in what they can do. First and foremost, with bag-of-words models, words are encoded using one-hot-encoding. Instead, Word2vec, published by Google in 2013, is a neural network implementation that learns **distributed representations for words** (using vectors of real numbers). It does not need labels in order to create meaningful representations. This is useful, since most data in the real world is unlabeled. If the network is given enough training data (tens of billions of words), it produces word vectors with intriguing characteristics. Words with similar meanings appear in clusters, and clusters are spaced such that some word relationships, such as analogies, can be reproduced using vector math.\n",
    "\n",
    "\n",
    "**We'll train a Word2vec model on our IMBD dataset and then we will use its word vectors to train our ML models and make predictions.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e095b050",
   "metadata": {},
   "source": [
    "## Fetch the data\n",
    "\n",
    "The data files are located in the **data** folder. The training set contains [pos/, neg/] directories for the reviews with binary labels positive and negative. Within these directories, reviews are stored in text files named following the convention [[id]_[rating].txt] where [id] is a unique id and [rating] is the star rating for that review on a 1-10 scale. For example, the file [train/pos/200_8.txt] is the text for a positive-labeled train set example with unique id 200 and star rating 8/10 from IMDb.\n",
    "\n",
    "Let's write some functions to get and store the dato:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b9bda09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def load_imbd_dataset(data_path, unsup = False):\n",
    "    \"\"\"\n",
    "    Load the IMDb dataset into Pandas DataFrames.\n",
    "\n",
    "    Parameters:\n",
    "        data_path (str): The root directory where the IMDb dataset is stored.\n",
    "        unsup (bool): Whether the data is labeled or not\n",
    "        \n",
    "    Returns:\n",
    "        df (pandas.DataFrame): A DataFrame containing the reviews and their labels.\n",
    "    \"\"\"\n",
    "    reviews = []\n",
    "    labels = []\n",
    "    \n",
    "    if not unsup:\n",
    "        for label in ['pos', 'neg']:\n",
    "            label_dir = os.path.join(data_path, label)\n",
    "            for filename in os.listdir(label_dir):\n",
    "                filepath = os.path.join(label_dir, filename)\n",
    "                with open(filepath, 'r', encoding='utf-8') as file:\n",
    "                    review_text = file.read()\n",
    "                rating = int(filename.split('_')[1].split('.')[0])\n",
    "                sentiment = 1 if label == 'pos' else 0\n",
    "                reviews.append(review_text)\n",
    "                labels.append(sentiment)\n",
    "        df = pd.DataFrame({'review': reviews, 'sentiment': labels})\n",
    "        return df\n",
    "    else:\n",
    "        label_dir = os.path.join(data_path, 'unsup')\n",
    "        for filename in os.listdir(label_dir):\n",
    "                filepath = os.path.join(label_dir, filename)\n",
    "                with open(filepath, 'r', encoding='utf-8') as file:\n",
    "                    review_text = file.read()\n",
    "                reviews.append(review_text)\n",
    "        df = pd.DataFrame({'review': reviews})\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "80d946f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = load_imbd_dataset('data/train')\n",
    "test_set = load_imbd_dataset('data/test')\n",
    "unlabeled_train_set = load_imbd_dataset('data/train', unsup=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "54290998",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 2)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f945eb93",
   "metadata": {},
   "source": [
    "## Data Cleaning and Text Preprocessing\n",
    "We will implement all the preprocessing steps as Transformers, so then we can apply them in a preprocessing Pipeline.\n",
    "\n",
    "\n",
    "**Important:** to train Word2Vec **it is better not to remove stop words** because the algorithm relies on the broader context of the sentence in order to produce high-quality word vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b67beaf5",
   "metadata": {},
   "source": [
    "### Removing HTML Markup\n",
    "First, we'll remove the HTML tags. We will use the **Beautiful Soup** package. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b5b0fc1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class HTMLTagRemover(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        # Function to remove HTML tags from each element in the input X\n",
    "        def remove_html_tags(html_text):\n",
    "            soup = BeautifulSoup(html_text, 'html.parser')\n",
    "            return soup.get_text()\n",
    "        \n",
    "        return [remove_html_tags(text) for text in X]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3771bd6a",
   "metadata": {},
   "source": [
    "### Dealing with Punctuation, Numbers and Stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d7f6a58",
   "metadata": {},
   "source": [
    "When considering text cleaning, it is essential to tailor the approach to the specific data problem we aim to solve. For certain tasks, removing punctuation can be beneficial, but in the context of sentiment analysis, expressions like \"!!!\" or \":-(\" might contain sentiment and could be treated as words. Nevertheless, for simplicity, we will proceed with punctuation removal.\n",
    "\n",
    "Similarly, we'll exclude numbers, although alternative methods exist, such as treating them as words or substituting them with a placeholder like \"NUM.\"\n",
    "\n",
    "To execute the punctuation and number removal, we'll leverage the re package, which handles regular expressions. Additionally, we'll tokenize the reviews, breaking them down into individual words.\n",
    "\n",
    "Lastly, we must address frequently occurring words that carry little meaning, known as \"stop words\". In English, these encompass words like \"a,\" \"and,\" \"is,\" and \"the.\" Fortunately, Python packages like the Natural Language Toolkit (NLTK) provide built-in stop word lists that we can utilize by importing them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7e61a94f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Agustin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "class TextCleaner(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, remove_stopwords = False):\n",
    "        self.remove_stopwords = remove_stopwords\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        # Function to clean text (lowercase, remove numbers and punctuation)\n",
    "        def clean_text(text):\n",
    "            if isinstance(text, str):\n",
    "                text = text.lower()\n",
    "\n",
    "                # Remove numbers using regex\n",
    "                text = re.sub(r'\\d+', '', text)\n",
    "\n",
    "                # Remove punctuation using string library\n",
    "                text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "                # Split the text into words\n",
    "                words = text.split()\n",
    "                if self.remove_stopwords:\n",
    "                    # Remove stop words from \"words\"\n",
    "                    stops = set(stopwords.words(\"english\"))   \n",
    "                    words = [w for w in words if not w in stops]\n",
    "\n",
    "                # Returns a list of words\n",
    "                return words\n",
    "            \n",
    "            elif isinstance(text, list):\n",
    "                word_list = []\n",
    "                for sentence in text:\n",
    "                    sentence = sentence.lower()\n",
    "\n",
    "                    # Remove numbers using regex\n",
    "                    sentence = re.sub(r'\\d+', '', sentence)\n",
    "\n",
    "                    # Remove punctuation using string library\n",
    "                    sentence = sentence.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "                    # Split the text into words\n",
    "                    words = sentence.split()\n",
    "                    if self.remove_stopwords:\n",
    "                        # Remove stop words from \"words\"\n",
    "                        stops = set(stopwords.words(\"english\"))   \n",
    "                        words = [w for w in words if not w in stops]\n",
    "                    \n",
    "                    word_list.append(words)\n",
    "                    \n",
    "                # Returns a list of lists of words\n",
    "                return word_list\n",
    "\n",
    "        return [clean_text(text) for text in X]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a191ae",
   "metadata": {},
   "source": [
    "### Sentence Tokenizer\n",
    "\n",
    "Word2Vec expects single sentences, each one as a list of words. In other words, the input format is a list of lists.\n",
    "\n",
    "It is not at all straightforward how to split a paragraph into sentences. There are all kinds of gotchas in natural language. English sentences can end with \"?\", \"!\", \"\"\", or \".\", among other things, and spacing and capitalization are not reliable guides either. For this reason, we'll use NLTK's **punkt** tokenizer for sentence splitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "52f633ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceTokenizer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        nltk.download('punkt')\n",
    "        self.tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        def tokenize_sentence(text):\n",
    "            # Use the NLTK tokenizer to split the paragraph into sentences\n",
    "            raw_sentences = self.tokenizer.tokenize(text.strip())\n",
    "            \n",
    "            sentences = []\n",
    "            for raw_sentence in raw_sentences:\n",
    "                if len(raw_sentence) > 0:\n",
    "                    sentences.append(raw_sentence)\n",
    "                    \n",
    "            return sentences\n",
    "                    \n",
    "        return [tokenize_sentence(text) for text in X]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fbe47a1",
   "metadata": {},
   "source": [
    "We have to apply this transformer before applying our DataCleaner Transformer so that we obtain a list of words for each sentence in a review."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "896effc0",
   "metadata": {},
   "source": [
    "### Building the Preprocessing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "93c2c1c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Agustin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "review_to_sentences_pipeline = Pipeline([\n",
    "    ('html_tag_remover', HTMLTagRemover()),\n",
    "    ('sentence_tokenizer', SentenceTokenizer()),\n",
    "    ('text_cleaner', TextCleaner()),\n",
    "])\n",
    "\n",
    "review_to_wordlist_pipeline = Pipeline([\n",
    "    ('html_tag_remover', HTMLTagRemover()),\n",
    "    ('text_cleaner', TextCleaner()),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8702d26",
   "metadata": {},
   "source": [
    "Now we can use this pipeline to transform our training set. Let's prepare our training set and preprocess it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ae7162fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Agustin\\AppData\\Local\\Temp\\ipykernel_13052\\1586490070.py:11: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  soup = BeautifulSoup(html_text, 'html.parser')\n"
     ]
    }
   ],
   "source": [
    "X_train_labeled, y_train = train_set[\"review\"], train_set[\"sentiment\"]\n",
    "X_test, y_test = test_set[\"review\"], test_set[\"sentiment\"]\n",
    "X_train_unlabeled = unlabeled_train_set[\"review\"]\n",
    "\n",
    "X_train = pd.concat([X_train_labeled, X_train_unlabeled])\n",
    "\n",
    "# Transform our training set so we can train our Words2vec model.\n",
    "# We will later apply other transformations to our training data to extract meaninful information\n",
    "# from our W2V model.\n",
    "X_train_w2v = review_to_sentences_pipeline.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d35dbcef",
   "metadata": {},
   "source": [
    " Now we can extract the sentences of both lists into a single list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "da7937eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "812440\n"
     ]
    }
   ],
   "source": [
    "sentences = []\n",
    "for i in range(len(X_train_w2v)):\n",
    "    sentences += X_train_w2v[i]\n",
    "    \n",
    "print(len(sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee95694",
   "metadata": {},
   "source": [
    "We have around 798 thousands sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa204f1",
   "metadata": {},
   "source": [
    "## Training and Saving our Words2vec Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "134323ca",
   "metadata": {},
   "source": [
    "With the list of nicely parsed sentences, we're ready to train the model. There are a number of parameter choices that affect the run time and the quality of the final model that is produced. For details on the algorithms below, see the word2vec [API documentation](https://radimrehurek.com/gensim/models/word2vec.html) as well as the [Google documentation](https://code.google.com/archive/p/word2vec/).\n",
    "\n",
    "- **Architecture**: Architecture options are skip-gram (default) or continuous bag of words. We found that skip-gram was very slightly slower but produced better results.\n",
    "- **Training algorithm**: Hierarchical softmax (default) or negative sampling. For us, the default worked well.\n",
    "- **Downsampling of frequent words**: The Google documentation recommends values between .00001 and .001. For us, values closer 0.001 seemed to improve the accuracy of the final model.\n",
    "- **Word vector dimensionality**: More features result in longer runtimes, and often, but not always, result in better models. Reasonable values can be in the tens to hundreds; we used 300.\n",
    "- **Context / window size**: How many words of context should the training algorithm take into account? 10 seems to work well for hierarchical softmax (more is better, up to a point).\n",
    "- **Worker threads**: Number of parallel processes to run.\n",
    "- **Minimum word count**: This helps limit the size of the vocabulary to meaningful words. Any word that does not occur at least this many times across all documents is ignored. Reasonable values could be between 10 and 100. In this case, since each movie occurs 30 times, we set the minimum word count to 40, to avoid attaching too much importance to individual movie titles. This resulted in an overall vocabulary size of around 15,000 words. Higher values also help limit run time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "724a5c98",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-06 12:28:19,820 : INFO : collecting all words and their counts\n",
      "2023-08-06 12:28:19,821 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2023-08-06 12:28:19,867 : INFO : PROGRESS: at sentence #10000, processed 217680 words, keeping 18588 word types\n",
      "2023-08-06 12:28:19,917 : INFO : PROGRESS: at sentence #20000, processed 441935 words, keeping 28870 word types\n",
      "2023-08-06 12:28:19,966 : INFO : PROGRESS: at sentence #30000, processed 661959 words, keeping 36734 word types\n",
      "2023-08-06 12:28:20,013 : INFO : PROGRESS: at sentence #40000, processed 867989 words, keeping 42994 word types\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-06 12:28:20,067 : INFO : PROGRESS: at sentence #50000, processed 1084246 words, keeping 48774 word types\n",
      "2023-08-06 12:28:20,115 : INFO : PROGRESS: at sentence #60000, processed 1304628 words, keeping 54830 word types\n",
      "2023-08-06 12:28:20,168 : INFO : PROGRESS: at sentence #70000, processed 1525673 words, keeping 60382 word types\n",
      "2023-08-06 12:28:20,221 : INFO : PROGRESS: at sentence #80000, processed 1747403 words, keeping 65838 word types\n",
      "2023-08-06 12:28:20,275 : INFO : PROGRESS: at sentence #90000, processed 1958677 words, keeping 70537 word types\n",
      "2023-08-06 12:28:20,329 : INFO : PROGRESS: at sentence #100000, processed 2179880 words, keeping 75221 word types\n",
      "2023-08-06 12:28:20,385 : INFO : PROGRESS: at sentence #110000, processed 2393830 words, keeping 79727 word types\n",
      "2023-08-06 12:28:20,436 : INFO : PROGRESS: at sentence #120000, processed 2614721 words, keeping 84197 word types\n",
      "2023-08-06 12:28:20,490 : INFO : PROGRESS: at sentence #130000, processed 2830967 words, keeping 88466 word types\n",
      "2023-08-06 12:28:20,540 : INFO : PROGRESS: at sentence #140000, processed 3033948 words, keeping 92458 word types\n",
      "2023-08-06 12:28:20,590 : INFO : PROGRESS: at sentence #150000, processed 3231651 words, keeping 96204 word types\n",
      "2023-08-06 12:28:20,643 : INFO : PROGRESS: at sentence #160000, processed 3435384 words, keeping 100021 word types\n",
      "2023-08-06 12:28:20,695 : INFO : PROGRESS: at sentence #170000, processed 3633002 words, keeping 103576 word types\n",
      "2023-08-06 12:28:20,746 : INFO : PROGRESS: at sentence #180000, processed 3834663 words, keeping 106988 word types\n",
      "2023-08-06 12:28:20,796 : INFO : PROGRESS: at sentence #190000, processed 4034148 words, keeping 110009 word types\n",
      "2023-08-06 12:28:20,846 : INFO : PROGRESS: at sentence #200000, processed 4240806 words, keeping 113589 word types\n",
      "2023-08-06 12:28:20,898 : INFO : PROGRESS: at sentence #210000, processed 4440103 words, keeping 117086 word types\n",
      "2023-08-06 12:28:20,946 : INFO : PROGRESS: at sentence #220000, processed 4647601 words, keeping 120730 word types\n",
      "2023-08-06 12:28:21,001 : INFO : PROGRESS: at sentence #230000, processed 4847916 words, keeping 123948 word types\n",
      "2023-08-06 12:28:21,053 : INFO : PROGRESS: at sentence #240000, processed 5047259 words, keeping 127266 word types\n",
      "2023-08-06 12:28:21,102 : INFO : PROGRESS: at sentence #250000, processed 5248210 words, keeping 130267 word types\n",
      "2023-08-06 12:28:21,154 : INFO : PROGRESS: at sentence #260000, processed 5451470 words, keeping 133455 word types\n",
      "2023-08-06 12:28:21,203 : INFO : PROGRESS: at sentence #270000, processed 5654397 words, keeping 136487 word types\n",
      "2023-08-06 12:28:21,256 : INFO : PROGRESS: at sentence #280000, processed 5866875 words, keeping 139638 word types\n",
      "2023-08-06 12:28:21,312 : INFO : PROGRESS: at sentence #290000, processed 6081852 words, keeping 142911 word types\n",
      "2023-08-06 12:28:21,363 : INFO : PROGRESS: at sentence #300000, processed 6294969 words, keeping 146081 word types\n",
      "2023-08-06 12:28:21,414 : INFO : PROGRESS: at sentence #310000, processed 6503829 words, keeping 149280 word types\n",
      "2023-08-06 12:28:21,469 : INFO : PROGRESS: at sentence #320000, processed 6721724 words, keeping 152450 word types\n",
      "2023-08-06 12:28:21,526 : INFO : PROGRESS: at sentence #330000, processed 6938539 words, keeping 155819 word types\n",
      "2023-08-06 12:28:21,580 : INFO : PROGRESS: at sentence #340000, processed 7154833 words, keeping 158860 word types\n",
      "2023-08-06 12:28:21,640 : INFO : PROGRESS: at sentence #350000, processed 7374804 words, keeping 162034 word types\n",
      "2023-08-06 12:28:21,727 : INFO : PROGRESS: at sentence #360000, processed 7591023 words, keeping 164946 word types\n",
      "2023-08-06 12:28:21,783 : INFO : PROGRESS: at sentence #370000, processed 7813520 words, keeping 168209 word types\n",
      "2023-08-06 12:28:21,839 : INFO : PROGRESS: at sentence #380000, processed 8032701 words, keeping 171395 word types\n",
      "2023-08-06 12:28:21,895 : INFO : PROGRESS: at sentence #390000, processed 8247181 words, keeping 174377 word types\n",
      "2023-08-06 12:28:21,957 : INFO : PROGRESS: at sentence #400000, processed 8462650 words, keeping 177288 word types\n",
      "2023-08-06 12:28:22,019 : INFO : PROGRESS: at sentence #410000, processed 8675571 words, keeping 180036 word types\n",
      "2023-08-06 12:28:22,085 : INFO : PROGRESS: at sentence #420000, processed 8891347 words, keeping 182707 word types\n",
      "2023-08-06 12:28:22,139 : INFO : PROGRESS: at sentence #430000, processed 9108682 words, keeping 185692 word types\n",
      "2023-08-06 12:28:22,194 : INFO : PROGRESS: at sentence #440000, processed 9324284 words, keeping 188557 word types\n",
      "2023-08-06 12:28:22,246 : INFO : PROGRESS: at sentence #450000, processed 9536607 words, keeping 191445 word types\n",
      "2023-08-06 12:28:22,306 : INFO : PROGRESS: at sentence #460000, processed 9753882 words, keeping 194226 word types\n",
      "2023-08-06 12:28:22,362 : INFO : PROGRESS: at sentence #470000, processed 9971325 words, keeping 197068 word types\n",
      "2023-08-06 12:28:22,416 : INFO : PROGRESS: at sentence #480000, processed 10189250 words, keeping 199985 word types\n",
      "2023-08-06 12:28:22,470 : INFO : PROGRESS: at sentence #490000, processed 10405844 words, keeping 202781 word types\n",
      "2023-08-06 12:28:22,597 : INFO : PROGRESS: at sentence #500000, processed 10625659 words, keeping 205506 word types\n",
      "2023-08-06 12:28:22,656 : INFO : PROGRESS: at sentence #510000, processed 10839231 words, keeping 208314 word types\n",
      "2023-08-06 12:28:22,768 : INFO : PROGRESS: at sentence #520000, processed 11054407 words, keeping 211204 word types\n",
      "2023-08-06 12:28:22,889 : INFO : PROGRESS: at sentence #530000, processed 11268220 words, keeping 213939 word types\n",
      "2023-08-06 12:28:22,999 : INFO : PROGRESS: at sentence #540000, processed 11482933 words, keeping 216761 word types\n",
      "2023-08-06 12:28:23,088 : INFO : PROGRESS: at sentence #550000, processed 11689312 words, keeping 219218 word types\n",
      "2023-08-06 12:28:23,183 : INFO : PROGRESS: at sentence #560000, processed 11894048 words, keeping 221799 word types\n",
      "2023-08-06 12:28:23,256 : INFO : PROGRESS: at sentence #570000, processed 12091758 words, keeping 224178 word types\n",
      "2023-08-06 12:28:23,327 : INFO : PROGRESS: at sentence #580000, processed 12299067 words, keeping 226921 word types\n",
      "2023-08-06 12:28:23,393 : INFO : PROGRESS: at sentence #590000, processed 12504700 words, keeping 229440 word types\n",
      "2023-08-06 12:28:23,508 : INFO : PROGRESS: at sentence #600000, processed 12708181 words, keeping 231859 word types\n",
      "2023-08-06 12:28:23,594 : INFO : PROGRESS: at sentence #610000, processed 12911621 words, keeping 234268 word types\n",
      "2023-08-06 12:28:23,666 : INFO : PROGRESS: at sentence #620000, processed 13115763 words, keeping 236704 word types\n",
      "2023-08-06 12:28:23,761 : INFO : PROGRESS: at sentence #630000, processed 13318796 words, keeping 239134 word types\n",
      "2023-08-06 12:28:23,839 : INFO : PROGRESS: at sentence #640000, processed 13528126 words, keeping 241735 word types\n",
      "2023-08-06 12:28:23,923 : INFO : PROGRESS: at sentence #650000, processed 13730221 words, keeping 244142 word types\n",
      "2023-08-06 12:28:24,004 : INFO : PROGRESS: at sentence #660000, processed 13934864 words, keeping 246633 word types\n",
      "2023-08-06 12:28:24,069 : INFO : PROGRESS: at sentence #670000, processed 14138561 words, keeping 249090 word types\n",
      "2023-08-06 12:28:24,141 : INFO : PROGRESS: at sentence #680000, processed 14340324 words, keeping 251415 word types\n",
      "2023-08-06 12:28:24,205 : INFO : PROGRESS: at sentence #690000, processed 14540683 words, keeping 253623 word types\n",
      "2023-08-06 12:28:24,282 : INFO : PROGRESS: at sentence #700000, processed 14752062 words, keeping 256243 word types\n",
      "2023-08-06 12:28:24,348 : INFO : PROGRESS: at sentence #710000, processed 14955942 words, keeping 258765 word types\n",
      "2023-08-06 12:28:24,398 : INFO : PROGRESS: at sentence #720000, processed 15163672 words, keeping 261257 word types\n",
      "2023-08-06 12:28:24,498 : INFO : PROGRESS: at sentence #730000, processed 15371233 words, keeping 263801 word types\n",
      "2023-08-06 12:28:24,573 : INFO : PROGRESS: at sentence #740000, processed 15579812 words, keeping 266095 word types\n",
      "2023-08-06 12:28:24,652 : INFO : PROGRESS: at sentence #750000, processed 15784598 words, keeping 268487 word types\n",
      "2023-08-06 12:28:24,720 : INFO : PROGRESS: at sentence #760000, processed 15994242 words, keeping 270929 word types\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-06 12:28:24,800 : INFO : PROGRESS: at sentence #770000, processed 16208738 words, keeping 273307 word types\n",
      "2023-08-06 12:28:24,877 : INFO : PROGRESS: at sentence #780000, processed 16421644 words, keeping 275473 word types\n",
      "2023-08-06 12:28:24,952 : INFO : PROGRESS: at sentence #790000, processed 16639359 words, keeping 278120 word types\n",
      "2023-08-06 12:28:25,024 : INFO : PROGRESS: at sentence #800000, processed 16853982 words, keeping 280485 word types\n",
      "2023-08-06 12:28:25,098 : INFO : PROGRESS: at sentence #810000, processed 17076297 words, keeping 283066 word types\n",
      "2023-08-06 12:28:25,117 : INFO : collected 283624 word types from a corpus of 17125640 raw words and 812440 sentences\n",
      "2023-08-06 12:28:25,120 : INFO : Creating a fresh vocabulary\n",
      "2023-08-06 12:28:25,355 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=40 retains 16561 unique words (5.84% of original 283624, drops 267063)', 'datetime': '2023-08-06T12:28:25.355754', 'gensim': '4.3.0', 'python': '3.10.12 | packaged by Anaconda, Inc. | (main, Jul  5 2023, 19:09:20) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'prepare_vocab'}\n",
      "2023-08-06 12:28:25,355 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=40 leaves 16306732 word corpus (95.22% of original 17125640, drops 818908)', 'datetime': '2023-08-06T12:28:25.355754', 'gensim': '4.3.0', 'python': '3.10.12 | packaged by Anaconda, Inc. | (main, Jul  5 2023, 19:09:20) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'prepare_vocab'}\n",
      "2023-08-06 12:28:25,515 : INFO : deleting the raw counts dictionary of 283624 items\n",
      "2023-08-06 12:28:25,521 : INFO : sample=0.001 downsamples 48 most-common words\n",
      "2023-08-06 12:28:25,524 : INFO : Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 12098323.671008095 word corpus (74.2%% of prior 16306732)', 'datetime': '2023-08-06T12:28:25.524557', 'gensim': '4.3.0', 'python': '3.10.12 | packaged by Anaconda, Inc. | (main, Jul  5 2023, 19:09:20) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'prepare_vocab'}\n",
      "2023-08-06 12:28:25,699 : INFO : estimated required memory for 16561 words and 300 dimensions: 48026900 bytes\n",
      "2023-08-06 12:28:25,700 : INFO : resetting layer weights\n",
      "2023-08-06 12:28:25,737 : INFO : Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2023-08-06T12:28:25.737083', 'gensim': '4.3.0', 'python': '3.10.12 | packaged by Anaconda, Inc. | (main, Jul  5 2023, 19:09:20) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'build_vocab'}\n",
      "2023-08-06 12:28:25,738 : INFO : Word2Vec lifecycle event {'msg': 'training model with 4 workers on 16561 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=10 shrink_windows=True', 'datetime': '2023-08-06T12:28:25.738078', 'gensim': '4.3.0', 'python': '3.10.12 | packaged by Anaconda, Inc. | (main, Jul  5 2023, 19:09:20) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'train'}\n",
      "2023-08-06 12:28:26,759 : INFO : EPOCH 0 - PROGRESS: at 4.58% examples, 561483 words/s, in_qsize 7, out_qsize 0\n",
      "2023-08-06 12:28:27,770 : INFO : EPOCH 0 - PROGRESS: at 9.23% examples, 569127 words/s, in_qsize 7, out_qsize 0\n",
      "2023-08-06 12:28:28,777 : INFO : EPOCH 0 - PROGRESS: at 13.72% examples, 562925 words/s, in_qsize 7, out_qsize 0\n",
      "2023-08-06 12:28:29,782 : INFO : EPOCH 0 - PROGRESS: at 18.22% examples, 557200 words/s, in_qsize 7, out_qsize 0\n",
      "2023-08-06 12:28:30,789 : INFO : EPOCH 0 - PROGRESS: at 23.38% examples, 564114 words/s, in_qsize 7, out_qsize 0\n",
      "2023-08-06 12:28:31,798 : INFO : EPOCH 0 - PROGRESS: at 28.58% examples, 570753 words/s, in_qsize 7, out_qsize 0\n",
      "2023-08-06 12:28:32,805 : INFO : EPOCH 0 - PROGRESS: at 33.68% examples, 573703 words/s, in_qsize 7, out_qsize 0\n",
      "2023-08-06 12:28:33,811 : INFO : EPOCH 0 - PROGRESS: at 37.83% examples, 564809 words/s, in_qsize 7, out_qsize 0\n",
      "2023-08-06 12:28:34,816 : INFO : EPOCH 0 - PROGRESS: at 42.69% examples, 568933 words/s, in_qsize 7, out_qsize 0\n",
      "2023-08-06 12:28:35,819 : INFO : EPOCH 0 - PROGRESS: at 47.70% examples, 574510 words/s, in_qsize 7, out_qsize 0\n",
      "2023-08-06 12:28:36,841 : INFO : EPOCH 0 - PROGRESS: at 52.29% examples, 572463 words/s, in_qsize 7, out_qsize 0\n",
      "2023-08-06 12:28:37,880 : INFO : EPOCH 0 - PROGRESS: at 56.16% examples, 562867 words/s, in_qsize 7, out_qsize 0\n",
      "2023-08-06 12:28:38,907 : INFO : EPOCH 0 - PROGRESS: at 60.12% examples, 556345 words/s, in_qsize 8, out_qsize 0\n",
      "2023-08-06 12:28:39,915 : INFO : EPOCH 0 - PROGRESS: at 63.98% examples, 550494 words/s, in_qsize 7, out_qsize 0\n",
      "2023-08-06 12:28:40,931 : INFO : EPOCH 0 - PROGRESS: at 68.00% examples, 545631 words/s, in_qsize 7, out_qsize 0\n",
      "2023-08-06 12:28:41,937 : INFO : EPOCH 0 - PROGRESS: at 72.76% examples, 546320 words/s, in_qsize 7, out_qsize 0\n",
      "2023-08-06 12:28:42,946 : INFO : EPOCH 0 - PROGRESS: at 77.95% examples, 549736 words/s, in_qsize 7, out_qsize 0\n",
      "2023-08-06 12:28:43,971 : INFO : EPOCH 0 - PROGRESS: at 83.28% examples, 553409 words/s, in_qsize 7, out_qsize 0\n",
      "2023-08-06 12:28:44,980 : INFO : EPOCH 0 - PROGRESS: at 88.67% examples, 557485 words/s, in_qsize 7, out_qsize 0\n",
      "2023-08-06 12:28:45,994 : INFO : EPOCH 0 - PROGRESS: at 93.92% examples, 560574 words/s, in_qsize 7, out_qsize 0\n",
      "2023-08-06 12:28:46,997 : INFO : EPOCH 0 - PROGRESS: at 99.01% examples, 563620 words/s, in_qsize 7, out_qsize 0\n",
      "2023-08-06 12:28:47,173 : INFO : EPOCH 0: training on 17125640 raw words (12099141 effective words) took 21.4s, 564728 effective words/s\n",
      "2023-08-06 12:28:48,188 : INFO : EPOCH 1 - PROGRESS: at 4.92% examples, 606159 words/s, in_qsize 7, out_qsize 0\n",
      "2023-08-06 12:28:49,194 : INFO : EPOCH 1 - PROGRESS: at 9.68% examples, 599159 words/s, in_qsize 7, out_qsize 0\n",
      "2023-08-06 12:28:50,201 : INFO : EPOCH 1 - PROGRESS: at 14.34% examples, 589796 words/s, in_qsize 7, out_qsize 0\n",
      "2023-08-06 12:28:51,203 : INFO : EPOCH 1 - PROGRESS: at 19.63% examples, 598911 words/s, in_qsize 7, out_qsize 0\n",
      "2023-08-06 12:28:52,203 : INFO : EPOCH 1 - PROGRESS: at 24.86% examples, 601077 words/s, in_qsize 7, out_qsize 0\n",
      "2023-08-06 12:28:53,207 : INFO : EPOCH 1 - PROGRESS: at 30.26% examples, 604587 words/s, in_qsize 7, out_qsize 0\n",
      "2023-08-06 12:28:54,217 : INFO : EPOCH 1 - PROGRESS: at 35.50% examples, 607145 words/s, in_qsize 7, out_qsize 0\n",
      "2023-08-06 12:28:55,232 : INFO : EPOCH 1 - PROGRESS: at 40.43% examples, 605580 words/s, in_qsize 7, out_qsize 0\n",
      "2023-08-06 12:28:56,247 : INFO : EPOCH 1 - PROGRESS: at 45.44% examples, 606945 words/s, in_qsize 7, out_qsize 0\n",
      "2023-08-06 12:28:57,270 : INFO : EPOCH 1 - PROGRESS: at 50.57% examples, 608328 words/s, in_qsize 7, out_qsize 0\n",
      "2023-08-06 12:28:58,278 : INFO : EPOCH 1 - PROGRESS: at 55.65% examples, 609520 words/s, in_qsize 7, out_qsize 0\n",
      "2023-08-06 12:28:59,297 : INFO : EPOCH 1 - PROGRESS: at 60.61% examples, 609308 words/s, in_qsize 7, out_qsize 0\n",
      "2023-08-06 12:29:00,301 : INFO : EPOCH 1 - PROGRESS: at 65.68% examples, 610298 words/s, in_qsize 7, out_qsize 0\n",
      "2023-08-06 12:29:01,303 : INFO : EPOCH 1 - PROGRESS: at 70.97% examples, 611127 words/s, in_qsize 7, out_qsize 0\n",
      "2023-08-06 12:29:02,309 : INFO : EPOCH 1 - PROGRESS: at 76.26% examples, 611771 words/s, in_qsize 7, out_qsize 0\n",
      "2023-08-06 12:29:03,329 : INFO : EPOCH 1 - PROGRESS: at 81.65% examples, 612559 words/s, in_qsize 7, out_qsize 0\n",
      "2023-08-06 12:29:04,338 : INFO : EPOCH 1 - PROGRESS: at 86.88% examples, 612428 words/s, in_qsize 7, out_qsize 0\n",
      "2023-08-06 12:29:05,338 : INFO : EPOCH 1 - PROGRESS: at 92.04% examples, 612544 words/s, in_qsize 7, out_qsize 0\n",
      "2023-08-06 12:29:06,347 : INFO : EPOCH 1 - PROGRESS: at 97.19% examples, 613027 words/s, in_qsize 7, out_qsize 0\n",
      "2023-08-06 12:29:06,904 : INFO : EPOCH 1: training on 17125640 raw words (12098235 effective words) took 19.7s, 613358 effective words/s\n",
      "2023-08-06 12:29:07,929 : INFO : EPOCH 2 - PROGRESS: at 4.75% examples, 582500 words/s, in_qsize 7, out_qsize 0\n",
      "2023-08-06 12:29:08,929 : INFO : EPOCH 2 - PROGRESS: at 9.46% examples, 585904 words/s, in_qsize 7, out_qsize 0\n",
      "2023-08-06 12:29:09,930 : INFO : EPOCH 2 - PROGRESS: at 14.40% examples, 593691 words/s, in_qsize 7, out_qsize 0\n",
      "2023-08-06 12:29:10,935 : INFO : EPOCH 2 - PROGRESS: at 19.63% examples, 599861 words/s, in_qsize 7, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-06 12:29:11,945 : INFO : EPOCH 2 - PROGRESS: at 24.80% examples, 599319 words/s, in_qsize 7, out_qsize 0\n",
      "2023-08-06 12:29:12,946 : INFO : EPOCH 2 - PROGRESS: at 30.14% examples, 602164 words/s, in_qsize 7, out_qsize 0\n",
      "2023-08-06 12:29:13,952 : INFO : EPOCH 2 - PROGRESS: at 35.19% examples, 601614 words/s, in_qsize 7, out_qsize 0\n",
      "2023-08-06 12:29:14,976 : INFO : EPOCH 2 - PROGRESS: at 40.19% examples, 601729 words/s, in_qsize 7, out_qsize 0\n",
      "2023-08-06 12:29:15,981 : INFO : EPOCH 2 - PROGRESS: at 45.28% examples, 604908 words/s, in_qsize 7, out_qsize 0\n",
      "2023-08-06 12:29:16,991 : INFO : EPOCH 2 - PROGRESS: at 50.29% examples, 605885 words/s, in_qsize 7, out_qsize 0\n",
      "2023-08-06 12:29:17,991 : INFO : EPOCH 2 - PROGRESS: at 55.25% examples, 606494 words/s, in_qsize 7, out_qsize 0\n",
      "2023-08-06 12:29:18,995 : INFO : EPOCH 2 - PROGRESS: at 60.29% examples, 607912 words/s, in_qsize 7, out_qsize 0\n",
      "2023-08-06 12:29:19,999 : INFO : EPOCH 2 - PROGRESS: at 65.24% examples, 607945 words/s, in_qsize 7, out_qsize 0\n",
      "2023-08-06 12:29:21,017 : INFO : EPOCH 2 - PROGRESS: at 70.44% examples, 607672 words/s, in_qsize 7, out_qsize 0\n",
      "2023-08-06 12:29:22,024 : INFO : EPOCH 2 - PROGRESS: at 74.33% examples, 597670 words/s, in_qsize 7, out_qsize 0\n",
      "2023-08-06 12:29:23,062 : INFO : EPOCH 2 - PROGRESS: at 78.28% examples, 588212 words/s, in_qsize 6, out_qsize 1\n",
      "2023-08-06 12:29:24,081 : INFO : EPOCH 2 - PROGRESS: at 82.20% examples, 580091 words/s, in_qsize 7, out_qsize 0\n",
      "2023-08-06 12:29:25,091 : INFO : EPOCH 2 - PROGRESS: at 87.32% examples, 580930 words/s, in_qsize 7, out_qsize 0\n",
      "2023-08-06 12:29:26,094 : INFO : EPOCH 2 - PROGRESS: at 92.47% examples, 582652 words/s, in_qsize 7, out_qsize 0\n",
      "2023-08-06 12:29:27,110 : INFO : EPOCH 2 - PROGRESS: at 97.53% examples, 584024 words/s, in_qsize 7, out_qsize 0\n",
      "2023-08-06 12:29:27,579 : INFO : EPOCH 2: training on 17125640 raw words (12098341 effective words) took 20.7s, 585563 effective words/s\n",
      "2023-08-06 12:29:28,598 : INFO : EPOCH 3 - PROGRESS: at 4.87% examples, 597862 words/s, in_qsize 7, out_qsize 0\n",
      "2023-08-06 12:29:29,607 : INFO : EPOCH 3 - PROGRESS: at 9.78% examples, 604371 words/s, in_qsize 7, out_qsize 0\n",
      "2023-08-06 12:29:30,634 : INFO : EPOCH 3 - PROGRESS: at 14.83% examples, 605659 words/s, in_qsize 7, out_qsize 0\n",
      "2023-08-06 12:29:31,636 : INFO : EPOCH 3 - PROGRESS: at 20.11% examples, 609451 words/s, in_qsize 6, out_qsize 1\n",
      "2023-08-06 12:29:32,647 : INFO : EPOCH 3 - PROGRESS: at 25.49% examples, 611014 words/s, in_qsize 7, out_qsize 0\n",
      "2023-08-06 12:29:33,659 : INFO : EPOCH 3 - PROGRESS: at 30.84% examples, 611967 words/s, in_qsize 7, out_qsize 0\n",
      "2023-08-06 12:29:34,683 : INFO : EPOCH 3 - PROGRESS: at 36.03% examples, 611203 words/s, in_qsize 7, out_qsize 0\n",
      "2023-08-06 12:29:35,683 : INFO : EPOCH 3 - PROGRESS: at 41.07% examples, 611978 words/s, in_qsize 7, out_qsize 0\n",
      "2023-08-06 12:29:36,698 : INFO : EPOCH 3 - PROGRESS: at 45.98% examples, 611065 words/s, in_qsize 7, out_qsize 0\n",
      "2023-08-06 12:29:37,699 : INFO : EPOCH 3 - PROGRESS: at 50.68% examples, 608520 words/s, in_qsize 7, out_qsize 0\n",
      "2023-08-06 12:29:38,706 : INFO : EPOCH 3 - PROGRESS: at 55.43% examples, 605919 words/s, in_qsize 7, out_qsize 0\n",
      "2023-08-06 12:29:39,719 : INFO : EPOCH 3 - PROGRESS: at 60.29% examples, 605185 words/s, in_qsize 7, out_qsize 0\n",
      "2023-08-06 12:29:40,723 : INFO : EPOCH 3 - PROGRESS: at 65.30% examples, 606044 words/s, in_qsize 7, out_qsize 0\n",
      "2023-08-06 12:29:41,726 : INFO : EPOCH 3 - PROGRESS: at 70.56% examples, 607051 words/s, in_qsize 7, out_qsize 0\n",
      "2023-08-06 12:29:42,738 : INFO : EPOCH 3 - PROGRESS: at 75.73% examples, 606759 words/s, in_qsize 7, out_qsize 0\n",
      "2023-08-06 12:29:43,737 : INFO : EPOCH 3 - PROGRESS: at 80.93% examples, 607402 words/s, in_qsize 8, out_qsize 1\n",
      "2023-08-06 12:29:44,744 : INFO : EPOCH 3 - PROGRESS: at 86.23% examples, 608045 words/s, in_qsize 7, out_qsize 0\n",
      "2023-08-06 12:29:45,747 : INFO : EPOCH 3 - PROGRESS: at 91.39% examples, 608332 words/s, in_qsize 7, out_qsize 0\n",
      "2023-08-06 12:29:46,754 : INFO : EPOCH 3 - PROGRESS: at 96.55% examples, 609095 words/s, in_qsize 7, out_qsize 0\n",
      "2023-08-06 12:29:47,434 : INFO : EPOCH 3: training on 17125640 raw words (12098985 effective words) took 19.8s, 609681 effective words/s\n",
      "2023-08-06 12:29:48,453 : INFO : EPOCH 4 - PROGRESS: at 4.92% examples, 604005 words/s, in_qsize 7, out_qsize 0\n",
      "2023-08-06 12:29:49,487 : INFO : EPOCH 4 - PROGRESS: at 9.88% examples, 603288 words/s, in_qsize 7, out_qsize 0\n",
      "2023-08-06 12:29:50,488 : INFO : EPOCH 4 - PROGRESS: at 14.95% examples, 610042 words/s, in_qsize 7, out_qsize 0\n",
      "2023-08-06 12:29:51,491 : INFO : EPOCH 4 - PROGRESS: at 19.09% examples, 579297 words/s, in_qsize 7, out_qsize 0\n",
      "2023-08-06 12:29:52,492 : INFO : EPOCH 4 - PROGRESS: at 22.21% examples, 536021 words/s, in_qsize 7, out_qsize 0\n",
      "2023-08-06 12:29:53,500 : INFO : EPOCH 4 - PROGRESS: at 25.49% examples, 510061 words/s, in_qsize 7, out_qsize 0\n",
      "2023-08-06 12:29:54,512 : INFO : EPOCH 4 - PROGRESS: at 29.57% examples, 504266 words/s, in_qsize 7, out_qsize 0\n",
      "2023-08-06 12:29:55,538 : INFO : EPOCH 4 - PROGRESS: at 33.90% examples, 503300 words/s, in_qsize 7, out_qsize 0\n",
      "2023-08-06 12:29:56,539 : INFO : EPOCH 4 - PROGRESS: at 36.76% examples, 486568 words/s, in_qsize 7, out_qsize 0\n",
      "2023-08-06 12:29:57,571 : INFO : EPOCH 4 - PROGRESS: at 41.13% examples, 489684 words/s, in_qsize 7, out_qsize 0\n",
      "2023-08-06 12:29:58,574 : INFO : EPOCH 4 - PROGRESS: at 45.98% examples, 499895 words/s, in_qsize 7, out_qsize 0\n",
      "2023-08-06 12:29:59,585 : INFO : EPOCH 4 - PROGRESS: at 50.68% examples, 506534 words/s, in_qsize 7, out_qsize 0\n",
      "2023-08-06 12:30:00,602 : INFO : EPOCH 4 - PROGRESS: at 54.78% examples, 505990 words/s, in_qsize 7, out_qsize 0\n",
      "2023-08-06 12:30:01,617 : INFO : EPOCH 4 - PROGRESS: at 59.57% examples, 511404 words/s, in_qsize 7, out_qsize 0\n",
      "2023-08-06 12:30:02,622 : INFO : EPOCH 4 - PROGRESS: at 65.01% examples, 521971 words/s, in_qsize 7, out_qsize 0\n",
      "2023-08-06 12:30:03,624 : INFO : EPOCH 4 - PROGRESS: at 69.96% examples, 525882 words/s, in_qsize 7, out_qsize 0\n",
      "2023-08-06 12:30:04,648 : INFO : EPOCH 4 - PROGRESS: at 75.49% examples, 532498 words/s, in_qsize 8, out_qsize 0\n",
      "2023-08-06 12:30:05,661 : INFO : EPOCH 4 - PROGRESS: at 81.05% examples, 539041 words/s, in_qsize 7, out_qsize 0\n",
      "2023-08-06 12:30:06,662 : INFO : EPOCH 4 - PROGRESS: at 85.94% examples, 540808 words/s, in_qsize 7, out_qsize 0\n",
      "2023-08-06 12:30:07,684 : INFO : EPOCH 4 - PROGRESS: at 90.79% examples, 542118 words/s, in_qsize 7, out_qsize 0\n",
      "2023-08-06 12:30:08,706 : INFO : EPOCH 4 - PROGRESS: at 95.70% examples, 543925 words/s, in_qsize 7, out_qsize 0\n",
      "2023-08-06 12:30:09,709 : INFO : EPOCH 4 - PROGRESS: at 100.00% examples, 543306 words/s, in_qsize 0, out_qsize 1\n",
      "2023-08-06 12:30:09,711 : INFO : EPOCH 4: training on 17125640 raw words (12097365 effective words) took 22.3s, 543257 effective words/s\n",
      "2023-08-06 12:30:09,713 : INFO : Word2Vec lifecycle event {'msg': 'training on 85628200 raw words (60492067 effective words) took 104.0s, 581804 effective words/s', 'datetime': '2023-08-06T12:30:09.713122', 'gensim': '4.3.0', 'python': '3.10.12 | packaged by Anaconda, Inc. | (main, Jul  5 2023, 19:09:20) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'train'}\n",
      "2023-08-06 12:30:09,714 : INFO : Word2Vec lifecycle event {'params': 'Word2Vec<vocab=16561, vector_size=300, alpha=0.025>', 'datetime': '2023-08-06T12:30:09.714122', 'gensim': '4.3.0', 'python': '3.10.12 | packaged by Anaconda, Inc. | (main, Jul  5 2023, 19:09:20) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'}\n",
      "C:\\Users\\Agustin\\AppData\\Local\\Temp\\ipykernel_13052\\3060679937.py:19: DeprecationWarning: Call to deprecated `init_sims` (Gensim 4.0.0 implemented internal optimizations that make calls to init_sims() unnecessary. init_sims() is now obsoleted and will be completely removed in future versions. See https://github.com/RaRe-Technologies/gensim/wiki/Migrating-from-Gensim-3.x-to-4).\n",
      "  model.init_sims(replace=True)\n",
      "2023-08-06 12:30:09,730 : WARNING : destructive init_sims(replace=True) deprecated & no longer required for space-efficiency\n",
      "2023-08-06 12:30:09,735 : INFO : Word2Vec lifecycle event {'fname_or_handle': '300features_40minwords_10context', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2023-08-06T12:30:09.735123', 'gensim': '4.3.0', 'python': '3.10.12 | packaged by Anaconda, Inc. | (main, Jul  5 2023, 19:09:20) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'saving'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-06 12:30:09,737 : INFO : not storing attribute cum_table\n",
      "2023-08-06 12:30:09,820 : INFO : saved 300features_40minwords_10context\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "# Set values for various parameters\n",
    "num_features = 300    # Word vector dimensionality                      \n",
    "min_word_count = 40   # Minimum word count                        \n",
    "num_workers = 4       # Number of threads to run in parallel\n",
    "context = 10          # Context window size                                                                                    \n",
    "downsampling = 1e-3   # Downsample setting for frequent words\n",
    "\n",
    "# Initialize and train the model\n",
    "from gensim.models import word2vec\n",
    "print(\"Training model...\")\n",
    "model = word2vec.Word2Vec(sentences, workers=num_workers, vector_size=num_features, min_count = min_word_count, \n",
    "                          window = context, sample = downsampling)\n",
    "\n",
    "# If you don't plan to train the model any further, calling \n",
    "# init_sims will make the model much more memory-efficient.\n",
    "model.init_sims(replace=True)\n",
    "\n",
    "# It can be helpful to create a meaningful model name and \n",
    "# save the model for later use. You can load it later using Word2Vec.load()\n",
    "model_name = \"300features_40minwords_10context\"\n",
    "model.save(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa7cfe9",
   "metadata": {},
   "source": [
    "### Exploring the models results\n",
    "Let's take a look at the model we created out of our 75,000 training reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8b48f0b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kitchen\n"
     ]
    }
   ],
   "source": [
    "print(model.wv.doesnt_match(\"man woman child kitchen\".split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f219ec43",
   "metadata": {},
   "source": [
    "Our model is capable of distinguishing differences in meaning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26188876",
   "metadata": {},
   "source": [
    "## Numeric Representation of Words\n",
    "The Word2Vec model trained consists of a feature vector for each word in the vocabulary, stored in a numpy array called \"vectors\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9a1415d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(model.wv.vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "62cf70c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16561, 300)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.vectors.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e8b2d7",
   "metadata": {},
   "source": [
    "The number of rows in vectors is the number of words in the model's vocabulary, and the number of columns corresponds to the size of the feature vector, which we set before.  Setting the minimum word count to 40 gave us a total vocabulary of 16,492 words with 300 features apiece."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3623163d",
   "metadata": {},
   "source": [
    "One challenge with the IMDB dataset is the variable-length reviews. We need to find a way to take individual word vectors and transform them into a feature set that is the same length for every review."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e0b0e9",
   "metadata": {},
   "source": [
    "### Option 1: Vector Averaging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba55ccc",
   "metadata": {},
   "source": [
    "Since each word is a vector in 300-dimensional space, we can use vector operations to combine the words in each review. One method we tried was to simply average the word vectors in a given review  (for this purpose, we will remove stop words, which would just add noise)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6dfd0967",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class VectorAverageVectorizer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, w2v_model, num_features):\n",
    "        self.w2v_model = w2v_model\n",
    "        self.num_features = num_features\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        def make_feature_vector(words):\n",
    "            # Function to average all of the word vectors in a given review\n",
    "            \n",
    "            # Pre-initialize an empty numpy array (for speed)\n",
    "            featureVec = np.zeros((self.num_features,),dtype=\"float32\")\n",
    "        \n",
    "            nwords = 0.\n",
    "            \n",
    "            # Index2word is a list that contains the names of the words in \n",
    "            # the model's vocabulary. Convert it to a set, for speed \n",
    "            index2word_set = set(self.w2v_model.wv.index_to_key)\n",
    "            \n",
    "            # Loop over each word in the review and, if it is in the model's\n",
    "            # vocabulary, add its feature vector to the total\n",
    "            for word in words:\n",
    "                if word in index2word_set: \n",
    "                    nwords = nwords + 1.\n",
    "                    featureVec = np.add(featureVec,self.w2v_model.wv[word])\n",
    "                    \n",
    "            # Divide the result by the number of words to get the average\n",
    "            featureVec = np.divide(featureVec,nwords)\n",
    "            \n",
    "            return featureVec\n",
    "        \n",
    "        def getAvgFeatureVecs(reviews):\n",
    "            # Given a set of reviews (each one a list of words), calculate \n",
    "            # the average feature vector for each one and return a 2D numpy array \n",
    "            \n",
    "            counter = 0\n",
    "            \n",
    "            # Preallocate a 2D numpy array, for speed\n",
    "            reviewFeatureVecs = np.zeros((len(reviews), self.num_features),dtype=\"float32\") \n",
    "        \n",
    "            for review in reviews:\n",
    "                # Call the function (defined above) that makes average feature vectors\n",
    "                reviewFeatureVecs[counter] = make_feature_vector(review)\n",
    "                counter += 1\n",
    "                \n",
    "            return reviewFeatureVecs\n",
    "        \n",
    "        return getAvgFeatureVecs(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "73a8b0c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-06 12:30:09,919 : INFO : loading Word2Vec object from 300features_40minwords_10context\n",
      "2023-08-06 12:30:09,964 : INFO : loading wv recursively from 300features_40minwords_10context.wv.* with mmap=None\n",
      "2023-08-06 12:30:09,965 : INFO : setting ignored attribute cum_table to None\n",
      "2023-08-06 12:30:10,424 : INFO : Word2Vec lifecycle event {'fname': '300features_40minwords_10context', 'datetime': '2023-08-06T12:30:10.424665', 'gensim': '4.3.0', 'python': '3.10.12 | packaged by Anaconda, Inc. | (main, Jul  5 2023, 19:09:20) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'loaded'}\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "w2v_model = Word2Vec.load(\"300features_40minwords_10context\")\n",
    "\n",
    "vector_average_preprocessor = Pipeline([\n",
    "    ('review_to_wordlist', review_to_wordlist_pipeline),\n",
    "    ('vector_average_vectorizer', VectorAverageVectorizer(w2v_model, w2v_model.wv.vectors.shape[1])),\n",
    "]);\n",
    "\n",
    "# We will remove stopwords in this step, so we need to set this hyperparameter manually.\n",
    "vector_average_preprocessor.set_params(review_to_wordlist__text_cleaner__remove_stopwords=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "00aa4d54",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Agustin\\AppData\\Local\\Temp\\ipykernel_13052\\1586490070.py:11: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  soup = BeautifulSoup(html_text, 'html.parser')\n"
     ]
    }
   ],
   "source": [
    "X_train_transformed_vect_avrg = vector_average_preprocessor.fit_transform(X_train_labeled)\n",
    "X_test_transformed_vect_avrg = vector_average_preprocessor.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d1e026b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 300)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_transformed_vect_avrg.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "423629af",
   "metadata": {},
   "source": [
    "#### Building our classification models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "75c13dea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (20000,) - Valid shape: (5000,)\n"
     ]
    }
   ],
   "source": [
    "# Create a little validation dataset to perform EarlyStop\n",
    "X_train_transformed_vect_avrg, X_valid_transformed_vect_avrg = X_train_transformed_vect_avrg[:20000], X_train_transformed_vect_avrg[20000:]\n",
    "y_train, y_valid = y_train[:20000], y_train[20000:]\n",
    "\n",
    "print(f\"Train shape: {y_train.shape} - Valid shape: {y_valid.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf845466",
   "metadata": {},
   "source": [
    "##### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4415b899",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.83075 0.81875 0.81875 0.823   0.80375]\n",
      "0.819\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Initialize a Random Forest classifier with 100 trees\n",
    "rf_clf = RandomForestClassifier(n_estimators = 100)\n",
    "cv = cross_val_score(rf_clf, X_train_transformed_vect_avrg, y_train, cv=5)\n",
    "print(cv)\n",
    "print(cv.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89251b27",
   "metadata": {},
   "source": [
    "#### FeedForward Neural Network (FNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6bbe63f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Agustin\\AppData\\Local\\Temp\\ipykernel_13052\\3850558586.py:18: DeprecationWarning: KerasClassifier is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n",
      "  keras_clf = KerasClassifier(build_model, input_shape=X_train_transformed_vect_avrg.shape[1:])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "417/417 [==============================] - 2s 2ms/step - loss: 0.6641 - accuracy: 0.5953 - val_loss: 0.5501 - val_accuracy: 0.9542\n",
      "Epoch 2/20\n",
      "417/417 [==============================] - 1s 2ms/step - loss: 0.6274 - accuracy: 0.6729 - val_loss: 0.5165 - val_accuracy: 0.9150\n",
      "Epoch 3/20\n",
      "417/417 [==============================] - 1s 2ms/step - loss: 0.6010 - accuracy: 0.7120 - val_loss: 0.4927 - val_accuracy: 0.8938\n",
      "Epoch 4/20\n",
      "417/417 [==============================] - 1s 2ms/step - loss: 0.5800 - accuracy: 0.7302 - val_loss: 0.4862 - val_accuracy: 0.8754\n",
      "Epoch 5/20\n",
      "417/417 [==============================] - 1s 2ms/step - loss: 0.5622 - accuracy: 0.7477 - val_loss: 0.4745 - val_accuracy: 0.8676\n",
      "Epoch 6/20\n",
      "417/417 [==============================] - 1s 2ms/step - loss: 0.5469 - accuracy: 0.7598 - val_loss: 0.4651 - val_accuracy: 0.8626\n",
      "Epoch 7/20\n",
      "417/417 [==============================] - 1s 2ms/step - loss: 0.5334 - accuracy: 0.7679 - val_loss: 0.4520 - val_accuracy: 0.8630\n",
      "Epoch 8/20\n",
      "417/417 [==============================] - 1s 2ms/step - loss: 0.5214 - accuracy: 0.7754 - val_loss: 0.4452 - val_accuracy: 0.8600\n",
      "Epoch 9/20\n",
      "417/417 [==============================] - 1s 2ms/step - loss: 0.5106 - accuracy: 0.7844 - val_loss: 0.4272 - val_accuracy: 0.8676\n",
      "Epoch 10/20\n",
      "417/417 [==============================] - 1s 2ms/step - loss: 0.5009 - accuracy: 0.7869 - val_loss: 0.4261 - val_accuracy: 0.8624\n",
      "Epoch 11/20\n",
      "417/417 [==============================] - 1s 2ms/step - loss: 0.4920 - accuracy: 0.7934 - val_loss: 0.4171 - val_accuracy: 0.8648\n",
      "Epoch 12/20\n",
      "417/417 [==============================] - 1s 2ms/step - loss: 0.4838 - accuracy: 0.7969 - val_loss: 0.4049 - val_accuracy: 0.8710\n",
      "Epoch 13/20\n",
      "417/417 [==============================] - 1s 2ms/step - loss: 0.4763 - accuracy: 0.8014 - val_loss: 0.3988 - val_accuracy: 0.8716\n",
      "Epoch 14/20\n",
      "417/417 [==============================] - 1s 2ms/step - loss: 0.4695 - accuracy: 0.8033 - val_loss: 0.3943 - val_accuracy: 0.8716\n",
      "Epoch 15/20\n",
      "417/417 [==============================] - 1s 2ms/step - loss: 0.4631 - accuracy: 0.8078 - val_loss: 0.3957 - val_accuracy: 0.8666\n",
      "Epoch 16/20\n",
      "417/417 [==============================] - 1s 2ms/step - loss: 0.4573 - accuracy: 0.8103 - val_loss: 0.3883 - val_accuracy: 0.8694\n",
      "Epoch 17/20\n",
      "417/417 [==============================] - 1s 2ms/step - loss: 0.4519 - accuracy: 0.8128 - val_loss: 0.3792 - val_accuracy: 0.8726\n",
      "Epoch 18/20\n",
      "417/417 [==============================] - 1s 2ms/step - loss: 0.4469 - accuracy: 0.8137 - val_loss: 0.3809 - val_accuracy: 0.8692\n",
      "Epoch 19/20\n",
      "417/417 [==============================] - 1s 2ms/step - loss: 0.4422 - accuracy: 0.8165 - val_loss: 0.3849 - val_accuracy: 0.8644\n",
      "Epoch 20/20\n",
      "417/417 [==============================] - 1s 2ms/step - loss: 0.4380 - accuracy: 0.8181 - val_loss: 0.3707 - val_accuracy: 0.8702\n",
      "209/209 [==============================] - 1s 2ms/step - loss: 0.4867 - accuracy: 0.7821\n",
      "Epoch 1/20\n",
      "417/417 [==============================] - 2s 2ms/step - loss: 0.6679 - accuracy: 0.7046 - val_loss: 0.6418 - val_accuracy: 0.7510\n",
      "Epoch 2/20\n",
      "417/417 [==============================] - 1s 2ms/step - loss: 0.6275 - accuracy: 0.7364 - val_loss: 0.6123 - val_accuracy: 0.7474\n",
      "Epoch 3/20\n",
      "417/417 [==============================] - 1s 2ms/step - loss: 0.5979 - accuracy: 0.7490 - val_loss: 0.5841 - val_accuracy: 0.7594\n",
      "Epoch 4/20\n",
      "417/417 [==============================] - 1s 2ms/step - loss: 0.5746 - accuracy: 0.7580 - val_loss: 0.5729 - val_accuracy: 0.7532\n",
      "Epoch 5/20\n",
      "417/417 [==============================] - 1s 2ms/step - loss: 0.5555 - accuracy: 0.7664 - val_loss: 0.5456 - val_accuracy: 0.7758\n",
      "Epoch 6/20\n",
      "417/417 [==============================] - 1s 2ms/step - loss: 0.5391 - accuracy: 0.7736 - val_loss: 0.5276 - val_accuracy: 0.7866\n",
      "Epoch 7/20\n",
      "417/417 [==============================] - 1s 2ms/step - loss: 0.5250 - accuracy: 0.7796 - val_loss: 0.5208 - val_accuracy: 0.7854\n",
      "Epoch 8/20\n",
      "417/417 [==============================] - 1s 2ms/step - loss: 0.5124 - accuracy: 0.7872 - val_loss: 0.5027 - val_accuracy: 0.7970\n",
      "Epoch 9/20\n",
      "417/417 [==============================] - 1s 2ms/step - loss: 0.5013 - accuracy: 0.7916 - val_loss: 0.4879 - val_accuracy: 0.8068\n",
      "Epoch 10/20\n",
      "417/417 [==============================] - 1s 2ms/step - loss: 0.4912 - accuracy: 0.7967 - val_loss: 0.4795 - val_accuracy: 0.8106\n",
      "Epoch 11/20\n",
      "417/417 [==============================] - 1s 2ms/step - loss: 0.4821 - accuracy: 0.8024 - val_loss: 0.4659 - val_accuracy: 0.8184\n",
      "Epoch 12/20\n",
      "417/417 [==============================] - 1s 2ms/step - loss: 0.4740 - accuracy: 0.8053 - val_loss: 0.4619 - val_accuracy: 0.8180\n",
      "Epoch 13/20\n",
      "417/417 [==============================] - 1s 2ms/step - loss: 0.4665 - accuracy: 0.8096 - val_loss: 0.4551 - val_accuracy: 0.8198\n",
      "Epoch 14/20\n",
      "417/417 [==============================] - 1s 2ms/step - loss: 0.4596 - accuracy: 0.8116 - val_loss: 0.4540 - val_accuracy: 0.8182\n",
      "Epoch 15/20\n",
      "417/417 [==============================] - 1s 2ms/step - loss: 0.4533 - accuracy: 0.8142 - val_loss: 0.4457 - val_accuracy: 0.8220\n",
      "Epoch 16/20\n",
      "417/417 [==============================] - 1s 2ms/step - loss: 0.4476 - accuracy: 0.8171 - val_loss: 0.4406 - val_accuracy: 0.8220\n",
      "Epoch 17/20\n",
      "417/417 [==============================] - 1s 2ms/step - loss: 0.4422 - accuracy: 0.8198 - val_loss: 0.4338 - val_accuracy: 0.8244\n",
      "Epoch 18/20\n",
      "417/417 [==============================] - 1s 2ms/step - loss: 0.4373 - accuracy: 0.8209 - val_loss: 0.4341 - val_accuracy: 0.8224\n",
      "Epoch 19/20\n",
      "417/417 [==============================] - 1s 2ms/step - loss: 0.4327 - accuracy: 0.8230 - val_loss: 0.4225 - val_accuracy: 0.8290\n",
      "Epoch 20/20\n",
      "417/417 [==============================] - 1s 2ms/step - loss: 0.4284 - accuracy: 0.8245 - val_loss: 0.4221 - val_accuracy: 0.8272\n",
      "209/209 [==============================] - 1s 2ms/step - loss: 0.4510 - accuracy: 0.8116\n",
      "Epoch 1/20\n",
      "417/417 [==============================] - 1s 2ms/step - loss: 0.4944 - accuracy: 0.9253 - val_loss: 1.3406 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/20\n",
      "417/417 [==============================] - 1s 2ms/step - loss: 0.3037 - accuracy: 0.9375 - val_loss: 1.8225 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/20\n",
      "417/417 [==============================] - 1s 2ms/step - loss: 0.2482 - accuracy: 0.9375 - val_loss: 2.1266 - val_accuracy: 0.0000e+00\n",
      "Epoch 4/20\n",
      "417/417 [==============================] - 1s 2ms/step - loss: 0.2287 - accuracy: 0.9375 - val_loss: 2.2859 - val_accuracy: 0.0000e+00\n",
      "209/209 [==============================] - 1s 2ms/step - loss: 1.3397 - accuracy: 0.0000e+00\n",
      "Epoch 1/20\n",
      "417/417 [==============================] - 3s 6ms/step - loss: 0.4304 - accuracy: 0.7969 - val_loss: 0.4100 - val_accuracy: 0.8130\n",
      "Epoch 2/20\n",
      "417/417 [==============================] - 2s 6ms/step - loss: 0.3509 - accuracy: 0.8471 - val_loss: 0.2974 - val_accuracy: 0.8814\n",
      "Epoch 3/20\n",
      "417/417 [==============================] - 2s 4ms/step - loss: 0.3399 - accuracy: 0.8524 - val_loss: 0.4170 - val_accuracy: 0.8114\n",
      "Epoch 4/20\n",
      "417/417 [==============================] - 2s 4ms/step - loss: 0.3379 - accuracy: 0.8544 - val_loss: 0.3437 - val_accuracy: 0.8534\n",
      "Epoch 5/20\n",
      "417/417 [==============================] - 2s 4ms/step - loss: 0.3319 - accuracy: 0.8577 - val_loss: 0.3834 - val_accuracy: 0.8286\n",
      "209/209 [==============================] - 1s 3ms/step - loss: 0.3820 - accuracy: 0.8348\n",
      "Epoch 1/20\n",
      "417/417 [==============================] - 3s 4ms/step - loss: 0.4337 - accuracy: 0.8024 - val_loss: 0.3549 - val_accuracy: 0.8462\n",
      "Epoch 2/20\n",
      "417/417 [==============================] - 2s 4ms/step - loss: 0.3448 - accuracy: 0.8522 - val_loss: 0.3489 - val_accuracy: 0.8490\n",
      "Epoch 3/20\n",
      "417/417 [==============================] - 2s 4ms/step - loss: 0.3417 - accuracy: 0.8559 - val_loss: 0.2869 - val_accuracy: 0.8808\n",
      "Epoch 4/20\n",
      "417/417 [==============================] - 2s 4ms/step - loss: 0.3326 - accuracy: 0.8599 - val_loss: 0.3086 - val_accuracy: 0.8692\n",
      "Epoch 5/20\n",
      "417/417 [==============================] - 2s 4ms/step - loss: 0.3287 - accuracy: 0.8600 - val_loss: 0.3385 - val_accuracy: 0.8528\n",
      "Epoch 6/20\n",
      "417/417 [==============================] - 2s 4ms/step - loss: 0.3263 - accuracy: 0.8625 - val_loss: 0.3404 - val_accuracy: 0.8514\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "209/209 [==============================] - 2s 3ms/step - loss: 0.4017 - accuracy: 0.8252\n",
      "Epoch 1/20\n",
      "417/417 [==============================] - 3s 5ms/step - loss: 0.2051 - accuracy: 0.9400 - val_loss: 1.3017 - val_accuracy: 0.4492\n",
      "Epoch 2/20\n",
      "417/417 [==============================] - 2s 4ms/step - loss: 0.1308 - accuracy: 0.9533 - val_loss: 1.3900 - val_accuracy: 0.4238\n",
      "Epoch 3/20\n",
      "417/417 [==============================] - 2s 4ms/step - loss: 0.1243 - accuracy: 0.9564 - val_loss: 1.4523 - val_accuracy: 0.4206\n",
      "Epoch 4/20\n",
      "417/417 [==============================] - 2s 4ms/step - loss: 0.1182 - accuracy: 0.9579 - val_loss: 1.7250 - val_accuracy: 0.3620\n",
      "209/209 [==============================] - 1s 3ms/step - loss: 1.3499 - accuracy: 0.4295\n",
      "Epoch 1/20\n",
      "417/417 [==============================] - 2s 3ms/step - loss: 0.6654 - accuracy: 0.5991 - val_loss: 0.5453 - val_accuracy: 0.9636\n",
      "Epoch 2/20\n",
      "417/417 [==============================] - 1s 2ms/step - loss: 0.6286 - accuracy: 0.6693 - val_loss: 0.5130 - val_accuracy: 0.9228\n",
      "Epoch 3/20\n",
      "417/417 [==============================] - 1s 2ms/step - loss: 0.6019 - accuracy: 0.7056 - val_loss: 0.5083 - val_accuracy: 0.8784\n",
      "Epoch 4/20\n",
      "417/417 [==============================] - 1s 2ms/step - loss: 0.5806 - accuracy: 0.7316 - val_loss: 0.4852 - val_accuracy: 0.8754\n",
      "Epoch 5/20\n",
      "417/417 [==============================] - 1s 2ms/step - loss: 0.5628 - accuracy: 0.7469 - val_loss: 0.4749 - val_accuracy: 0.8670\n",
      "Epoch 6/20\n",
      "417/417 [==============================] - 1s 3ms/step - loss: 0.5473 - accuracy: 0.7595 - val_loss: 0.4521 - val_accuracy: 0.8726\n",
      "Epoch 7/20\n",
      "417/417 [==============================] - 1s 2ms/step - loss: 0.5339 - accuracy: 0.7658 - val_loss: 0.4532 - val_accuracy: 0.8636\n",
      "Epoch 8/20\n",
      "417/417 [==============================] - 1s 2ms/step - loss: 0.5219 - accuracy: 0.7748 - val_loss: 0.4471 - val_accuracy: 0.8588\n",
      "Epoch 9/20\n",
      "417/417 [==============================] - 1s 3ms/step - loss: 0.5111 - accuracy: 0.7824 - val_loss: 0.4365 - val_accuracy: 0.8592\n",
      "Epoch 10/20\n",
      "417/417 [==============================] - 1s 3ms/step - loss: 0.5013 - accuracy: 0.7873 - val_loss: 0.4211 - val_accuracy: 0.8670\n",
      "Epoch 11/20\n",
      "417/417 [==============================] - 1s 2ms/step - loss: 0.4923 - accuracy: 0.7931 - val_loss: 0.4160 - val_accuracy: 0.8652\n",
      "Epoch 12/20\n",
      "417/417 [==============================] - 1s 2ms/step - loss: 0.4842 - accuracy: 0.7967 - val_loss: 0.4083 - val_accuracy: 0.8672\n",
      "Epoch 13/20\n",
      "417/417 [==============================] - 1s 2ms/step - loss: 0.4767 - accuracy: 0.8004 - val_loss: 0.4054 - val_accuracy: 0.8660\n",
      "Epoch 14/20\n",
      "417/417 [==============================] - 1s 2ms/step - loss: 0.4699 - accuracy: 0.8042 - val_loss: 0.3948 - val_accuracy: 0.8712\n",
      "Epoch 15/20\n",
      "417/417 [==============================] - 1s 2ms/step - loss: 0.4636 - accuracy: 0.8084 - val_loss: 0.3931 - val_accuracy: 0.8688\n",
      "Epoch 16/20\n",
      "417/417 [==============================] - 1s 2ms/step - loss: 0.4578 - accuracy: 0.8105 - val_loss: 0.3865 - val_accuracy: 0.8704\n",
      "Epoch 17/20\n",
      "417/417 [==============================] - 1s 2ms/step - loss: 0.4523 - accuracy: 0.8117 - val_loss: 0.3862 - val_accuracy: 0.8682\n",
      "Epoch 18/20\n",
      "417/417 [==============================] - 1s 2ms/step - loss: 0.4473 - accuracy: 0.8136 - val_loss: 0.3814 - val_accuracy: 0.8688\n",
      "Epoch 19/20\n",
      "417/417 [==============================] - 1s 2ms/step - loss: 0.4426 - accuracy: 0.8161 - val_loss: 0.3763 - val_accuracy: 0.8694\n",
      "Epoch 20/20\n",
      "417/417 [==============================] - 1s 2ms/step - loss: 0.4383 - accuracy: 0.8171 - val_loss: 0.3704 - val_accuracy: 0.8706\n",
      "209/209 [==============================] - 1s 3ms/step - loss: 0.4880 - accuracy: 0.7823\n",
      "Epoch 1/20\n",
      "417/417 [==============================] - 3s 5ms/step - loss: 0.6679 - accuracy: 0.7115 - val_loss: 0.6376 - val_accuracy: 0.7678\n",
      "Epoch 2/20\n",
      "417/417 [==============================] - 1s 2ms/step - loss: 0.6278 - accuracy: 0.7376 - val_loss: 0.5999 - val_accuracy: 0.7726\n",
      "Epoch 3/20\n",
      "417/417 [==============================] - 1s 2ms/step - loss: 0.5983 - accuracy: 0.7478 - val_loss: 0.5815 - val_accuracy: 0.7648\n",
      "Epoch 4/20\n",
      "417/417 [==============================] - 1s 2ms/step - loss: 0.5749 - accuracy: 0.7590 - val_loss: 0.5631 - val_accuracy: 0.7682\n",
      "Epoch 5/20\n",
      "417/417 [==============================] - 1s 2ms/step - loss: 0.5556 - accuracy: 0.7666 - val_loss: 0.5457 - val_accuracy: 0.7756\n",
      "Epoch 6/20\n",
      "417/417 [==============================] - 1s 2ms/step - loss: 0.5392 - accuracy: 0.7736 - val_loss: 0.5298 - val_accuracy: 0.7826\n",
      "Epoch 7/20\n",
      "417/417 [==============================] - 1s 2ms/step - loss: 0.5249 - accuracy: 0.7799 - val_loss: 0.5161 - val_accuracy: 0.7904\n",
      "Epoch 8/20\n",
      "417/417 [==============================] - 1s 2ms/step - loss: 0.5124 - accuracy: 0.7868 - val_loss: 0.4968 - val_accuracy: 0.8056\n",
      "Epoch 9/20\n",
      "417/417 [==============================] - 1s 2ms/step - loss: 0.5011 - accuracy: 0.7922 - val_loss: 0.5075 - val_accuracy: 0.7874\n",
      "Epoch 10/20\n",
      "417/417 [==============================] - 1s 2ms/step - loss: 0.4912 - accuracy: 0.7973 - val_loss: 0.4807 - val_accuracy: 0.8094\n",
      "Epoch 11/20\n",
      "417/417 [==============================] - 1s 2ms/step - loss: 0.4821 - accuracy: 0.8018 - val_loss: 0.4740 - val_accuracy: 0.8108\n",
      "Epoch 12/20\n",
      "417/417 [==============================] - 1s 3ms/step - loss: 0.4739 - accuracy: 0.8059 - val_loss: 0.4620 - val_accuracy: 0.8182\n",
      "Epoch 13/20\n",
      "417/417 [==============================] - 1s 3ms/step - loss: 0.4664 - accuracy: 0.8088 - val_loss: 0.4609 - val_accuracy: 0.8158\n",
      "Epoch 14/20\n",
      "417/417 [==============================] - 1s 3ms/step - loss: 0.4595 - accuracy: 0.8116 - val_loss: 0.4452 - val_accuracy: 0.8236\n",
      "Epoch 15/20\n",
      "417/417 [==============================] - 1s 3ms/step - loss: 0.4533 - accuracy: 0.8144 - val_loss: 0.4487 - val_accuracy: 0.8204\n",
      "Epoch 16/20\n",
      "417/417 [==============================] - 1s 3ms/step - loss: 0.4475 - accuracy: 0.8164 - val_loss: 0.4321 - val_accuracy: 0.8286\n",
      "Epoch 17/20\n",
      "417/417 [==============================] - 1s 3ms/step - loss: 0.4422 - accuracy: 0.8196 - val_loss: 0.4337 - val_accuracy: 0.8250\n",
      "Epoch 18/20\n",
      "417/417 [==============================] - 1s 3ms/step - loss: 0.4372 - accuracy: 0.8211 - val_loss: 0.4264 - val_accuracy: 0.8286\n",
      "Epoch 19/20\n",
      "417/417 [==============================] - 1s 3ms/step - loss: 0.4326 - accuracy: 0.8229 - val_loss: 0.4178 - val_accuracy: 0.8320\n",
      "Epoch 20/20\n",
      "417/417 [==============================] - 1s 2ms/step - loss: 0.4284 - accuracy: 0.8259 - val_loss: 0.4203 - val_accuracy: 0.8286\n",
      "209/209 [==============================] - 1s 2ms/step - loss: 0.4525 - accuracy: 0.8107\n",
      "Epoch 1/20\n",
      "417/417 [==============================] - 2s 2ms/step - loss: 0.4898 - accuracy: 0.9345 - val_loss: 1.3414 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/20\n",
      "417/417 [==============================] - 1s 2ms/step - loss: 0.3023 - accuracy: 0.9375 - val_loss: 1.8216 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/20\n",
      "417/417 [==============================] - 1s 3ms/step - loss: 0.2479 - accuracy: 0.9375 - val_loss: 2.1156 - val_accuracy: 0.0000e+00\n",
      "Epoch 4/20\n",
      "417/417 [==============================] - 1s 2ms/step - loss: 0.2285 - accuracy: 0.9375 - val_loss: 2.2769 - val_accuracy: 0.0000e+00\n",
      "209/209 [==============================] - 1s 3ms/step - loss: 1.3406 - accuracy: 0.0000e+00\n",
      "Epoch 1/20\n",
      "417/417 [==============================] - 3s 5ms/step - loss: 0.5303 - accuracy: 0.7243 - val_loss: 0.2104 - val_accuracy: 0.9238\n",
      "Epoch 2/20\n",
      "417/417 [==============================] - 2s 4ms/step - loss: 0.3583 - accuracy: 0.8440 - val_loss: 0.3031 - val_accuracy: 0.8734\n",
      "Epoch 3/20\n",
      "417/417 [==============================] - 2s 5ms/step - loss: 0.3460 - accuracy: 0.8501 - val_loss: 0.3448 - val_accuracy: 0.8482\n",
      "Epoch 4/20\n",
      "417/417 [==============================] - 2s 5ms/step - loss: 0.3401 - accuracy: 0.8545 - val_loss: 0.3568 - val_accuracy: 0.8426\n",
      "209/209 [==============================] - 2s 4ms/step - loss: 0.5777 - accuracy: 0.7292\n",
      "Epoch 1/20\n",
      "417/417 [==============================] - 4s 5ms/step - loss: 0.4835 - accuracy: 0.7917 - val_loss: 0.3552 - val_accuracy: 0.8492\n",
      "Epoch 2/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "417/417 [==============================] - 2s 4ms/step - loss: 0.3502 - accuracy: 0.8513 - val_loss: 0.4247 - val_accuracy: 0.8076\n",
      "Epoch 3/20\n",
      "417/417 [==============================] - 2s 4ms/step - loss: 0.3417 - accuracy: 0.8532 - val_loss: 0.3692 - val_accuracy: 0.8386\n",
      "Epoch 4/20\n",
      "417/417 [==============================] - 2s 4ms/step - loss: 0.3384 - accuracy: 0.8561 - val_loss: 0.3777 - val_accuracy: 0.8324\n",
      "209/209 [==============================] - 1s 3ms/step - loss: 0.3697 - accuracy: 0.8505\n",
      "Epoch 1/20\n",
      "417/417 [==============================] - 2s 4ms/step - loss: 0.2490 - accuracy: 0.9369 - val_loss: 2.2217 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/20\n",
      "417/417 [==============================] - 1s 4ms/step - loss: 0.1546 - accuracy: 0.9375 - val_loss: 1.9276 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/20\n",
      "417/417 [==============================] - 2s 4ms/step - loss: 0.1334 - accuracy: 0.9474 - val_loss: 1.9950 - val_accuracy: 0.2292\n",
      "Epoch 4/20\n",
      "417/417 [==============================] - 2s 4ms/step - loss: 0.1282 - accuracy: 0.9552 - val_loss: 1.4998 - val_accuracy: 0.3778\n",
      "Epoch 5/20\n",
      "417/417 [==============================] - 1s 3ms/step - loss: 0.1239 - accuracy: 0.9561 - val_loss: 1.6024 - val_accuracy: 0.3626\n",
      "Epoch 6/20\n",
      "417/417 [==============================] - 2s 4ms/step - loss: 0.1207 - accuracy: 0.9575 - val_loss: 1.5339 - val_accuracy: 0.4284\n",
      "Epoch 7/20\n",
      "417/417 [==============================] - 1s 4ms/step - loss: 0.1195 - accuracy: 0.9594 - val_loss: 1.7469 - val_accuracy: 0.3686\n",
      "209/209 [==============================] - 1s 3ms/step - loss: 1.5504 - accuracy: 0.3589\n",
      "Epoch 1/20\n",
      "417/417 [==============================] - 3s 5ms/step - loss: 0.4971 - accuracy: 0.7739 - val_loss: 0.3044 - val_accuracy: 0.8858\n",
      "Epoch 2/20\n",
      "417/417 [==============================] - 2s 4ms/step - loss: 0.3695 - accuracy: 0.8423 - val_loss: 0.3019 - val_accuracy: 0.8820\n",
      "Epoch 3/20\n",
      "417/417 [==============================] - 1s 3ms/step - loss: 0.3519 - accuracy: 0.8496 - val_loss: 0.3325 - val_accuracy: 0.8654\n",
      "Epoch 4/20\n",
      "417/417 [==============================] - 1s 4ms/step - loss: 0.3442 - accuracy: 0.8523 - val_loss: 0.3270 - val_accuracy: 0.8660\n",
      "Epoch 5/20\n",
      "417/417 [==============================] - 1s 3ms/step - loss: 0.3407 - accuracy: 0.8535 - val_loss: 0.3026 - val_accuracy: 0.8762\n",
      "209/209 [==============================] - 1s 3ms/step - loss: 0.4018 - accuracy: 0.8218\n",
      "Epoch 1/20\n",
      "417/417 [==============================] - 2s 4ms/step - loss: 0.4868 - accuracy: 0.7856 - val_loss: 0.3666 - val_accuracy: 0.8482\n",
      "Epoch 2/20\n",
      "417/417 [==============================] - 1s 3ms/step - loss: 0.3620 - accuracy: 0.8464 - val_loss: 0.2789 - val_accuracy: 0.8940\n",
      "Epoch 3/20\n",
      "417/417 [==============================] - 1s 4ms/step - loss: 0.3460 - accuracy: 0.8549 - val_loss: 0.3716 - val_accuracy: 0.8380\n",
      "Epoch 4/20\n",
      "417/417 [==============================] - 1s 3ms/step - loss: 0.3400 - accuracy: 0.8558 - val_loss: 0.3741 - val_accuracy: 0.8374\n",
      "Epoch 5/20\n",
      "417/417 [==============================] - 1s 3ms/step - loss: 0.3370 - accuracy: 0.8556 - val_loss: 0.3454 - val_accuracy: 0.8536\n",
      "209/209 [==============================] - 1s 3ms/step - loss: 0.4326 - accuracy: 0.8064\n",
      "Epoch 1/20\n",
      "417/417 [==============================] - 2s 4ms/step - loss: 0.2474 - accuracy: 0.9358 - val_loss: 2.2436 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/20\n",
      "417/417 [==============================] - 1s 4ms/step - loss: 0.1734 - accuracy: 0.9375 - val_loss: 2.0208 - val_accuracy: 6.0000e-04\n",
      "Epoch 3/20\n",
      "417/417 [==============================] - 1s 3ms/step - loss: 0.1494 - accuracy: 0.9456 - val_loss: 1.6285 - val_accuracy: 0.2218\n",
      "Epoch 4/20\n",
      "417/417 [==============================] - 1s 3ms/step - loss: 0.1363 - accuracy: 0.9513 - val_loss: 1.4886 - val_accuracy: 0.3088\n",
      "Epoch 5/20\n",
      "417/417 [==============================] - 1s 3ms/step - loss: 0.1287 - accuracy: 0.9528 - val_loss: 1.5020 - val_accuracy: 0.3396\n",
      "Epoch 6/20\n",
      "417/417 [==============================] - 1s 3ms/step - loss: 0.1244 - accuracy: 0.9546 - val_loss: 1.5281 - val_accuracy: 0.3538\n",
      "Epoch 7/20\n",
      "417/417 [==============================] - 1s 3ms/step - loss: 0.1221 - accuracy: 0.9548 - val_loss: 1.4320 - val_accuracy: 0.3910\n",
      "Epoch 8/20\n",
      "417/417 [==============================] - 1s 3ms/step - loss: 0.1209 - accuracy: 0.9555 - val_loss: 1.3493 - val_accuracy: 0.4362\n",
      "Epoch 9/20\n",
      "417/417 [==============================] - 1s 3ms/step - loss: 0.1193 - accuracy: 0.9563 - val_loss: 1.4631 - val_accuracy: 0.4062\n",
      "Epoch 10/20\n",
      "417/417 [==============================] - 1s 3ms/step - loss: 0.1179 - accuracy: 0.9568 - val_loss: 1.4080 - val_accuracy: 0.4262\n",
      "Epoch 11/20\n",
      "417/417 [==============================] - 1s 3ms/step - loss: 0.1168 - accuracy: 0.9576 - val_loss: 1.4532 - val_accuracy: 0.4146\n",
      "209/209 [==============================] - 2s 3ms/step - loss: 1.3988 - accuracy: 0.4191\n",
      "Epoch 1/20\n",
      "417/417 [==============================] - 4s 5ms/step - loss: 0.4257 - accuracy: 0.7973 - val_loss: 0.2620 - val_accuracy: 0.8926\n",
      "Epoch 2/20\n",
      "417/417 [==============================] - 2s 4ms/step - loss: 0.3493 - accuracy: 0.8483 - val_loss: 0.3632 - val_accuracy: 0.8416\n",
      "Epoch 3/20\n",
      "417/417 [==============================] - 2s 4ms/step - loss: 0.3403 - accuracy: 0.8500 - val_loss: 0.3131 - val_accuracy: 0.8658\n",
      "Epoch 4/20\n",
      "417/417 [==============================] - 2s 4ms/step - loss: 0.3366 - accuracy: 0.8551 - val_loss: 0.2732 - val_accuracy: 0.8894\n",
      "209/209 [==============================] - 1s 3ms/step - loss: 0.4341 - accuracy: 0.8141\n",
      "Epoch 1/20\n",
      "417/417 [==============================] - 3s 6ms/step - loss: 0.4237 - accuracy: 0.8096 - val_loss: 0.3395 - val_accuracy: 0.8588\n",
      "Epoch 2/20\n",
      "417/417 [==============================] - 2s 4ms/step - loss: 0.3467 - accuracy: 0.8507 - val_loss: 0.3843 - val_accuracy: 0.8336\n",
      "Epoch 3/20\n",
      "417/417 [==============================] - 2s 4ms/step - loss: 0.3356 - accuracy: 0.8573 - val_loss: 0.2113 - val_accuracy: 0.9202\n",
      "Epoch 4/20\n",
      "417/417 [==============================] - 2s 4ms/step - loss: 0.3313 - accuracy: 0.8615 - val_loss: 0.2920 - val_accuracy: 0.8806\n",
      "Epoch 5/20\n",
      "417/417 [==============================] - 2s 4ms/step - loss: 0.3268 - accuracy: 0.8645 - val_loss: 0.3934 - val_accuracy: 0.8274\n",
      "Epoch 6/20\n",
      "417/417 [==============================] - 2s 4ms/step - loss: 0.3221 - accuracy: 0.8630 - val_loss: 0.3155 - val_accuracy: 0.8688\n",
      "209/209 [==============================] - 1s 3ms/step - loss: 0.4998 - accuracy: 0.7702\n",
      "Epoch 1/20\n",
      "417/417 [==============================] - 3s 4ms/step - loss: 0.1994 - accuracy: 0.9375 - val_loss: 1.7918 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/20\n",
      "417/417 [==============================] - 2s 4ms/step - loss: 0.1329 - accuracy: 0.9484 - val_loss: 2.0412 - val_accuracy: 0.2294\n",
      "Epoch 3/20\n",
      "417/417 [==============================] - 2s 4ms/step - loss: 0.1276 - accuracy: 0.9554 - val_loss: 1.3429 - val_accuracy: 0.4656\n",
      "Epoch 4/20\n",
      "417/417 [==============================] - 2s 4ms/step - loss: 0.1228 - accuracy: 0.9570 - val_loss: 1.4144 - val_accuracy: 0.4694\n",
      "Epoch 5/20\n",
      "417/417 [==============================] - 2s 4ms/step - loss: 0.1167 - accuracy: 0.9588 - val_loss: 1.5345 - val_accuracy: 0.4322\n",
      "Epoch 6/20\n",
      "417/417 [==============================] - 2s 4ms/step - loss: 0.1122 - accuracy: 0.9610 - val_loss: 1.5736 - val_accuracy: 0.4442\n",
      "209/209 [==============================] - 1s 3ms/step - loss: 1.3963 - accuracy: 0.4504\n",
      "Epoch 1/20\n",
      "417/417 [==============================] - 4s 6ms/step - loss: 0.4453 - accuracy: 0.7974 - val_loss: 0.3428 - val_accuracy: 0.8594\n",
      "Epoch 2/20\n",
      "417/417 [==============================] - 2s 5ms/step - loss: 0.3505 - accuracy: 0.8488 - val_loss: 0.3183 - val_accuracy: 0.8656\n",
      "Epoch 3/20\n",
      "417/417 [==============================] - 2s 5ms/step - loss: 0.3405 - accuracy: 0.8525 - val_loss: 0.3775 - val_accuracy: 0.8334\n",
      "Epoch 4/20\n",
      "417/417 [==============================] - 1s 3ms/step - loss: 0.3369 - accuracy: 0.8549 - val_loss: 0.2744 - val_accuracy: 0.8878\n",
      "Epoch 5/20\n",
      "417/417 [==============================] - 1s 4ms/step - loss: 0.3305 - accuracy: 0.8562 - val_loss: 0.2021 - val_accuracy: 0.9182\n",
      "Epoch 6/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "417/417 [==============================] - 2s 4ms/step - loss: 0.3258 - accuracy: 0.8591 - val_loss: 0.2485 - val_accuracy: 0.8966\n",
      "Epoch 7/20\n",
      "417/417 [==============================] - 1s 3ms/step - loss: 0.3219 - accuracy: 0.8612 - val_loss: 0.2909 - val_accuracy: 0.8740\n",
      "Epoch 8/20\n",
      "417/417 [==============================] - 2s 4ms/step - loss: 0.3189 - accuracy: 0.8657 - val_loss: 0.3095 - val_accuracy: 0.8664\n",
      "209/209 [==============================] - 2s 4ms/step - loss: 0.5243 - accuracy: 0.7688\n",
      "Epoch 1/20\n",
      "417/417 [==============================] - 3s 5ms/step - loss: 0.4360 - accuracy: 0.8124 - val_loss: 0.3376 - val_accuracy: 0.8600\n",
      "Epoch 2/20\n",
      "417/417 [==============================] - 2s 4ms/step - loss: 0.3441 - accuracy: 0.8515 - val_loss: 0.3930 - val_accuracy: 0.8248\n",
      "Epoch 3/20\n",
      "417/417 [==============================] - 2s 4ms/step - loss: 0.3370 - accuracy: 0.8584 - val_loss: 0.2273 - val_accuracy: 0.9150\n",
      "Epoch 4/20\n",
      "417/417 [==============================] - 2s 4ms/step - loss: 0.3328 - accuracy: 0.8588 - val_loss: 0.2771 - val_accuracy: 0.8884\n",
      "Epoch 5/20\n",
      "417/417 [==============================] - 2s 4ms/step - loss: 0.3283 - accuracy: 0.8610 - val_loss: 0.2238 - val_accuracy: 0.9162\n",
      "Epoch 6/20\n",
      "417/417 [==============================] - 2s 4ms/step - loss: 0.3255 - accuracy: 0.8595 - val_loss: 0.3176 - val_accuracy: 0.8678\n",
      "Epoch 7/20\n",
      "417/417 [==============================] - 2s 4ms/step - loss: 0.3246 - accuracy: 0.8634 - val_loss: 0.3145 - val_accuracy: 0.8682\n",
      "Epoch 8/20\n",
      "417/417 [==============================] - 2s 5ms/step - loss: 0.3193 - accuracy: 0.8667 - val_loss: 0.2859 - val_accuracy: 0.8800\n",
      "209/209 [==============================] - 2s 4ms/step - loss: 0.4672 - accuracy: 0.7862\n",
      "Epoch 1/20\n",
      "417/417 [==============================] - 3s 6ms/step - loss: 0.2190 - accuracy: 0.9375 - val_loss: 2.0485 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/20\n",
      "417/417 [==============================] - 2s 6ms/step - loss: 0.1393 - accuracy: 0.9479 - val_loss: 1.5233 - val_accuracy: 0.3368\n",
      "Epoch 3/20\n",
      "417/417 [==============================] - 2s 5ms/step - loss: 0.1252 - accuracy: 0.9549 - val_loss: 1.3727 - val_accuracy: 0.4524\n",
      "Epoch 4/20\n",
      "417/417 [==============================] - 2s 4ms/step - loss: 0.1200 - accuracy: 0.9572 - val_loss: 1.5457 - val_accuracy: 0.4102\n",
      "Epoch 5/20\n",
      "417/417 [==============================] - 2s 5ms/step - loss: 0.1170 - accuracy: 0.9583 - val_loss: 1.7817 - val_accuracy: 0.3310\n",
      "Epoch 6/20\n",
      "417/417 [==============================] - 2s 5ms/step - loss: 0.1134 - accuracy: 0.9597 - val_loss: 1.3319 - val_accuracy: 0.4848\n",
      "Epoch 7/20\n",
      "417/417 [==============================] - 2s 5ms/step - loss: 0.1122 - accuracy: 0.9595 - val_loss: 1.6514 - val_accuracy: 0.3972\n",
      "Epoch 8/20\n",
      "417/417 [==============================] - 2s 4ms/step - loss: 0.1090 - accuracy: 0.9618 - val_loss: 1.6695 - val_accuracy: 0.4024\n",
      "Epoch 9/20\n",
      "417/417 [==============================] - 2s 4ms/step - loss: 0.1079 - accuracy: 0.9615 - val_loss: 1.5138 - val_accuracy: 0.4390\n",
      "209/209 [==============================] - 1s 3ms/step - loss: 1.3964 - accuracy: 0.4722\n",
      "Epoch 1/20\n",
      "417/417 [==============================] - 3s 4ms/step - loss: 0.4295 - accuracy: 0.8018 - val_loss: 0.3208 - val_accuracy: 0.8656\n",
      "Epoch 2/20\n",
      "417/417 [==============================] - 2s 4ms/step - loss: 0.3484 - accuracy: 0.8483 - val_loss: 0.3269 - val_accuracy: 0.8670\n",
      "Epoch 3/20\n",
      "417/417 [==============================] - 2s 4ms/step - loss: 0.3406 - accuracy: 0.8507 - val_loss: 0.2997 - val_accuracy: 0.8744\n",
      "Epoch 4/20\n",
      "417/417 [==============================] - 2s 4ms/step - loss: 0.3368 - accuracy: 0.8551 - val_loss: 0.3153 - val_accuracy: 0.8648\n",
      "Epoch 5/20\n",
      "417/417 [==============================] - 2s 4ms/step - loss: 0.3310 - accuracy: 0.8559 - val_loss: 0.2860 - val_accuracy: 0.8802\n",
      "Epoch 6/20\n",
      "417/417 [==============================] - 2s 4ms/step - loss: 0.3279 - accuracy: 0.8588 - val_loss: 0.2303 - val_accuracy: 0.9068\n",
      "Epoch 7/20\n",
      "417/417 [==============================] - 2s 4ms/step - loss: 0.3229 - accuracy: 0.8625 - val_loss: 0.2750 - val_accuracy: 0.8870\n",
      "Epoch 8/20\n",
      "417/417 [==============================] - 2s 4ms/step - loss: 0.3224 - accuracy: 0.8602 - val_loss: 0.2640 - val_accuracy: 0.8916\n",
      "Epoch 9/20\n",
      "417/417 [==============================] - 2s 4ms/step - loss: 0.3153 - accuracy: 0.8632 - val_loss: 0.4140 - val_accuracy: 0.8118\n",
      "209/209 [==============================] - 1s 3ms/step - loss: 0.4651 - accuracy: 0.8018\n",
      "Epoch 1/20\n",
      "417/417 [==============================] - 3s 5ms/step - loss: 0.4165 - accuracy: 0.8144 - val_loss: 0.3117 - val_accuracy: 0.8770\n",
      "Epoch 2/20\n",
      "417/417 [==============================] - 2s 4ms/step - loss: 0.3445 - accuracy: 0.8531 - val_loss: 0.2889 - val_accuracy: 0.8844\n",
      "Epoch 3/20\n",
      "417/417 [==============================] - 2s 4ms/step - loss: 0.3368 - accuracy: 0.8573 - val_loss: 0.3028 - val_accuracy: 0.8728\n",
      "Epoch 4/20\n",
      "417/417 [==============================] - 2s 4ms/step - loss: 0.3302 - accuracy: 0.8588 - val_loss: 0.2367 - val_accuracy: 0.9098\n",
      "Epoch 5/20\n",
      "417/417 [==============================] - 2s 4ms/step - loss: 0.3266 - accuracy: 0.8617 - val_loss: 0.3468 - val_accuracy: 0.8514\n",
      "Epoch 6/20\n",
      "417/417 [==============================] - 2s 4ms/step - loss: 0.3223 - accuracy: 0.8626 - val_loss: 0.3738 - val_accuracy: 0.8370\n",
      "Epoch 7/20\n",
      "417/417 [==============================] - 2s 4ms/step - loss: 0.3169 - accuracy: 0.8675 - val_loss: 0.2407 - val_accuracy: 0.9048\n",
      "209/209 [==============================] - 2s 3ms/step - loss: 0.4588 - accuracy: 0.7944\n",
      "Epoch 1/20\n",
      "417/417 [==============================] - 2s 4ms/step - loss: 0.2045 - accuracy: 0.9390 - val_loss: 1.6696 - val_accuracy: 0.2402\n",
      "Epoch 2/20\n",
      "417/417 [==============================] - 1s 4ms/step - loss: 0.1312 - accuracy: 0.9543 - val_loss: 1.0877 - val_accuracy: 0.5212\n",
      "Epoch 3/20\n",
      "417/417 [==============================] - 2s 4ms/step - loss: 0.1233 - accuracy: 0.9575 - val_loss: 1.8659 - val_accuracy: 0.3124\n",
      "Epoch 4/20\n",
      "417/417 [==============================] - 2s 4ms/step - loss: 0.1182 - accuracy: 0.9566 - val_loss: 1.3663 - val_accuracy: 0.4284\n",
      "Epoch 5/20\n",
      "417/417 [==============================] - 1s 4ms/step - loss: 0.1153 - accuracy: 0.9595 - val_loss: 1.3732 - val_accuracy: 0.4556\n",
      "209/209 [==============================] - 1s 3ms/step - loss: 1.1356 - accuracy: 0.5077\n",
      "Epoch 1/20\n",
      "417/417 [==============================] - 2s 4ms/step - loss: 0.5220 - accuracy: 0.7504 - val_loss: 0.3190 - val_accuracy: 0.8774\n",
      "Epoch 2/20\n",
      "417/417 [==============================] - 2s 4ms/step - loss: 0.3668 - accuracy: 0.8426 - val_loss: 0.3519 - val_accuracy: 0.8546\n",
      "Epoch 3/20\n",
      "417/417 [==============================] - 1s 4ms/step - loss: 0.3478 - accuracy: 0.8516 - val_loss: 0.2849 - val_accuracy: 0.8866\n",
      "Epoch 4/20\n",
      "417/417 [==============================] - 1s 4ms/step - loss: 0.3433 - accuracy: 0.8513 - val_loss: 0.2730 - val_accuracy: 0.8894\n",
      "Epoch 5/20\n",
      "417/417 [==============================] - 1s 3ms/step - loss: 0.3389 - accuracy: 0.8518 - val_loss: 0.2982 - val_accuracy: 0.8790\n",
      "Epoch 6/20\n",
      "417/417 [==============================] - 2s 4ms/step - loss: 0.3373 - accuracy: 0.8544 - val_loss: 0.2980 - val_accuracy: 0.8790\n",
      "Epoch 7/20\n",
      "417/417 [==============================] - 2s 4ms/step - loss: 0.3348 - accuracy: 0.8561 - val_loss: 0.2528 - val_accuracy: 0.8984\n",
      "Epoch 8/20\n",
      "417/417 [==============================] - 1s 3ms/step - loss: 0.3338 - accuracy: 0.8546 - val_loss: 0.3039 - val_accuracy: 0.8736\n",
      "Epoch 9/20\n",
      "417/417 [==============================] - 1s 3ms/step - loss: 0.3300 - accuracy: 0.8553 - val_loss: 0.2981 - val_accuracy: 0.8780\n",
      "Epoch 10/20\n",
      "417/417 [==============================] - 1s 3ms/step - loss: 0.3291 - accuracy: 0.8573 - val_loss: 0.2525 - val_accuracy: 0.8984\n",
      "Epoch 11/20\n",
      "417/417 [==============================] - 1s 3ms/step - loss: 0.3274 - accuracy: 0.8573 - val_loss: 0.3142 - val_accuracy: 0.8682\n",
      "Epoch 12/20\n",
      "417/417 [==============================] - 1s 4ms/step - loss: 0.3270 - accuracy: 0.8589 - val_loss: 0.2428 - val_accuracy: 0.9020\n",
      "Epoch 13/20\n",
      "417/417 [==============================] - 1s 3ms/step - loss: 0.3252 - accuracy: 0.8579 - val_loss: 0.3255 - val_accuracy: 0.8612\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/20\n",
      "417/417 [==============================] - 1s 3ms/step - loss: 0.3246 - accuracy: 0.8604 - val_loss: 0.3026 - val_accuracy: 0.8732\n",
      "Epoch 15/20\n",
      "417/417 [==============================] - 1s 3ms/step - loss: 0.3233 - accuracy: 0.8600 - val_loss: 0.3529 - val_accuracy: 0.8444\n",
      "209/209 [==============================] - 1s 3ms/step - loss: 0.4462 - accuracy: 0.8027\n",
      "Epoch 1/20\n",
      "417/417 [==============================] - 2s 4ms/step - loss: 0.5153 - accuracy: 0.7906 - val_loss: 0.3678 - val_accuracy: 0.8484\n",
      "Epoch 2/20\n",
      "417/417 [==============================] - 1s 3ms/step - loss: 0.3604 - accuracy: 0.8458 - val_loss: 0.3181 - val_accuracy: 0.8698\n",
      "Epoch 3/20\n",
      "417/417 [==============================] - 1s 3ms/step - loss: 0.3450 - accuracy: 0.8546 - val_loss: 0.3104 - val_accuracy: 0.8726\n",
      "Epoch 4/20\n",
      "417/417 [==============================] - 2s 4ms/step - loss: 0.3391 - accuracy: 0.8551 - val_loss: 0.2710 - val_accuracy: 0.8916\n",
      "Epoch 5/20\n",
      "417/417 [==============================] - 2s 4ms/step - loss: 0.3362 - accuracy: 0.8586 - val_loss: 0.3156 - val_accuracy: 0.8680\n",
      "Epoch 6/20\n",
      "417/417 [==============================] - 2s 5ms/step - loss: 0.3334 - accuracy: 0.8588 - val_loss: 0.3737 - val_accuracy: 0.8368\n",
      "Epoch 7/20\n",
      "417/417 [==============================] - 2s 5ms/step - loss: 0.3320 - accuracy: 0.8571 - val_loss: 0.2864 - val_accuracy: 0.8820\n",
      "209/209 [==============================] - 1s 3ms/step - loss: 0.4166 - accuracy: 0.8183\n",
      "Epoch 1/20\n",
      "417/417 [==============================] - 3s 5ms/step - loss: 0.3263 - accuracy: 0.9337 - val_loss: 2.3486 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/20\n",
      "417/417 [==============================] - 2s 4ms/step - loss: 0.1858 - accuracy: 0.9381 - val_loss: 1.8462 - val_accuracy: 0.0352\n",
      "Epoch 3/20\n",
      "417/417 [==============================] - 1s 3ms/step - loss: 0.1535 - accuracy: 0.9441 - val_loss: 1.6149 - val_accuracy: 0.2208\n",
      "Epoch 4/20\n",
      "417/417 [==============================] - 1s 3ms/step - loss: 0.1375 - accuracy: 0.9507 - val_loss: 1.5217 - val_accuracy: 0.2988\n",
      "Epoch 5/20\n",
      "417/417 [==============================] - 2s 4ms/step - loss: 0.1299 - accuracy: 0.9528 - val_loss: 1.4483 - val_accuracy: 0.3596\n",
      "Epoch 6/20\n",
      "417/417 [==============================] - 1s 3ms/step - loss: 0.1260 - accuracy: 0.9539 - val_loss: 1.5266 - val_accuracy: 0.3552\n",
      "Epoch 7/20\n",
      "417/417 [==============================] - 1s 4ms/step - loss: 0.1228 - accuracy: 0.9543 - val_loss: 1.5511 - val_accuracy: 0.3536\n",
      "Epoch 8/20\n",
      "417/417 [==============================] - 2s 4ms/step - loss: 0.1213 - accuracy: 0.9562 - val_loss: 1.5070 - val_accuracy: 0.3736\n",
      "209/209 [==============================] - 1s 3ms/step - loss: 1.4947 - accuracy: 0.3417\n",
      "Epoch 1/20\n",
      "417/417 [==============================] - 3s 4ms/step - loss: 0.5296 - accuracy: 0.7424 - val_loss: 0.3151 - val_accuracy: 0.8798\n",
      "Epoch 2/20\n",
      "417/417 [==============================] - 1s 3ms/step - loss: 0.3652 - accuracy: 0.8441 - val_loss: 0.3086 - val_accuracy: 0.8760\n",
      "Epoch 3/20\n",
      "417/417 [==============================] - 2s 4ms/step - loss: 0.3479 - accuracy: 0.8514 - val_loss: 0.2889 - val_accuracy: 0.8820\n",
      "Epoch 4/20\n",
      "417/417 [==============================] - 1s 3ms/step - loss: 0.3440 - accuracy: 0.8528 - val_loss: 0.2545 - val_accuracy: 0.8988\n",
      "Epoch 5/20\n",
      "417/417 [==============================] - 2s 4ms/step - loss: 0.3390 - accuracy: 0.8526 - val_loss: 0.2554 - val_accuracy: 0.8986\n",
      "Epoch 6/20\n",
      "417/417 [==============================] - 1s 3ms/step - loss: 0.3377 - accuracy: 0.8564 - val_loss: 0.2536 - val_accuracy: 0.8974\n",
      "Epoch 7/20\n",
      "417/417 [==============================] - 1s 4ms/step - loss: 0.3342 - accuracy: 0.8554 - val_loss: 0.2543 - val_accuracy: 0.8974\n",
      "Epoch 8/20\n",
      "417/417 [==============================] - 2s 4ms/step - loss: 0.3323 - accuracy: 0.8561 - val_loss: 0.3533 - val_accuracy: 0.8470\n",
      "Epoch 9/20\n",
      "417/417 [==============================] - 2s 4ms/step - loss: 0.3306 - accuracy: 0.8582 - val_loss: 0.3066 - val_accuracy: 0.8726\n",
      "209/209 [==============================] - 2s 4ms/step - loss: 0.4388 - accuracy: 0.8072\n",
      "Epoch 1/20\n",
      "417/417 [==============================] - 3s 5ms/step - loss: 0.5024 - accuracy: 0.7788 - val_loss: 0.3852 - val_accuracy: 0.8334\n",
      "Epoch 2/20\n",
      "417/417 [==============================] - 2s 4ms/step - loss: 0.3565 - accuracy: 0.8476 - val_loss: 0.3495 - val_accuracy: 0.8528\n",
      "Epoch 3/20\n",
      "417/417 [==============================] - 2s 4ms/step - loss: 0.3430 - accuracy: 0.8544 - val_loss: 0.3214 - val_accuracy: 0.8680\n",
      "Epoch 4/20\n",
      "417/417 [==============================] - 2s 4ms/step - loss: 0.3384 - accuracy: 0.8582 - val_loss: 0.2836 - val_accuracy: 0.8856\n",
      "Epoch 5/20\n",
      "417/417 [==============================] - 2s 4ms/step - loss: 0.3342 - accuracy: 0.8585 - val_loss: 0.4511 - val_accuracy: 0.7982\n",
      "Epoch 6/20\n",
      "417/417 [==============================] - 2s 4ms/step - loss: 0.3320 - accuracy: 0.8588 - val_loss: 0.3983 - val_accuracy: 0.8226\n",
      "Epoch 7/20\n",
      "417/417 [==============================] - 2s 4ms/step - loss: 0.3309 - accuracy: 0.8601 - val_loss: 0.3570 - val_accuracy: 0.8454\n",
      "209/209 [==============================] - 1s 3ms/step - loss: 0.3956 - accuracy: 0.8294\n",
      "Epoch 1/20\n",
      "417/417 [==============================] - 2s 4ms/step - loss: 0.2971 - accuracy: 0.9372 - val_loss: 2.4443 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/20\n",
      "417/417 [==============================] - 2s 5ms/step - loss: 0.1926 - accuracy: 0.9375 - val_loss: 1.9187 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/20\n",
      "417/417 [==============================] - 2s 4ms/step - loss: 0.1579 - accuracy: 0.9411 - val_loss: 1.7360 - val_accuracy: 0.2026\n",
      "Epoch 4/20\n",
      "417/417 [==============================] - 2s 4ms/step - loss: 0.1370 - accuracy: 0.9525 - val_loss: 1.5251 - val_accuracy: 0.3496\n",
      "Epoch 5/20\n",
      "417/417 [==============================] - 2s 4ms/step - loss: 0.1291 - accuracy: 0.9543 - val_loss: 1.6587 - val_accuracy: 0.3502\n",
      "Epoch 6/20\n",
      "417/417 [==============================] - 1s 3ms/step - loss: 0.1239 - accuracy: 0.9551 - val_loss: 1.3103 - val_accuracy: 0.4718\n",
      "Epoch 7/20\n",
      "417/417 [==============================] - 1s 3ms/step - loss: 0.1214 - accuracy: 0.9558 - val_loss: 1.6631 - val_accuracy: 0.3734\n",
      "Epoch 8/20\n",
      "417/417 [==============================] - 2s 4ms/step - loss: 0.1193 - accuracy: 0.9567 - val_loss: 1.2077 - val_accuracy: 0.5130\n",
      "Epoch 9/20\n",
      "417/417 [==============================] - 2s 4ms/step - loss: 0.1185 - accuracy: 0.9568 - val_loss: 1.5955 - val_accuracy: 0.4004\n",
      "Epoch 10/20\n",
      "417/417 [==============================] - 2s 4ms/step - loss: 0.1170 - accuracy: 0.9567 - val_loss: 1.5868 - val_accuracy: 0.4070\n",
      "Epoch 11/20\n",
      "417/417 [==============================] - 1s 3ms/step - loss: 0.1158 - accuracy: 0.9583 - val_loss: 1.4711 - val_accuracy: 0.4368\n",
      "209/209 [==============================] - 1s 3ms/step - loss: 1.2597 - accuracy: 0.4984\n",
      "Epoch 1/20\n",
      "625/625 [==============================] - 4s 3ms/step - loss: 0.5163 - accuracy: 0.7358 - val_loss: 0.6584 - val_accuracy: 0.7140\n",
      "Epoch 2/20\n",
      "625/625 [==============================] - 2s 3ms/step - loss: 0.3526 - accuracy: 0.8536 - val_loss: 0.5251 - val_accuracy: 0.7666\n",
      "Epoch 3/20\n",
      "625/625 [==============================] - 2s 3ms/step - loss: 0.3328 - accuracy: 0.8591 - val_loss: 0.4633 - val_accuracy: 0.8004\n",
      "Epoch 4/20\n",
      "625/625 [==============================] - 2s 3ms/step - loss: 0.3264 - accuracy: 0.8621 - val_loss: 0.4652 - val_accuracy: 0.8024\n",
      "Epoch 5/20\n",
      "625/625 [==============================] - 2s 3ms/step - loss: 0.3224 - accuracy: 0.8652 - val_loss: 0.4206 - val_accuracy: 0.8260\n",
      "Epoch 6/20\n",
      "625/625 [==============================] - 2s 3ms/step - loss: 0.3205 - accuracy: 0.8658 - val_loss: 0.4005 - val_accuracy: 0.8308\n",
      "Epoch 7/20\n",
      "625/625 [==============================] - 2s 3ms/step - loss: 0.3188 - accuracy: 0.8668 - val_loss: 0.5168 - val_accuracy: 0.7758\n",
      "Epoch 8/20\n",
      "625/625 [==============================] - 2s 3ms/step - loss: 0.3152 - accuracy: 0.8673 - val_loss: 0.4524 - val_accuracy: 0.8066\n",
      "Epoch 9/20\n",
      "625/625 [==============================] - 2s 3ms/step - loss: 0.3145 - accuracy: 0.8688 - val_loss: 0.3847 - val_accuracy: 0.8376\n",
      "Epoch 10/20\n",
      "625/625 [==============================] - 2s 3ms/step - loss: 0.3128 - accuracy: 0.8691 - val_loss: 0.3807 - val_accuracy: 0.8422\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/20\n",
      "625/625 [==============================] - 2s 3ms/step - loss: 0.3133 - accuracy: 0.8684 - val_loss: 0.4188 - val_accuracy: 0.8256\n",
      "Epoch 12/20\n",
      "625/625 [==============================] - 2s 3ms/step - loss: 0.3107 - accuracy: 0.8706 - val_loss: 0.4898 - val_accuracy: 0.7910\n",
      "Epoch 13/20\n",
      "625/625 [==============================] - 2s 3ms/step - loss: 0.3105 - accuracy: 0.8708 - val_loss: 0.5069 - val_accuracy: 0.7850\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from scipy.stats import reciprocal\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "def build_model(input_shape, n_hidden=1, n_neurons=30, learning_rate=3e-3):\n",
    "    model = keras.models.Sequential()\n",
    "    options = {\"input_shape\": input_shape}\n",
    "    for layer in range(n_hidden):\n",
    "        model.add(keras.layers.Dense(n_neurons, activation=\"relu\", **options))\n",
    "        options = {}\n",
    "    model.add(keras.layers.Dense(1, activation=\"sigmoid\"))\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "keras_clf = KerasClassifier(build_model, input_shape=X_train_transformed_vect_avrg.shape[1:])\n",
    "\n",
    "param_distribs = {\n",
    " \"n_hidden\": [0, 1, 2, 3],\n",
    " \"n_neurons\": np.arange(1, 100),\n",
    " \"learning_rate\": reciprocal(3e-4, 3e-2)\n",
    "}\n",
    "\n",
    "vect_avrg_rnd_search_cv =  RandomizedSearchCV(keras_clf, param_distribs, n_iter=10, cv=3)\n",
    "\n",
    "# Callbacks\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=3, restore_best_weights=True)\n",
    "\n",
    "\n",
    "vect_avrg_rnd_search_cv.fit(X_train_transformed_vect_avrg, y_train, epochs=20, \n",
    "                        validation_data=(X_valid_transformed_vect_avrg, y_valid), \n",
    "                        callbacks=[early_stopping_cb]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c25ef68e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.711654524008433"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect_avrg_rnd_search_cv.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea59255",
   "metadata": {},
   "source": [
    "### Option 2: Clustering\n",
    "Word2Vec creates clusters of semantically related words, so another possible approach is to exploit the similarity of words within a cluster. Grouping vectors in this way is known as \"vector quantization.\" To accomplish this, we first need to find the centers of the word clusters, which we can do by using a clustering algorithm such as K-Means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "86076913",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Agustin\\anaconda3\\envs\\movie_sentiment_analysis\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Set \"k\" (num_clusters) to be 1/5th of the vocabulary size\n",
    "word_vectors = model.wv.vectors\n",
    "num_clusters = int(word_vectors.shape[0] / 5)\n",
    "\n",
    "# Initalize a k-means object and use it to extract centroids\n",
    "kmeans_clustering = KMeans(n_clusters=num_clusters)\n",
    "idx = kmeans_clustering.fit_predict(word_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d5a674",
   "metadata": {},
   "source": [
    "The cluster assignment for each word is now stored in idx, and the vocabulary from our original Word2Vec model is still stored in `model.mv.index_to_key`. For convenience, we zip these into one dictionary as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c9243018",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Word / Index dictionary, mapping each vocabulary word to\n",
    "# a cluster number                                                                                            \n",
    "word_centroid_map = dict(zip(model.wv.index_to_key,idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e3c86799",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 0\n",
      "['knows', 'thinks', 'cares', 'believes', 'understands', 'considers']\n",
      "Cluster 1\n",
      "['sand', 'moonlight', 'bushes']\n",
      "Cluster 2\n",
      "['professor', 'assistant', 'ex', 'ally', 'magician', 'knox', 'acquaintance', 'apprentice', 'physician', 'ivy']\n",
      "Cluster 3\n",
      "['horrors', 'atrocities', 'tragedies', 'genocide', 'symptoms']\n",
      "Cluster 4\n",
      "['enhance', 'contributing', 'heighten', 'relieve', 'reinforce']\n",
      "Cluster 5\n",
      "['rogers', 'solo', 'ruby', 'lil', 'merry', 'lucille', 'macmurray', 'dolly', 'burlesque', 'duet', 'rodgers', 'prima', 'keeler']\n",
      "Cluster 6\n",
      "['gregg', 'araki']\n",
      "Cluster 7\n",
      "['brady', 'evans', 'dale', 'meyers', 'hicks', 'perennial', 'kruger', 'trey', 'culkin', 'feldman', 'fosters', 'rowan', 'fenton', 'richie', 'talbot', 'mcdowall', 'joness', 'adrienne', 'brewster', 'robby', 'matthews', 'hammond', 'sawyer', 'rory', 'henson', 'damien', 'angelo']\n",
      "Cluster 8\n",
      "['go', 'sit', 'sat']\n",
      "Cluster 9\n",
      "['schedule']\n"
     ]
    }
   ],
   "source": [
    "# For the first 10 clusters\n",
    "for cluster in range(0,10):\n",
    "    #\n",
    "    # Print the cluster number  \n",
    "    print(f\"Cluster {cluster}\")\n",
    "    #\n",
    "    # Find all of the words for that cluster number, and print them out\n",
    "    words = []\n",
    "    for i in range(0,len(word_centroid_map.values())):\n",
    "        if(list(word_centroid_map.values())[i] == cluster ):\n",
    "            words.append(list(word_centroid_map.keys())[i])\n",
    "    print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82e858bc",
   "metadata": {},
   "source": [
    "Now we have a cluster (or \"centroid\") assignment for each word, and we can define a function to convert reviews into bags-of-centroids. This works just like Bag of Words but uses semantically related clusters instead of individual words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c9c4c3d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BagOfCentroidsVectorizer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, num_clusters, word_centroid_map):\n",
    "        self.num_clusters = num_clusters\n",
    "        self.word_centroid_map = word_centroid_map\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        # Pre-allocate an array for the training set bags of centroids (for speed)\n",
    "        self.train_centroids = np.zeros((len(X), self.num_clusters), dtype=\"float32\")\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        def create_bag_of_centroids(wordlists):\n",
    "            for count, wordlist in enumerate(wordlists):\n",
    "                # The number of clusters is equal to the highest cluster index\n",
    "                # in the word / centroid map\n",
    "                num_centroids = max(word_centroid_map.values()) + 1\n",
    "\n",
    "                # Pre-allocate the bag of centroids vector (for speed)\n",
    "                bag_of_centroids = np.zeros( num_centroids, dtype=\"float32\" )\n",
    "\n",
    "                # Loop over the words in the review. If the word is in the vocabulary,\n",
    "                # find which cluster it belongs to, and increment that cluster count \n",
    "                # by one\n",
    "                for word in wordlist:\n",
    "                    if word in word_centroid_map:\n",
    "                        index = word_centroid_map[word]\n",
    "                        bag_of_centroids[index] += 1\n",
    "\n",
    "                self.train_centroids[count] = bag_of_centroids\n",
    "            return self.train_centroids\n",
    "        \n",
    "        return create_bag_of_centroids(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5bbff794",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-06 13:01:58,045 : INFO : loading Word2Vec object from 300features_40minwords_10context\n",
      "2023-08-06 13:01:58,068 : INFO : loading wv recursively from 300features_40minwords_10context.wv.* with mmap=None\n",
      "2023-08-06 13:01:58,068 : INFO : setting ignored attribute cum_table to None\n",
      "2023-08-06 13:01:58,228 : INFO : Word2Vec lifecycle event {'fname': '300features_40minwords_10context', 'datetime': '2023-08-06T13:01:58.226989', 'gensim': '4.3.0', 'python': '3.10.12 | packaged by Anaconda, Inc. | (main, Jul  5 2023, 19:09:20) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'loaded'}\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "w2v_model = Word2Vec.load(\"300features_40minwords_10context\")\n",
    "\n",
    "boc_preprocessor = Pipeline([\n",
    "    ('review_to_wordlist', review_to_wordlist_pipeline),\n",
    "    ('bag_centroids_vectorizer', BagOfCentroidsVectorizer(num_clusters, word_centroid_map)),\n",
    "]);\n",
    "\n",
    "# We will remove stopwords in this step, so we need to set this hyperparameter manually.\n",
    "boc_preprocessor.set_params(review_to_wordlist__text_cleaner__remove_stopwords=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "393dcd59",
   "metadata": {},
   "source": [
    "Here we will transform all our training data **(both our labeled and unlabeled data)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ec3b53c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Agustin\\AppData\\Local\\Temp\\ipykernel_13052\\1586490070.py:11: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  soup = BeautifulSoup(html_text, 'html.parser')\n"
     ]
    }
   ],
   "source": [
    "X_train_transformed_boc = boc_preprocessor.fit_transform(X_train_labeled)\n",
    "X_test_transformed_boc = boc_preprocessor.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "40cae7e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 3312)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_transformed_boc.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7754633c",
   "metadata": {},
   "source": [
    "#### Building our classification models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ba8f2da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_transformed_boc, X_valid_transformed_boc = X_train_transformed_boc[:20000], X_train_transformed_boc[20000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ade190f",
   "metadata": {},
   "source": [
    "##### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4afb470c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.855   0.83375 0.84275 0.8345  0.83625]\n",
      "0.8404499999999999\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Initialize a Random Forest classifier with 100 trees\n",
    "rf_clf = RandomForestClassifier(n_estimators = 100)\n",
    "cv = cross_val_score(rf_clf, X_train_transformed_boc, y_train, cv=5)\n",
    "print(cv)\n",
    "print(cv.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd0c3f4",
   "metadata": {},
   "source": [
    "#### FeedForward Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "41606ecc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Agustin\\AppData\\Local\\Temp\\ipykernel_13052\\2765094469.py:1: DeprecationWarning: KerasClassifier is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n",
      "  keras_clf = KerasClassifier(build_model, input_shape=X_train_transformed_boc.shape[1:])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "417/417 [==============================] - 2s 4ms/step - loss: 0.4876 - accuracy: 0.8103 - val_loss: 0.3483 - val_accuracy: 0.8940\n",
      "Epoch 2/20\n",
      "417/417 [==============================] - 1s 3ms/step - loss: 0.3457 - accuracy: 0.8825 - val_loss: 0.3264 - val_accuracy: 0.8834\n",
      "Epoch 3/20\n",
      "417/417 [==============================] - 1s 3ms/step - loss: 0.2971 - accuracy: 0.8975 - val_loss: 0.2915 - val_accuracy: 0.8936\n",
      "Epoch 4/20\n",
      "417/417 [==============================] - 1s 4ms/step - loss: 0.2705 - accuracy: 0.9044 - val_loss: 0.2974 - val_accuracy: 0.8862\n",
      "Epoch 5/20\n",
      "417/417 [==============================] - 1s 3ms/step - loss: 0.2520 - accuracy: 0.9098 - val_loss: 0.2960 - val_accuracy: 0.8834\n",
      "Epoch 6/20\n",
      "417/417 [==============================] - 1s 3ms/step - loss: 0.2385 - accuracy: 0.9143 - val_loss: 0.2776 - val_accuracy: 0.8900\n",
      "Epoch 7/20\n",
      "417/417 [==============================] - 1s 4ms/step - loss: 0.2278 - accuracy: 0.9175 - val_loss: 0.2734 - val_accuracy: 0.8926\n",
      "Epoch 8/20\n",
      "417/417 [==============================] - 1s 3ms/step - loss: 0.2191 - accuracy: 0.9204 - val_loss: 0.2877 - val_accuracy: 0.8848\n",
      "Epoch 9/20\n",
      "417/417 [==============================] - 1s 3ms/step - loss: 0.2118 - accuracy: 0.9224 - val_loss: 0.3196 - val_accuracy: 0.8696\n",
      "Epoch 10/20\n",
      "417/417 [==============================] - 1s 3ms/step - loss: 0.2055 - accuracy: 0.9253 - val_loss: 0.3202 - val_accuracy: 0.8706\n",
      "209/209 [==============================] - 1s 3ms/step - loss: 0.4202 - accuracy: 0.8270\n",
      "Epoch 1/20\n",
      "417/417 [==============================] - 2s 4ms/step - loss: 0.4937 - accuracy: 0.8051 - val_loss: 0.4034 - val_accuracy: 0.8536\n",
      "Epoch 2/20\n",
      "417/417 [==============================] - 1s 3ms/step - loss: 0.3470 - accuracy: 0.8864 - val_loss: 0.4064 - val_accuracy: 0.8288\n",
      "Epoch 3/20\n",
      "417/417 [==============================] - 1s 3ms/step - loss: 0.2975 - accuracy: 0.9005 - val_loss: 0.3652 - val_accuracy: 0.8498\n",
      "Epoch 4/20\n",
      "417/417 [==============================] - 1s 3ms/step - loss: 0.2701 - accuracy: 0.9071 - val_loss: 0.3461 - val_accuracy: 0.8570\n",
      "Epoch 5/20\n",
      "417/417 [==============================] - 1s 3ms/step - loss: 0.2515 - accuracy: 0.9113 - val_loss: 0.3666 - val_accuracy: 0.8454\n",
      "Epoch 6/20\n",
      "417/417 [==============================] - 1s 4ms/step - loss: 0.2378 - accuracy: 0.9156 - val_loss: 0.3772 - val_accuracy: 0.8396\n",
      "Epoch 7/20\n",
      "417/417 [==============================] - 1s 3ms/step - loss: 0.2270 - accuracy: 0.9194 - val_loss: 0.3389 - val_accuracy: 0.8598\n",
      "Epoch 8/20\n",
      "417/417 [==============================] - 1s 3ms/step - loss: 0.2180 - accuracy: 0.9227 - val_loss: 0.3650 - val_accuracy: 0.8478\n",
      "Epoch 9/20\n",
      "417/417 [==============================] - 2s 4ms/step - loss: 0.2103 - accuracy: 0.9233 - val_loss: 0.3800 - val_accuracy: 0.8396\n",
      "Epoch 10/20\n",
      "417/417 [==============================] - 1s 3ms/step - loss: 0.2038 - accuracy: 0.9263 - val_loss: 0.3436 - val_accuracy: 0.8606\n",
      "209/209 [==============================] - 1s 3ms/step - loss: 0.3591 - accuracy: 0.8436\n",
      "Epoch 1/20\n",
      "417/417 [==============================] - 2s 4ms/step - loss: 0.2682 - accuracy: 0.9338 - val_loss: 2.1814 - val_accuracy: 0.0136\n",
      "Epoch 2/20\n",
      "417/417 [==============================] - 1s 4ms/step - loss: 0.1730 - accuracy: 0.9458 - val_loss: 1.6674 - val_accuracy: 0.1712\n",
      "Epoch 3/20\n",
      "417/417 [==============================] - 1s 3ms/step - loss: 0.1342 - accuracy: 0.9578 - val_loss: 1.6816 - val_accuracy: 0.2294\n",
      "Epoch 4/20\n",
      "417/417 [==============================] - 1s 3ms/step - loss: 0.1131 - accuracy: 0.9642 - val_loss: 1.6880 - val_accuracy: 0.2646\n",
      "Epoch 5/20\n",
      "417/417 [==============================] - 1s 3ms/step - loss: 0.0990 - accuracy: 0.9689 - val_loss: 1.6291 - val_accuracy: 0.3182\n",
      "Epoch 6/20\n",
      "417/417 [==============================] - 1s 3ms/step - loss: 0.0888 - accuracy: 0.9726 - val_loss: 1.6854 - val_accuracy: 0.3278\n",
      "Epoch 7/20\n",
      "417/417 [==============================] - 1s 3ms/step - loss: 0.0807 - accuracy: 0.9765 - val_loss: 1.7530 - val_accuracy: 0.3356\n",
      "Epoch 8/20\n",
      "417/417 [==============================] - 1s 3ms/step - loss: 0.0740 - accuracy: 0.9779 - val_loss: 1.7435 - val_accuracy: 0.3560\n",
      "209/209 [==============================] - 1s 3ms/step - loss: 1.5553 - accuracy: 0.3380\n",
      "Epoch 1/20\n",
      "417/417 [==============================] - 3s 6ms/step - loss: 0.3619 - accuracy: 0.8488 - val_loss: 0.3730 - val_accuracy: 0.8436\n",
      "Epoch 2/20\n",
      "417/417 [==============================] - 2s 5ms/step - loss: 0.2190 - accuracy: 0.9132 - val_loss: 0.3794 - val_accuracy: 0.8490\n",
      "Epoch 3/20\n",
      "417/417 [==============================] - 2s 5ms/step - loss: 0.1526 - accuracy: 0.9419 - val_loss: 0.3377 - val_accuracy: 0.8730\n",
      "Epoch 4/20\n",
      "417/417 [==============================] - 2s 5ms/step - loss: 0.0932 - accuracy: 0.9691 - val_loss: 0.4895 - val_accuracy: 0.8356\n",
      "Epoch 5/20\n",
      "417/417 [==============================] - 2s 5ms/step - loss: 0.0502 - accuracy: 0.9877 - val_loss: 0.4050 - val_accuracy: 0.8832\n",
      "Epoch 6/20\n",
      "417/417 [==============================] - 3s 6ms/step - loss: 0.0246 - accuracy: 0.9957 - val_loss: 0.4542 - val_accuracy: 0.8730\n",
      "209/209 [==============================] - 1s 3ms/step - loss: 0.4873 - accuracy: 0.8240\n",
      "Epoch 1/20\n",
      "417/417 [==============================] - 3s 6ms/step - loss: 0.3592 - accuracy: 0.8519 - val_loss: 0.4383 - val_accuracy: 0.8178\n",
      "Epoch 2/20\n",
      "417/417 [==============================] - 2s 5ms/step - loss: 0.2169 - accuracy: 0.9148 - val_loss: 0.2854 - val_accuracy: 0.8880\n",
      "Epoch 3/20\n",
      "417/417 [==============================] - 2s 5ms/step - loss: 0.1481 - accuracy: 0.9444 - val_loss: 0.4117 - val_accuracy: 0.8542\n",
      "Epoch 4/20\n",
      "417/417 [==============================] - 2s 5ms/step - loss: 0.0836 - accuracy: 0.9736 - val_loss: 0.4872 - val_accuracy: 0.8410\n",
      "Epoch 5/20\n",
      "417/417 [==============================] - 2s 5ms/step - loss: 0.0442 - accuracy: 0.9896 - val_loss: 0.6276 - val_accuracy: 0.8246\n",
      "209/209 [==============================] - 1s 3ms/step - loss: 0.5162 - accuracy: 0.7887\n",
      "Epoch 1/20\n",
      "417/417 [==============================] - 3s 6ms/step - loss: 0.1751 - accuracy: 0.9471 - val_loss: 1.3362 - val_accuracy: 0.4754\n",
      "Epoch 2/20\n",
      "417/417 [==============================] - 2s 5ms/step - loss: 0.0750 - accuracy: 0.9757 - val_loss: 2.0171 - val_accuracy: 0.4492\n",
      "Epoch 3/20\n",
      "417/417 [==============================] - 2s 5ms/step - loss: 0.0414 - accuracy: 0.9873 - val_loss: 2.5896 - val_accuracy: 0.4560\n",
      "Epoch 4/20\n",
      "417/417 [==============================] - 2s 5ms/step - loss: 0.0195 - accuracy: 0.9954 - val_loss: 3.8027 - val_accuracy: 0.3990\n",
      "209/209 [==============================] - 1s 3ms/step - loss: 1.2883 - accuracy: 0.4860\n",
      "Epoch 1/20\n",
      "417/417 [==============================] - 3s 4ms/step - loss: 0.4886 - accuracy: 0.8041 - val_loss: 0.3525 - val_accuracy: 0.8916\n",
      "Epoch 2/20\n",
      "417/417 [==============================] - 1s 3ms/step - loss: 0.3455 - accuracy: 0.8824 - val_loss: 0.3029 - val_accuracy: 0.8962\n",
      "Epoch 3/20\n",
      "417/417 [==============================] - 1s 3ms/step - loss: 0.2971 - accuracy: 0.8980 - val_loss: 0.2811 - val_accuracy: 0.9004\n",
      "Epoch 4/20\n",
      "417/417 [==============================] - 1s 3ms/step - loss: 0.2702 - accuracy: 0.9035 - val_loss: 0.2968 - val_accuracy: 0.8822\n",
      "Epoch 5/20\n",
      "417/417 [==============================] - 1s 3ms/step - loss: 0.2523 - accuracy: 0.9092 - val_loss: 0.2800 - val_accuracy: 0.8906\n",
      "Epoch 6/20\n",
      "417/417 [==============================] - 1s 3ms/step - loss: 0.2390 - accuracy: 0.9153 - val_loss: 0.3040 - val_accuracy: 0.8760\n",
      "Epoch 7/20\n",
      "417/417 [==============================] - 1s 3ms/step - loss: 0.2282 - accuracy: 0.9180 - val_loss: 0.2982 - val_accuracy: 0.8784\n",
      "Epoch 8/20\n",
      "417/417 [==============================] - 1s 3ms/step - loss: 0.2193 - accuracy: 0.9185 - val_loss: 0.2839 - val_accuracy: 0.8860\n",
      "209/209 [==============================] - 1s 3ms/step - loss: 0.4034 - accuracy: 0.8363\n",
      "Epoch 1/20\n",
      "417/417 [==============================] - 2s 4ms/step - loss: 0.4953 - accuracy: 0.8034 - val_loss: 0.4289 - val_accuracy: 0.8414\n",
      "Epoch 2/20\n",
      "417/417 [==============================] - 1s 3ms/step - loss: 0.3489 - accuracy: 0.8857 - val_loss: 0.3949 - val_accuracy: 0.8412\n",
      "Epoch 3/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "417/417 [==============================] - 1s 3ms/step - loss: 0.2992 - accuracy: 0.8995 - val_loss: 0.3372 - val_accuracy: 0.8674\n",
      "Epoch 4/20\n",
      "417/417 [==============================] - 1s 3ms/step - loss: 0.2714 - accuracy: 0.9048 - val_loss: 0.3320 - val_accuracy: 0.8670\n",
      "Epoch 5/20\n",
      "417/417 [==============================] - 1s 3ms/step - loss: 0.2524 - accuracy: 0.9110 - val_loss: 0.3407 - val_accuracy: 0.8580\n",
      "Epoch 6/20\n",
      "417/417 [==============================] - 1s 3ms/step - loss: 0.2386 - accuracy: 0.9155 - val_loss: 0.3798 - val_accuracy: 0.8426\n",
      "Epoch 7/20\n",
      "417/417 [==============================] - 1s 3ms/step - loss: 0.2277 - accuracy: 0.9201 - val_loss: 0.3877 - val_accuracy: 0.8370\n",
      "209/209 [==============================] - 1s 2ms/step - loss: 0.3661 - accuracy: 0.8465\n",
      "Epoch 1/20\n",
      "417/417 [==============================] - 2s 4ms/step - loss: 0.2746 - accuracy: 0.9310 - val_loss: 1.9784 - val_accuracy: 0.0252\n",
      "Epoch 2/20\n",
      "417/417 [==============================] - 1s 3ms/step - loss: 0.1752 - accuracy: 0.9467 - val_loss: 1.6840 - val_accuracy: 0.1674\n",
      "Epoch 3/20\n",
      "417/417 [==============================] - 1s 3ms/step - loss: 0.1356 - accuracy: 0.9575 - val_loss: 1.5743 - val_accuracy: 0.2574\n",
      "Epoch 4/20\n",
      "417/417 [==============================] - 1s 3ms/step - loss: 0.1140 - accuracy: 0.9642 - val_loss: 1.6344 - val_accuracy: 0.2830\n",
      "Epoch 5/20\n",
      "417/417 [==============================] - 1s 3ms/step - loss: 0.0996 - accuracy: 0.9684 - val_loss: 1.7139 - val_accuracy: 0.2958\n",
      "Epoch 6/20\n",
      "417/417 [==============================] - 1s 3ms/step - loss: 0.0890 - accuracy: 0.9724 - val_loss: 1.6375 - val_accuracy: 0.3382\n",
      "209/209 [==============================] - 1s 2ms/step - loss: 1.4947 - accuracy: 0.2825\n",
      "Epoch 1/20\n",
      "417/417 [==============================] - 3s 6ms/step - loss: 0.3588 - accuracy: 0.8509 - val_loss: 0.3012 - val_accuracy: 0.8816\n",
      "Epoch 2/20\n",
      "417/417 [==============================] - 2s 6ms/step - loss: 0.2302 - accuracy: 0.9108 - val_loss: 0.2551 - val_accuracy: 0.9022\n",
      "Epoch 3/20\n",
      "417/417 [==============================] - 3s 6ms/step - loss: 0.1791 - accuracy: 0.9302 - val_loss: 0.2972 - val_accuracy: 0.8902\n",
      "Epoch 4/20\n",
      "417/417 [==============================] - 2s 5ms/step - loss: 0.1327 - accuracy: 0.9500 - val_loss: 0.4344 - val_accuracy: 0.8520\n",
      "Epoch 5/20\n",
      "417/417 [==============================] - 2s 5ms/step - loss: 0.0903 - accuracy: 0.9706 - val_loss: 0.4897 - val_accuracy: 0.8408\n",
      "209/209 [==============================] - 2s 5ms/step - loss: 0.5499 - accuracy: 0.7835\n",
      "Epoch 1/20\n",
      "417/417 [==============================] - 4s 8ms/step - loss: 0.3619 - accuracy: 0.8477 - val_loss: 0.3579 - val_accuracy: 0.8524\n",
      "Epoch 2/20\n",
      "417/417 [==============================] - 2s 5ms/step - loss: 0.2290 - accuracy: 0.9119 - val_loss: 0.4411 - val_accuracy: 0.8172\n",
      "Epoch 3/20\n",
      "417/417 [==============================] - 2s 5ms/step - loss: 0.1744 - accuracy: 0.9326 - val_loss: 0.5389 - val_accuracy: 0.7998\n",
      "Epoch 4/20\n",
      "417/417 [==============================] - 3s 8ms/step - loss: 0.1292 - accuracy: 0.9525 - val_loss: 0.5274 - val_accuracy: 0.8144\n",
      "209/209 [==============================] - 2s 4ms/step - loss: 0.3903 - accuracy: 0.8325\n",
      "Epoch 1/20\n",
      "417/417 [==============================] - 8s 17ms/step - loss: 0.1842 - accuracy: 0.9426 - val_loss: 1.4013 - val_accuracy: 0.4160\n",
      "Epoch 2/20\n",
      "417/417 [==============================] - 15s 35ms/step - loss: 0.0848 - accuracy: 0.9725 - val_loss: 2.3600 - val_accuracy: 0.3412\n",
      "Epoch 3/20\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0508 - accuracy: 0.9838 - val_loss: 2.4182 - val_accuracy: 0.4386\n",
      "Epoch 4/20\n",
      "417/417 [==============================] - 14s 33ms/step - loss: 0.0305 - accuracy: 0.9917 - val_loss: 2.8058 - val_accuracy: 0.4522\n",
      "209/209 [==============================] - 6s 24ms/step - loss: 1.3498 - accuracy: 0.4350\n",
      "Epoch 1/20\n",
      "417/417 [==============================] - 8s 17ms/step - loss: 0.3619 - accuracy: 0.8481 - val_loss: 0.2545 - val_accuracy: 0.9004\n",
      "Epoch 2/20\n",
      "417/417 [==============================] - 3s 7ms/step - loss: 0.2386 - accuracy: 0.9029 - val_loss: 0.3238 - val_accuracy: 0.8710\n",
      "Epoch 3/20\n",
      "417/417 [==============================] - 3s 6ms/step - loss: 0.1972 - accuracy: 0.9207 - val_loss: 0.3027 - val_accuracy: 0.8844\n",
      "Epoch 4/20\n",
      "417/417 [==============================] - 3s 7ms/step - loss: 0.1629 - accuracy: 0.9365 - val_loss: 0.3406 - val_accuracy: 0.8724\n",
      "209/209 [==============================] - 2s 4ms/step - loss: 0.4998 - accuracy: 0.8010\n",
      "Epoch 1/20\n",
      "417/417 [==============================] - 4s 7ms/step - loss: 0.3672 - accuracy: 0.8489 - val_loss: 0.4089 - val_accuracy: 0.8318\n",
      "Epoch 2/20\n",
      "417/417 [==============================] - 4s 8ms/step - loss: 0.2411 - accuracy: 0.9034 - val_loss: 0.4513 - val_accuracy: 0.8160\n",
      "Epoch 3/20\n",
      "417/417 [==============================] - 3s 6ms/step - loss: 0.1971 - accuracy: 0.9209 - val_loss: 0.4054 - val_accuracy: 0.8378\n",
      "Epoch 4/20\n",
      "417/417 [==============================] - 3s 6ms/step - loss: 0.1658 - accuracy: 0.9350 - val_loss: 0.5173 - val_accuracy: 0.8128\n",
      "Epoch 5/20\n",
      "417/417 [==============================] - 3s 7ms/step - loss: 0.1344 - accuracy: 0.9474 - val_loss: 0.6107 - val_accuracy: 0.7916\n",
      "Epoch 6/20\n",
      "417/417 [==============================] - 5s 11ms/step - loss: 0.1028 - accuracy: 0.9638 - val_loss: 0.5620 - val_accuracy: 0.8274\n",
      "209/209 [==============================] - 2s 5ms/step - loss: 0.4379 - accuracy: 0.8264\n",
      "Epoch 1/20\n",
      "417/417 [==============================] - 5s 10ms/step - loss: 0.1994 - accuracy: 0.9392 - val_loss: 1.6118 - val_accuracy: 0.3214\n",
      "Epoch 2/20\n",
      "417/417 [==============================] - 3s 6ms/step - loss: 0.0921 - accuracy: 0.9703 - val_loss: 2.2574 - val_accuracy: 0.3202\n",
      "Epoch 3/20\n",
      "417/417 [==============================] - 3s 7ms/step - loss: 0.0588 - accuracy: 0.9813 - val_loss: 2.0253 - val_accuracy: 0.4524\n",
      "Epoch 4/20\n",
      "417/417 [==============================] - 3s 7ms/step - loss: 0.0369 - accuracy: 0.9901 - val_loss: 2.2997 - val_accuracy: 0.4712\n",
      "209/209 [==============================] - 2s 5ms/step - loss: 1.5412 - accuracy: 0.3417\n",
      "Epoch 1/20\n",
      "417/417 [==============================] - 4s 7ms/step - loss: 0.3591 - accuracy: 0.8478 - val_loss: 0.3641 - val_accuracy: 0.8498\n",
      "Epoch 2/20\n",
      "417/417 [==============================] - 3s 6ms/step - loss: 0.2228 - accuracy: 0.9116 - val_loss: 0.2986 - val_accuracy: 0.8786\n",
      "Epoch 3/20\n",
      "417/417 [==============================] - 3s 7ms/step - loss: 0.1567 - accuracy: 0.9387 - val_loss: 0.4091 - val_accuracy: 0.8492\n",
      "Epoch 4/20\n",
      "417/417 [==============================] - 2s 5ms/step - loss: 0.0992 - accuracy: 0.9665 - val_loss: 0.3082 - val_accuracy: 0.8952\n",
      "Epoch 5/20\n",
      "417/417 [==============================] - 2s 5ms/step - loss: 0.0538 - accuracy: 0.9857 - val_loss: 0.4041 - val_accuracy: 0.8794\n",
      "209/209 [==============================] - 2s 5ms/step - loss: 0.4675 - accuracy: 0.8187\n",
      "Epoch 1/20\n",
      "417/417 [==============================] - 4s 7ms/step - loss: 0.3631 - accuracy: 0.8493 - val_loss: 0.3722 - val_accuracy: 0.8450\n",
      "Epoch 2/20\n",
      "417/417 [==============================] - 2s 5ms/step - loss: 0.2153 - accuracy: 0.9179 - val_loss: 0.4354 - val_accuracy: 0.8284\n",
      "Epoch 3/20\n",
      "417/417 [==============================] - 2s 5ms/step - loss: 0.1461 - accuracy: 0.9440 - val_loss: 0.4840 - val_accuracy: 0.8254\n",
      "Epoch 4/20\n",
      "417/417 [==============================] - 2s 5ms/step - loss: 0.0848 - accuracy: 0.9751 - val_loss: 0.4688 - val_accuracy: 0.8540\n",
      "209/209 [==============================] - 1s 3ms/step - loss: 0.3610 - accuracy: 0.8435\n",
      "Epoch 1/20\n",
      "417/417 [==============================] - 3s 5ms/step - loss: 0.1779 - accuracy: 0.9456 - val_loss: 1.7452 - val_accuracy: 0.3838\n",
      "Epoch 2/20\n",
      "417/417 [==============================] - 2s 5ms/step - loss: 0.0760 - accuracy: 0.9738 - val_loss: 2.0849 - val_accuracy: 0.4244\n",
      "Epoch 3/20\n",
      "417/417 [==============================] - 2s 5ms/step - loss: 0.0390 - accuracy: 0.9883 - val_loss: 2.8918 - val_accuracy: 0.4188\n",
      "Epoch 4/20\n",
      "417/417 [==============================] - 3s 7ms/step - loss: 0.0177 - accuracy: 0.9964 - val_loss: 3.6082 - val_accuracy: 0.4082\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "209/209 [==============================] - 1s 3ms/step - loss: 1.6692 - accuracy: 0.4010\n",
      "Epoch 1/20\n",
      "417/417 [==============================] - 3s 6ms/step - loss: 0.3618 - accuracy: 0.8466 - val_loss: 0.3064 - val_accuracy: 0.8786\n",
      "Epoch 2/20\n",
      "417/417 [==============================] - 3s 8ms/step - loss: 0.2275 - accuracy: 0.9092 - val_loss: 0.2922 - val_accuracy: 0.8854\n",
      "Epoch 3/20\n",
      "417/417 [==============================] - 3s 7ms/step - loss: 0.1657 - accuracy: 0.9370 - val_loss: 0.3604 - val_accuracy: 0.8688\n",
      "Epoch 4/20\n",
      "417/417 [==============================] - 2s 6ms/step - loss: 0.1118 - accuracy: 0.9615 - val_loss: 0.2666 - val_accuracy: 0.9098\n",
      "Epoch 5/20\n",
      "417/417 [==============================] - 2s 5ms/step - loss: 0.0685 - accuracy: 0.9801 - val_loss: 0.5280 - val_accuracy: 0.8364\n",
      "Epoch 6/20\n",
      "417/417 [==============================] - 2s 6ms/step - loss: 0.0421 - accuracy: 0.9901 - val_loss: 0.5575 - val_accuracy: 0.8492\n",
      "Epoch 7/20\n",
      "417/417 [==============================] - 2s 6ms/step - loss: 0.0220 - accuracy: 0.9968 - val_loss: 0.5587 - val_accuracy: 0.8638\n",
      "209/209 [==============================] - 2s 4ms/step - loss: 0.7623 - accuracy: 0.7602\n",
      "Epoch 1/20\n",
      "417/417 [==============================] - 5s 7ms/step - loss: 0.3588 - accuracy: 0.8513 - val_loss: 0.3360 - val_accuracy: 0.8626\n",
      "Epoch 2/20\n",
      "417/417 [==============================] - 2s 6ms/step - loss: 0.2281 - accuracy: 0.9095 - val_loss: 0.3437 - val_accuracy: 0.8586\n",
      "Epoch 3/20\n",
      "417/417 [==============================] - 2s 5ms/step - loss: 0.1680 - accuracy: 0.9350 - val_loss: 0.3860 - val_accuracy: 0.8514\n",
      "Epoch 4/20\n",
      "417/417 [==============================] - 2s 5ms/step - loss: 0.1131 - accuracy: 0.9618 - val_loss: 0.5458 - val_accuracy: 0.8144\n",
      "209/209 [==============================] - 2s 3ms/step - loss: 0.4032 - accuracy: 0.8315\n",
      "Epoch 1/20\n",
      "417/417 [==============================] - 3s 5ms/step - loss: 0.1811 - accuracy: 0.9441 - val_loss: 1.9006 - val_accuracy: 0.3002\n",
      "Epoch 2/20\n",
      "417/417 [==============================] - 2s 4ms/step - loss: 0.0826 - accuracy: 0.9738 - val_loss: 2.0665 - val_accuracy: 0.4070\n",
      "Epoch 3/20\n",
      "417/417 [==============================] - 2s 5ms/step - loss: 0.0454 - accuracy: 0.9855 - val_loss: 2.6844 - val_accuracy: 0.4320\n",
      "Epoch 4/20\n",
      "417/417 [==============================] - 2s 5ms/step - loss: 0.0260 - accuracy: 0.9933 - val_loss: 2.6948 - val_accuracy: 0.4926\n",
      "209/209 [==============================] - 1s 3ms/step - loss: 1.8250 - accuracy: 0.3199\n",
      "Epoch 1/20\n",
      "417/417 [==============================] - 4s 7ms/step - loss: 0.3607 - accuracy: 0.8425 - val_loss: 0.2988 - val_accuracy: 0.8792\n",
      "Epoch 2/20\n",
      "417/417 [==============================] - 3s 6ms/step - loss: 0.2048 - accuracy: 0.9176 - val_loss: 0.2200 - val_accuracy: 0.9168\n",
      "Epoch 3/20\n",
      "417/417 [==============================] - 3s 7ms/step - loss: 0.1030 - accuracy: 0.9593 - val_loss: 0.5496 - val_accuracy: 0.8310\n",
      "Epoch 4/20\n",
      "417/417 [==============================] - 2s 6ms/step - loss: 0.0402 - accuracy: 0.9850 - val_loss: 1.0986 - val_accuracy: 0.7944\n",
      "Epoch 5/20\n",
      "417/417 [==============================] - 2s 5ms/step - loss: 0.0263 - accuracy: 0.9919 - val_loss: 0.8691 - val_accuracy: 0.8458\n",
      "209/209 [==============================] - 1s 3ms/step - loss: 0.6908 - accuracy: 0.7447\n",
      "Epoch 1/20\n",
      "417/417 [==============================] - 5s 7ms/step - loss: 0.3651 - accuracy: 0.8454 - val_loss: 0.3671 - val_accuracy: 0.8350\n",
      "Epoch 2/20\n",
      "417/417 [==============================] - 3s 6ms/step - loss: 0.1923 - accuracy: 0.9218 - val_loss: 0.5395 - val_accuracy: 0.8060\n",
      "Epoch 3/20\n",
      "417/417 [==============================] - 2s 6ms/step - loss: 0.0798 - accuracy: 0.9712 - val_loss: 0.6058 - val_accuracy: 0.8430\n",
      "Epoch 4/20\n",
      "417/417 [==============================] - 3s 6ms/step - loss: 0.0271 - accuracy: 0.9912 - val_loss: 0.9375 - val_accuracy: 0.8358\n",
      "209/209 [==============================] - 2s 4ms/step - loss: 0.3460 - accuracy: 0.8499\n",
      "Epoch 1/20\n",
      "417/417 [==============================] - 4s 8ms/step - loss: 0.1792 - accuracy: 0.9440 - val_loss: 1.3868 - val_accuracy: 0.4756\n",
      "Epoch 2/20\n",
      "417/417 [==============================] - 3s 6ms/step - loss: 0.0699 - accuracy: 0.9757 - val_loss: 3.4007 - val_accuracy: 0.3440\n",
      "Epoch 3/20\n",
      "417/417 [==============================] - 4s 9ms/step - loss: 0.0302 - accuracy: 0.9903 - val_loss: 3.3845 - val_accuracy: 0.4346\n",
      "Epoch 4/20\n",
      "417/417 [==============================] - 3s 8ms/step - loss: 0.0169 - accuracy: 0.9946 - val_loss: 3.5955 - val_accuracy: 0.4238\n",
      "209/209 [==============================] - 2s 6ms/step - loss: 1.3497 - accuracy: 0.4828\n",
      "Epoch 1/20\n",
      "417/417 [==============================] - 6s 12ms/step - loss: 0.3719 - accuracy: 0.8381 - val_loss: 0.3430 - val_accuracy: 0.8504\n",
      "Epoch 2/20\n",
      "417/417 [==============================] - 2s 5ms/step - loss: 0.2249 - accuracy: 0.9094 - val_loss: 0.3492 - val_accuracy: 0.8580\n",
      "Epoch 3/20\n",
      "417/417 [==============================] - 3s 8ms/step - loss: 0.1433 - accuracy: 0.9460 - val_loss: 0.3591 - val_accuracy: 0.8718\n",
      "Epoch 4/20\n",
      "417/417 [==============================] - 6s 14ms/step - loss: 0.0713 - accuracy: 0.9761 - val_loss: 0.4123 - val_accuracy: 0.8874\n",
      "209/209 [==============================] - 2s 5ms/step - loss: 0.3512 - accuracy: 0.8622\n",
      "Epoch 1/20\n",
      "417/417 [==============================] - 4s 8ms/step - loss: 0.3641 - accuracy: 0.8486 - val_loss: 0.3591 - val_accuracy: 0.8528\n",
      "Epoch 2/20\n",
      "417/417 [==============================] - 3s 7ms/step - loss: 0.2213 - accuracy: 0.9123 - val_loss: 0.3211 - val_accuracy: 0.8642\n",
      "Epoch 3/20\n",
      "417/417 [==============================] - 3s 7ms/step - loss: 0.1352 - accuracy: 0.9488 - val_loss: 0.5015 - val_accuracy: 0.8278\n",
      "Epoch 4/20\n",
      "417/417 [==============================] - 3s 6ms/step - loss: 0.0590 - accuracy: 0.9811 - val_loss: 0.5852 - val_accuracy: 0.8492\n",
      "Epoch 5/20\n",
      "417/417 [==============================] - 3s 6ms/step - loss: 0.0230 - accuracy: 0.9943 - val_loss: 0.8975 - val_accuracy: 0.8082\n",
      "209/209 [==============================] - 4s 13ms/step - loss: 0.4616 - accuracy: 0.7996\n",
      "Epoch 1/20\n",
      "417/417 [==============================] - 4s 9ms/step - loss: 0.1841 - accuracy: 0.9403 - val_loss: 1.6680 - val_accuracy: 0.3464\n",
      "Epoch 2/20\n",
      "417/417 [==============================] - 4s 9ms/step - loss: 0.0784 - accuracy: 0.9723 - val_loss: 2.3129 - val_accuracy: 0.4174\n",
      "Epoch 3/20\n",
      "417/417 [==============================] - 7s 17ms/step - loss: 0.0381 - accuracy: 0.9886 - val_loss: 3.4688 - val_accuracy: 0.3860\n",
      "Epoch 4/20\n",
      "417/417 [==============================] - 5s 12ms/step - loss: 0.0145 - accuracy: 0.9962 - val_loss: 3.1140 - val_accuracy: 0.5472\n",
      "209/209 [==============================] - 2s 6ms/step - loss: 1.6176 - accuracy: 0.3641\n",
      "Epoch 1/20\n",
      "417/417 [==============================] - 5s 9ms/step - loss: 0.3646 - accuracy: 0.8405 - val_loss: 0.3835 - val_accuracy: 0.8308\n",
      "Epoch 2/20\n",
      "417/417 [==============================] - 4s 9ms/step - loss: 0.2058 - accuracy: 0.9146 - val_loss: 0.2874 - val_accuracy: 0.8800\n",
      "Epoch 3/20\n",
      "417/417 [==============================] - 3s 7ms/step - loss: 0.1032 - accuracy: 0.9625 - val_loss: 0.5930 - val_accuracy: 0.8132\n",
      "Epoch 4/20\n",
      "417/417 [==============================] - 4s 9ms/step - loss: 0.0397 - accuracy: 0.9863 - val_loss: 0.6593 - val_accuracy: 0.8506\n",
      "Epoch 5/20\n",
      "417/417 [==============================] - 3s 8ms/step - loss: 0.0270 - accuracy: 0.9920 - val_loss: 0.9241 - val_accuracy: 0.8276\n",
      "209/209 [==============================] - 2s 6ms/step - loss: 0.4644 - accuracy: 0.8061\n",
      "Epoch 1/20\n",
      "417/417 [==============================] - 4s 8ms/step - loss: 0.3683 - accuracy: 0.8434 - val_loss: 0.3433 - val_accuracy: 0.8532\n",
      "Epoch 2/20\n",
      "417/417 [==============================] - 3s 8ms/step - loss: 0.2004 - accuracy: 0.9206 - val_loss: 0.6201 - val_accuracy: 0.7640\n",
      "Epoch 3/20\n",
      "417/417 [==============================] - 3s 7ms/step - loss: 0.1002 - accuracy: 0.9629 - val_loss: 0.5636 - val_accuracy: 0.8224\n",
      "Epoch 4/20\n",
      "417/417 [==============================] - 3s 7ms/step - loss: 0.0373 - accuracy: 0.9875 - val_loss: 0.6467 - val_accuracy: 0.8534\n",
      "209/209 [==============================] - 2s 5ms/step - loss: 0.3685 - accuracy: 0.8366\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "417/417 [==============================] - 4s 7ms/step - loss: 0.1855 - accuracy: 0.9438 - val_loss: 1.6854 - val_accuracy: 0.4066\n",
      "Epoch 2/20\n",
      "417/417 [==============================] - 3s 6ms/step - loss: 0.0747 - accuracy: 0.9736 - val_loss: 2.3651 - val_accuracy: 0.4402\n",
      "Epoch 3/20\n",
      "417/417 [==============================] - 2s 6ms/step - loss: 0.0332 - accuracy: 0.9888 - val_loss: 3.1045 - val_accuracy: 0.3820\n",
      "Epoch 4/20\n",
      "417/417 [==============================] - 2s 6ms/step - loss: 0.0129 - accuracy: 0.9955 - val_loss: 4.5705 - val_accuracy: 0.3830\n",
      "209/209 [==============================] - 2s 4ms/step - loss: 1.6135 - accuracy: 0.4213\n",
      "Epoch 1/20\n",
      "625/625 [==============================] - 4s 5ms/step - loss: 0.3369 - accuracy: 0.8603 - val_loss: 0.4769 - val_accuracy: 0.7886\n",
      "Epoch 2/20\n",
      "625/625 [==============================] - 3s 5ms/step - loss: 0.2157 - accuracy: 0.9135 - val_loss: 0.5658 - val_accuracy: 0.7744\n",
      "Epoch 3/20\n",
      "625/625 [==============================] - 3s 5ms/step - loss: 0.1406 - accuracy: 0.9489 - val_loss: 0.5637 - val_accuracy: 0.7974\n",
      "Epoch 4/20\n",
      "625/625 [==============================] - 3s 5ms/step - loss: 0.0738 - accuracy: 0.9790 - val_loss: 0.7265 - val_accuracy: 0.7782\n"
     ]
    }
   ],
   "source": [
    "keras_clf = KerasClassifier(build_model, input_shape=X_train_transformed_boc.shape[1:])\n",
    "\n",
    "param_distribs = {\n",
    " \"n_hidden\": [0, 1, 2, 3],\n",
    " \"n_neurons\": np.arange(1, 100),\n",
    " \"learning_rate\": reciprocal(3e-4, 3e-2)\n",
    "}\n",
    "\n",
    "boc_rnd_search_cv =  RandomizedSearchCV(keras_clf, param_distribs, n_iter=10, cv=3)\n",
    "\n",
    "# Callbacks\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=3, restore_best_weights=True)\n",
    "\n",
    "\n",
    "boc_rnd_search_cv.fit(X_train_transformed_boc, y_train, epochs=20, \n",
    "                        validation_data=(X_valid_transformed_boc, y_valid), \n",
    "                        callbacks=[early_stopping_cb]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "354b349a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.699600358804067"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boc_rnd_search_cv.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0b51f2a",
   "metadata": {},
   "source": [
    "We found that the code above gives about the same (or slightly worse) results compared to the Bag of Words and TF-IDF."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f7b82c",
   "metadata": {},
   "source": [
    "## Comparing our models on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e617ac8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF - Vector Average Accuracy: 0.79928\n",
      "782/782 [==============================] - 3s 3ms/step - loss: 0.3283 - accuracy: 0.8627\n",
      "FNN - Vector Average Accuracy: 0.8626518249511719\n",
      "RF - Clustering Accuracy: 0.93224\n",
      "782/782 [==============================] - 4s 4ms/step - loss: 0.2605 - accuracy: 0.8975\n",
      "FNN - Clustering Accuracy: 0.8975383639335632\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the best models on the test set\n",
    "rf_clf = RandomForestClassifier(n_estimators=100).fit(X_train_transformed_vect_avrg, y_train)\n",
    "rf_vct_avrg_score = rf_clf.score(X_test_transformed_vect_avrg, y_test)\n",
    "print(f\"RF - Vector Average Accuracy: {rf_vct_avrg_score}\")\n",
    "\n",
    "fnn_avrg_score = vect_avrg_rnd_search_cv.score(X_test_transformed_vect_avrg, y_test)\n",
    "print(f\"FNN - Vector Average Accuracy: {fnn_avrg_score}\")\n",
    "\n",
    "rf_clf = RandomForestClassifier(n_estimators=100).fit(X_train_transformed_boc, y_train)\n",
    "rf_boc_score = rf_clf.score(X_test_transformed_boc, y_test)\n",
    "print(f\"RF - Clustering Accuracy: {rf_boc_score}\")\n",
    "\n",
    "fnn_boc_score = boc_rnd_search_cv.score(X_test_transformed_boc, y_test)\n",
    "print(f\"FNN - Clustering Accuracy: {fnn_boc_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "054f8210",
   "metadata": {},
   "source": [
    "We can see that the Clustering option outperforms Vector Average and also obtains much better results than Bag-of-Words and TF-IDF vectors. We might further improve our results if we had used some model like **Doc2vec** that directly outputs a representation vector for a review instead of having to compute the average or perform clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b1ac7d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
